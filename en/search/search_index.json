{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>The Sensitive Data Protection on AWS solution allows enterprise customers to create data catalogs, discover, protect, and visualize sensitive data across multiple AWS accounts. The solution eliminates the need for manual tagging to track sensitive data such as Personal Identifiable Information (PII) and classified information. </p> <p>The solution provides an automated approach to data protection with a self-service web application. You can perform regular or on-demand sensitive data discovery jobs using your own data classification templates. Moreover, you can access metrics such as the total number of sensitive data entries stored in all your AWS accounts, which accounts contain the most sensitive data, and the data source\u00a0where the sensitive data is located. </p> <p></p> <p>The solution helps enterprise customers (such as companies with security or big data businesses) to implement the following data protection measures: </p> <ul> <li>centralized management over hundreds of AWS accounts</li> <li>automatic discovery of data assets</li> <li>sensitive data detection and automatic labeling</li> <li>integration with other AWS services or application</li> </ul> <p>This guide provides an overview of the solution, its reference architecture and components, considerations for planning the deployment, configuration steps for deploying the solution to the Amazon Web Services (AWS) Cloud. </p> <p>Use this navigation table to quickly find answers to these questions:</p> If you want to \u2026 Read\u2026 Know the cost for running this solution Cost Understand the security considerations for this solution Security Know how to plan for quotas for this solution Quotas Know which AWS Regions are supported for this solution Supported AWS Regions View or download the AWS CloudFormation template included in this solution to automatically deploy the infrastructure resources (the \u201cstack\u201d) for this solution AWS CloudFormation templates <p>The guide is intended for IT architects, developers, DevOps, data engineers with practical experience architecting on the AWS Cloud.</p>"},{"location":"contributors/","title":"Contributors","text":"<ul> <li>Chen, Haiyun</li> <li>Cui, Hubin</li> <li>Gu, George</li> <li>Hao, Liang</li> <li>Han, Xu</li> <li>Ji, Junxiang</li> <li>Jia, Ting</li> <li>Li, Xiujuan</li> <li>Lv, Ning</li> <li>Qin, Dehua</li> <li>Su, Fan</li> <li>Wang, Yu</li> <li>Yi, Ke </li> <li>Yi, Yan</li> <li>Zhang, Junzhong</li> </ul>"},{"location":"cost/","title":"Cost","text":"<p>Last updated at\uff1a2023/06/05</p> <p>The cost for main AWS account running this solution with the default settings in the Ningxia (cn-northwest-1) Region is approximately \u00a5901.82 CNY a month.</p>"},{"location":"cost/#cost-estimation","title":"Cost estimation","text":"<p>Based on typical usage patterns, we have indicated a few scenarios to provide an estimation of monthly costs. The AWS services listed in the example cost tables below are billed on a monthly basis.</p> <p>The cost of running the Sensitive Data Protection solution is based on a variety of factors, such as the number and sizes of datasets, the number and complexity of data structures, and the frequency of discovery job updates. For example, running a discovery job on a large dataset with a fully scanned range will result in higher costs than running the job on smaller datasets with increased scan range and limited scan depth that are run on demand.</p>"},{"location":"cost/#base-cost-for-infra-in-main-account","title":"Base cost for Infra (in main account)","text":"Service Usage Type Usage Quantity Monthly cost (CNY) Amazon Relational Database Service for MySQL Community Edition CNY 0.8 per db.t3.medium Multi-AZ instance hour (or partial hour) running MySQL 720 Hrs 576 Reserved Amazon Relational Database Service Provisioned Storage CNY 1.5308 per GB-month of provisioned gp2 storage for Multi-AZ deployments running MySQL 20 GB-Mo 30.616 On Demand Amazon Elastic Compute Cloud NatGateway 0.37 CNY per GB Data Processed by NAT Gateways 30 GB-Mo 11.1 On Demand. Avg 80 url requests per day 0.37 CNY per NAT Gateway Hour 720 Hrs 266.4 Reserved Athena CNY34.34 per Terabytes for DataScannedInTB in China (Ningxia) 0.010 Terabytes 0.34 On Demand CloudWatch First 5GB-mo per month of logs storage is free. CNY 0.244 per GB archived per month 0.100 GB-Mo 0.244 On Demand EC2 Container Registry (ECR) 500MB-month Free Tier, CNY 0.69 per GB-month 0.003 GB-Mo 0.69 Reserved Elastic Load Balancing - Application 0.0 CNY per Application LoadBalancer-hour (or partial hour) under monthly free tier. CNY 0.156 per Application load balancer-hour (or partial hour) 10 Hrs 1.56 On Demand 0.0 CNY per used Application load balancer capacity unit-hour (or partial hour) under monthly free tier. CNY 0.072 per LCU-hour (or partial hour) 0.105 LCU-Hrs 0 On Demand Lambda CNY 0 for first 400K GB-second usage of AWS Lambda - Total Compute - China (Ningxia), CNY 0.0000001135 Price per 1ms  (Ningxia) 100,000.000 Lambda-GB-Second 11.35 On Demand CNY 0 for first 1M usage of AWS Lambda - Total Requests - China (Ningxia), CNY 1.36 per million requests 10,000 Request 0 On Demand Simple Queue Service First 1,000,000 Amazon SQS Requests per month are free, CNY 3.33 (per Million requests) 100,000 Requests 3.33 On Demand Simple Storage Service First 2,000 PUTs free under free tier, CNY 0.00405 PUT, COPY, POST, LIST requests (per 1,000 requests) 2,000 Requests 0.0081 On Demand First 20,000 GETs free under free tier, CNY 0.0135 per 10,000 requests 4,000 Requests 0.0054 On Demand First 5 GB free under free tier, CNY 0.1755 per GB 0.032 GB-Mo CNY 0.00 0.1755 On Demand Total 901.819"},{"location":"cost/#base-cost-for-infra-in-monitored-account","title":"Base cost for Infra (in monitored account)","text":"Service Usage Type Glue 3.021 CNY per DPU-Hour, billed per second, with a 10-minute minimum per crawler run On Demand 3.021 CNY per Data Processing Unit-Hour for Amazon Glue ETL job On Demand 6.866 CNY per 1,000,000 requests for Amazon Glue Data Catalog request On Demand Step Functions CNY 0.00 for first 4,000 state transitions, CNY 0.0002102 per state transition thereafter On Demand CloudWatch First 5GB-mo per month of logs storage is free. CNY 0.244 per GB archived per month On Demand <p>The AWS services listed in the example cost tables below are billed on a monthly basis in a monitored account. Glue Crawler minimum cost is 0.5035 (CNY), the crawler will be launched in data source connection creation and PII detection job execution, below cost table includes both of the connection creation and job execution cost with the example data sources.</p> Scenarios Service Running Cost (CNY) Monthly Cost (CNY) Account 1, scan frequency monthly, scan depth 1000 S3 Bucket A: 10000 CSV files, 1000 rows, 10 columns, total size 1.7GiB S3 Bucket B: 10000 JSON files, 1000 rows, 10 fields, total size 2.5GiB S3 Bucket C: 1000 PARQUET files, 1000 rows, 10 fields, total size 212Mb Glue Crawler: 1.007 Glue Job: 2.3161 3.3231 Account 2, scan frequency weekly, scan depth 1000 RDS Aurora MySQL A: 10 tables, 10000 rows, 10 columns, instance type: db.r5.large(8vCPUs, 64GiB RAM Network: 4,750 Mbps) RDS MySQL B: 10 tables, 1,000,000 rows, 10 columns, instance type: db.m5.12xlarge(48 vCPU 192 GiB RAM Network:9500 Mbps) Glue Crawler: 2.5175 Glue Job: 2.8196 5.3371 Account 3, scan frequency daily, scan depth 1000 S3 Bucket A: 10000 CSV files, 1000 rows, 10 columns, total size 1.7GiB S3 Bucket B: 10000 JSON files, 1000 rows, 10 fields, total size 2.5GiB S3 Bucket C: 1000 PARQUET files, 1000 rows, 10 fields, total size 212Mb RDS Aurora MySQL A: 10 tables, 10000 rows, 10 columns, instance type: db.r5.large(8vCPUs, 64GiB RAM Network: 4,750 Mbps) Glue Crawler: 15.6085 Glue Job: 70.9935 86.602 Total monthly cost in the above three accounts with different frequency (CNY) 95.2622 <p>We recommend using the AWS Cost Explorer feature in the solution to help manage costs. Prices are subject to change. For full details, refer to the pricing webpage for each AWS service used in this solution. </p>"},{"location":"cost/#us-east-1-region-cost","title":"US East-1 Region Cost","text":"Service Usage Type Usage Quantity Monthly cost (USD) Amazon Relational Database Service for MySQL Community Edition USD 0.136 per db.t3.medium Multi-AZ instance hour (or partial hour) running MySQL 720 Hrs 576 Reserved Amazon Relational Database Service Provisioned Storage USD 0.23 per GB-month of provisioned gp2 storage for Multi-AZ deployments running MySQL 20 GB-Mo 30.616 On Demand Amazon Elastic Compute Cloud NatGateway USD 0.045 per GB Data Processed by NAT Gateways 30 GB-Mo 11.1 On Demand. Avg 80 url requests per day USD 0.045 per NAT Gateway Hour 720 Hrs 266.4 Reserved Athena CNY34.34 per Terabytes for DataScannedInTB in China (Ningxia) 0.010 Terabytes 0.34 On Demand CloudWatch First 5GB-mo per month of logs storage is free. CNY 0.244 per GB archived per month 0.100 GB-Mo 0.244 On Demand EC2 Container Registry (ECR) 500MB-month Free Tier, CNY 0.69 per GB-month 0.003 GB-Mo 0.69 Reserved Elastic Load Balancing - Application 0.0 CNY per Application LoadBalancer-hour (or partial hour) under monthly free tier. CNY 0.156 per Application load balancer-hour (or partial hour) 10 Hrs 1.56 On Demand 0.0 CNY per used Application load balancer capacity unit-hour (or partial hour) under monthly free tier. CNY 0.072 per LCU-hour (or partial hour) 0.105 LCU-Hrs 0 On Demand Lambda CNY 0 for first 400K GB-second usage of AWS Lambda - Total Compute - China (Ningxia), CNY 0.0000001135 Price per 1ms  (Ningxia) 100,000.000 Lambda-GB-Second 11.35 On Demand CNY 0 for first 1M usage of AWS Lambda - Total Requests - China (Ningxia), CNY 1.36 per million requests 10,000 Request 0 On Demand Simple Queue Service First 1,000,000 Amazon SQS Requests per month are free, CNY 3.33 (per Million requests) 100,000 Requests 3.33 On Demand Simple Storage Service First 2,000 PUTs free under free tier, CNY 0.00405 PUT, COPY, POST, LIST requests (per 1,000 requests) 2,000 Requests 0.0081 On Demand First 20,000 GETs free under free tier, CNY 0.0135 per 10,000 requests 4,000 Requests 0.0054 On Demand First 5 GB free under free tier, CNY 0.1755 per GB 0.032 GB-Mo CNY 0.00 0.1755 On Demand Total 901.819 Service Usage Type Glue $0.44 per DPU-Hour, billed per second, with a 10-minute minimum per crawler run On Demand $0.44 per Data Processing Unit-Hour for Amazon Glue ETL job On Demand $1 per 1,000,000 requests for Amazon Glue Data Catalog request On Demand Step Functions US 0.00 for first 4,000 state transitions, CNY 0.0002102 per state transition thereafter On Demand CloudWatch First 5GB-mo per month of logs storage is free. CNY 0.244 per GB archived per month On Demand"},{"location":"notices/","title":"Notices","text":"<p>Customers are responsible for making their own independent assessment of the information in this document. This document: (a) is for informational purposes only, (b) represents Amazon Web Services current product offerings and practices, which are subject to change without notice, and (c) does not create any commitments or assurances from Amazon Web Services and its affiliates, suppliers or licensors. Amazon Web Services products or services are provided \u201cas is\u201d without warranties, representations, or conditions of any kind, whether express or implied. Amazon Web Services responsibilities and liabilities to its customers are controlled by Amazon Web Services agreements, and this document is not part of, nor does it modify, any agreement between Amazon Web Services and its customers.</p> <p>The Sensitive Data Protection on AWS solution is licensed under the terms of the Apache License Version 2.0 available at The Apache Software Foundation.</p>"},{"location":"regions/","title":"Regional Deployment","text":"<p>This solution uses services which may not be currently available in all AWS Regions. Launch this solution in an AWS Region where required services are available. For the most current availability by Region, refer to the AWS Regional Services List.</p>"},{"location":"regions/#supported-regions-for-deployment-in-aws-global-regions","title":"Supported regions for deployment in AWS Global Regions","text":"Region Name Region ID US East (N. Virginia) Region us-east-1 US East (Ohio) Region us-east-2 US West (N. California) Region us-west-1 US West (Oregon) Region us-west-2 Asia Pacific (Mumbai) Region ap-south-1 Asia Pacific (Tokyo) Region ap-northeast-1 Asia Pacific (Seoul) Region ap-northeast-2 Asia Pacific (Singapore) Region ap-southeast-1 Asia Pacific (Sydney) Region ap-southeast-2 Canada (Central) Region ca-central-1 Europe (Ireland) Region eu-west-1 Europe (London) Region eu-west-2 Europe (Paris) Region eu-west-3 Europe (Frankfurt) Region eu-central-1 South America (Sao Paulo) Region sa-east-1"},{"location":"regions/#supported-regions-for-deployment-in-aws-china-regions","title":"Supported regions for deployment in AWS China Regions","text":"Region Name Region ID AWS China (Beijing) Region operated by Sinnet cn-north-1 AWS China (Ningxia) Region operated by NWCD cn-northwest-1"},{"location":"revisions/","title":"Revisions","text":"Date Change June 2023 Initial release"},{"location":"security/","title":"Security Information","text":"<p>When you build solutions on Amazon Web Services, security responsibilities are shared between you and Amazon Cloud. This Shared Responsibility Model reduces your operational burden due to the Amazon Web Services operations, management, and control components, including host operations The physical security of the system, the virtualization layer, and the facility where the service runs. For more information on Amazon Web Services, visit Amazon Web Services Cloud Security.</p>"},{"location":"security/#iam-roles","title":"IAM roles","text":"<p>AWS Identity and Access Management (IAM) roles allow customers to assign fine-grained access policies and permissions to services and users on AWS. This solution creates IAM roles that grant access between components of the solution.</p>"},{"location":"uninstall/","title":"Uninstall the solution","text":"<p>To uninstall the solution, you must delete the AWS CloudFormation stack. </p> <p>You can use either the AWS Management Console or the AWS Command Line Interface (AWS CLI) to delete the CloudFormation stack.</p>"},{"location":"uninstall/#uninstall-the-stack-using-the-aws-management-console","title":"Uninstall the stack using the AWS Management Console","text":"<ol> <li>Sign in to the AWS CloudFormation console.</li> <li>Select this solution\u2019s installation parent stack.</li> <li>Choose Delete.</li> </ol>"},{"location":"uninstall/#uninstall-the-stack-using-aws-command-line-interface","title":"Uninstall the stack using AWS Command Line Interface","text":"<p>Determine whether the AWS Command Line Interface (AWS CLI) is available in your environment. For installation instructions, refer to What Is the AWS Command Line Interface in the AWS CLI User Guide. After confirming that the AWS CLI is available, run the following command.</p> <pre><code>aws cloudformation delete-stack --stack-name &lt;installation-stack-name&gt; --region &lt;aws-region&gt;\n</code></pre>"},{"location":"architecture-overview/architecture-details/","title":"Architecture details","text":"<p>This section describes the components and AWS services that make up this solution and the high level system design.</p> <p> Sensitive Data Protection on AWS high level system design</p> <p>As shown in the diagram, the centralized sensitive data governance account is the admin account. Solution users, typically security auditors, can access the solution via a web portal. Users can browse the data catalog and execute sensitive data detection jobs in the monitored account(s) after logging into the web portal. </p> <p>Multiple monitored accounts are connected to the admin account with data source access and job execution privileges, so that the admin account can invoke the Job Processor model in the specified monitored account for sensitive data detection.</p>"},{"location":"architecture-overview/architecture-details/#modules-in-admin-account","title":"Modules in Admin Account","text":"<ul> <li> <p>Web Portal (UI): The solution administrator or normal users can access the solution through the web portal. It provides secure user access management and a web UI for the solution.</p> </li> <li> <p>Data Source Management (DSM): The DSM is responsible for retrieving the data sources from monitored accounts by the Data Source Detector and storing the data source structure. Users can explore the data storage in the monitored accounts, such as S3 buckets and RDS instances.</p> </li> <li> <p>Data Catalog Management (DCM): The DCM can discover the latest schema (normally called metadata) of the data sources in DSM. The schema includes information such as table columns in RDS databases and the sensitive data detection results after the detection job has run.</p> </li> <li> <p>Job Controller (JC): The Job Controller is responsible for executing the detection job in the monitored account and collecting the detection results back to the admin account. It can configure the job to run on a user-defined schedule or as needed.</p> </li> <li> <p>Template Configuration (TC): The detection templates are stored in the TC model. It contains built-in templates and custom-defined templates. The JC can retrieve the templates for running the job processor.</p> </li> <li> <p>Account Management (AM): The monitored AWS account(s) are managed by the AM model.</p> </li> </ul>"},{"location":"architecture-overview/architecture-details/#modules-in-monitored-accounts","title":"Modules in Monitored Account(s)","text":"<ul> <li> <p>Job Processor: The Job Processor is the running container for sensitive data detection, invoked by the Job Controller. The Job Processor reads the raw data to the detection engine for detection and sends the analysis results and running state to the Job Controller.</p> </li> <li> <p>Detection Engine: The Detection Engine model is the core sensitive data detection engine with AI/ML support features. It receives the data from the Job Processor to identify the sensitive data type using a pre-trained ML model or pattern.</p> </li> </ul>"},{"location":"architecture-overview/architecture/","title":"Architecture diagram","text":"<p>Deploying this solution with the default parameters builds the following environment in the AWS Cloud.</p> <p> Sensitive Data Protection on AWS architecture</p> <ol> <li>The Application Load Balancer distributes the solution's frontend web UI assets hosted in AWS Lambda. </li> <li>Identity provider for user authentication. </li> <li>The AWS Lambda function is packaged as Docker images and stored in the Amazon ECR (Elastic Container Registry). </li> <li>The backend Lambda function is a target for the Application Load Balancer. </li> <li>The backend Lambda function invokes AWS Step Functions in monitored accounts for sensitive data detection. </li> <li>In AWS Step Functions workflow, the AWS Glue Crawler runs to take inventory of the data sources and is stored in the Glue Database as metadata tables.</li> <li>The Step Functions send Amazon SQS messages to the detection job queue after the Glue job has run. </li> <li>Lambda function processs messages from Amazon SQS.</li> <li>The Amazon Athena query detection results and save to MySQL instance in Amazon RDS.</li> </ol> <p>The solution uses the AWS Glue service as a core for building data catalog in the monitored account(s) and for invoking the Glue Job to detect sensitive data Personal Identifiable Information (PII). The distributed Glue job runs in each monitored account, and the admin account contains a centralized data catalog of data sources across AWS accounts. This is an implementation of the Data Mesh concept recommended by AWS.</p> <p>To be more specific, the solution introduces an event-driven process and uses AWS IAM roles to trigger and communicate between the admin account and the monitored account(s) for sensitive data discovery jobs. The admin account can start PII detection jobs and retrieve data catalogs. All monitored AWS accounts are permitted to be connected to the admin account, which is able to distinguish and access the monitored accounts.</p>"},{"location":"architecture-overview/design-considerations/","title":"Design Considerations","text":"<p>This solution was designed with best practices from the AWS Well-Architected Framework which helps customers design and operate reliable, secure, efficient, and cost-effective workloads in the cloud. </p> <p>This section describes how the design principles and best practices of the Well-Architected Framework were applied when building this solution. </p> <p>By default, this solution does not include alarms dashboard in Amazon CloudWatch. You can setup alarms for CPU Usage, Load Balancer Latency or Excessive Throughput to monitor and receive alerts about your AWS resources and applications.</p>"},{"location":"architecture-overview/services-in-the-solution/","title":"AWS services in this solution","text":"<p>The following AWS services are included in this solution:</p> AWS service Description Application Load Balancer Core. To distribute the frontend web UI assets. Amazon ECR Core. To store Docker images. AWS Lambda Core. To serve as a target for the application load balancer. AWS Step Functions Supporting.\u00a0To be invoked for sensitive data detection. AWS Glue Supporting.\u00a0To take inventory of data sources. Amazon RDS Supporting. To xx. Amazon SQS Supporting.\u00a0To xx."},{"location":"deployment/deployment/","title":"Deployment","text":"<p>Before you launch the solution, review the architecture, supported regions, and other considerations discussed in this guide. Follow the step-by-step instructions in this section to configure and deploy the solution into your account.</p> <p>Time to deploy: Approximately 30 minutes</p>"},{"location":"deployment/deployment/#deployment-overview","title":"Deployment overview","text":"<p>Use the following steps to deploy this solution on AWS. </p> <ul> <li>Create a user pool and an OIDC application.</li> <li>Deploy the AWS CloudFormation Admin template into your AWS admin account.</li> <li>(optional)If your accounts are all under the AWS organization, please deploy IT template under the IT account. The AWS organization root account needs to first register the IT account as a delegated administrator.</li> <li>Deploy the AWS CloudFormation Agent template into your AWS account that need to be detected.</li> </ul>"},{"location":"deployment/deployment/#deployment-steps","title":"Deployment steps","text":""},{"location":"deployment/deployment/#create-a-user-pool-and-an-oidc-application","title":"Create a user pool and an OIDC application","text":"<p>You can use different kinds of OpenID Connector (OIDC) providers. This section introduces Option 1 to Option 3. - Option 1: Using Amazon Cognito as OIDC provider.</p> <ul> <li> <p>Option 2: Authing, which is an example of a third-party authentication provider.</p> </li> <li> <p>Option 3: OKTA, which is an example of a third-party authentication provider.</p> </li> </ul>"},{"location":"deployment/deployment/#option-1cognito","title":"Option 1:Cognito","text":"<p>You can leverage the Cognito User Pool in a supported AWS Region as the OIDC provider. 1. Go to the Amazon Cognito console in an AWS Standard Region.</p> <ol> <li> <p>Set up the hosted UI with the Amazon Cognito console based on this guide.</p> </li> <li> <p>Choose Public client when selecting the App type.Choose Don't generate a client secret when selecting Client secret. </p> </li> <li> <p>In Advanced app client settings,Selected OpenID,Email and Profile when setting OpenID Connect scopes. </p> </li> <li> <p>Confirm that the Hosted UI status is Available. Confirm that the OpenID Connect scopes includes email, openid, and profile. </p> </li> <li> <p>Save the App Client ID, User pool ID and the AWS Region to a file, which will be used later. In the next section Deploy admin stack, the Client ID is the App Client ID, and Issuer URL is <code>https://cognito-idp.${REGION}.amazonaws.com/${USER_POOL_ID}</code> </p> </li> </ol>"},{"location":"deployment/deployment/#option-2authing","title":"Option 2:Authing","text":"<ol> <li> <p>Go to the Authing console.</p> </li> <li> <p>On the left navigation bar, select Self-built App under Applications.</p> </li> <li> <p>Click the Create button.</p> </li> <li> <p>Enter the Application Name, and Subdomain.</p> </li> <li> <p>Save the App ID(that is, Client ID) and Issuer(Issuer URL) to a text file from Endpoint Information, which will be used later. </p> </li> <li> <p>Set the Authorization Configuration in Protocol Configuration tab. </p> </li> <li>On the Access Authorization tab, select the accessible users.</li> </ol>"},{"location":"deployment/deployment/#option-3okta","title":"Option 3:OKTA","text":"<ol> <li> <p>Go to the OKTA console.</p> </li> <li> <p>Click Applications \u2192 Create App Integration </p> </li> <li> <p>choose OIDC - OpenID Connect \u2192 choose Single-Page Application \u2192 Click Next </p> </li> <li> <p>At Controlled access, Choose the way that suits you. </p> </li> <li>Save the Client ID and Issuer URL to a text file from Endpoint Information, which will be used later. The Issuer URL is in your profile.The full Issuer URL is \u201chttps://dev-xxx.okta.com\u201d.  </li> </ol>"},{"location":"deployment/deployment/#deploy-admin-stack","title":"Deploy admin stack","text":"<ol> <li>Sign in to the AWS Management Console and use Global region template(New VPC) or China region template(New VPC) to launch the AWS CloudFormation template.</li> <li>To launch this solution in a different AWS Region, use the Region selector in the console navigation bar.</li> <li>On the Create stack page, verify that the correct template URL is shown in the Amazon S3 URL text box and choose Next.</li> <li>On the Specify stack details page, assign a valid and account level unique name to your solution stack.</li> <li> <p>Under Parameters, review the parameters for the template and modify them as necessary. This solution uses the following default values.</p> Parameter Default Description Issuer URL Specify the secure OpenID Connect URL. Maximum 255 characters. URL must begin with \"https://\" Client ID Specify the client ID issued by the identity provider. Maximum 255 characters. Use alphanumeric or ?:_.-/? characters Public Access Yes If you choose No, the portal website can be accessed ONLY in the VPC. If you want to access the portal website over Internet, you need to choose Yes Port 80 If an ACM certificate ARN has been added, we recommend using port 443 as the default port for HTTPS protocol. Otherwise, port 80 can be set as an alternative option ACM Certificate ARN (optional)To enable secure communication through encryption and enhancing the security of the solution, you can add a public certificate ARN from ACM to create the portal website URL based on the HTTPS protocol Custom Domain Name (optional)By adding your own domain name, such as sdps.example.com, you can directly access the portal website by adding a CNAME record to that domain name after deploying the stack.Only fill in the domain name, do not fill in http(s):// </li> <li> <p>Choose Next.</p> </li> <li>On the Configure stack options page, choose Next.</li> <li>On the Review page, review and confirm the settings. Select 3 checkboxes that I acknowledge.</li> <li>Choose Create stack to deploy the stack. Wait about 20mins to ensure that all related resource are created.  You can choose the \u201cResource\u201d and \u201cEvent\u201d tab to see the status of the stack.</li> <li>In the \u201cOutputs\u201d tab, you will see the portal URL and SigninRedirectUri. </li> </ol>"},{"location":"deployment/deployment/#configure-oidc-application","title":"Configure OIDC application","text":"<p>Copy the value of SigninRedirectUriHTTP(S) and config it into your OIDC application</p>"},{"location":"deployment/deployment/#option-1cognito_1","title":"Option 1:Cognito","text":"<p>Your user pools -&gt; App integration -&gt; Your App </p>"},{"location":"deployment/deployment/#option-2authing_1","title":"Option 2:Authing","text":""},{"location":"deployment/deployment/#option-3otka","title":"Option 3:OTKA","text":""},{"location":"deployment/deployment/#configure-custom-domain-name","title":"Configure custom domain name","text":"<p>If a custom domain name is filled in when creating the stack, set the CName of the custom domain name to LoadBalancerDnsNameHTTP(S) value on the output tab of CloudFormat.</p>"},{"location":"deployment/deployment/#open-the-solution-portal","title":"Open the solution portal","text":"<p>The portal is the value of PortalUrlHTTP(S) on the output tab of CloudFormat </p>"},{"location":"deployment/deployment/#deploy-agent-stack","title":"Deploy agent stack","text":"<p>The installation steps are the same as the Admin stack.</p> <p>China region agent template</p> <p>Global region agent template</p> <p>Fill in \u201cStack name\u201d  in step 1 page, and parse your admin account(12 digits) in \u201cAdminAccountId\u201d parameter.</p>"},{"location":"deployment/template/","title":"AWS CloudFormation template","text":"<p>To automate deployment, this solution uses the following AWS CloudFormation templates, which you can download before deployment:</p>"},{"location":"deployment/template/#global-region","title":"Global Region","text":"<ul> <li>Admin(New VPC):https://aws-gcr-solutions.s3.amazonaws.com/aws-sensitive-data-protection/1.0.0/default/Admin.template.json</li> <li>Admin(Exist VPC):https://aws-gcr-solutions.s3.amazonaws.com/aws-sensitive-data-protection/1.0.0/default/AdminExistVpc.template.json</li> <li>Agent:https://aws-gcr-solutions.s3.amazonaws.com/aws-sensitive-data-protection/1.0.0/default/Agent.template.json</li> <li>IT:https://aws-gcr-solutions.s3.amazonaws.com/aws-sensitive-data-protection/1.0.0/default/IT.template.json</li> </ul>"},{"location":"deployment/template/#china-region","title":"China Region","text":"<ul> <li>Admin(New VPC):https://aws-gcr-solutions.s3.cn-north-1.amazonaws.com.cn/aws-sensitive-data-protection/1.0.0/cn/Admin.template.json</li> <li>Admin(Exist VPC):https://aws-gcr-solutions.s3.cn-north-1.amazonaws.com.cn/aws-sensitive-data-protection/1.0.0/cn/AdminExistVpc.template.json</li> <li>Agent:https://aws-gcr-solutions.s3.cn-north-1.amazonaws.com.cn/aws-sensitive-data-protection/1.0.0/cn/Agent.template.json</li> <li>IT:https://aws-gcr-solutions.s3.cn-north-1.amazonaws.com.cn/aws-sensitive-data-protection/1.0.0/cn/IT.template.json</li> </ul>"},{"location":"developer-guide/source/","title":"Source code","text":"<p>Visit our GitHub repository to download the templates and scripts for this solution. The XX template is generated using the AWS Cloud Development Kit (CDK). Refer to the README.md file for additional information.</p>"},{"location":"plan-deployment/cost/","title":"Cost","text":"<p>You will be responsible for the cost of the AWS services used when running the solution. We recommend using the AWS Cost Explorer feature in the solution to help manage costs. Prices are subject to change. For full details, refer to the pricing webpage for each AWS service used in this solution.</p> <p>The cost for running the Sensitive Data Protection solution varies based on the following factors:</p> <ul> <li>Number and size of datasets</li> <li>Number and complexity of data structures</li> <li>Frequency of discovery job updates</li> </ul> <p>For example, a large datasets that is running discovery job with fully scan range will result in higher costs than smaller datasets with increased scan range and limited scan depth that are run on demand.</p>"},{"location":"plan-deployment/cost/#cost-breakdown","title":"Cost breakdown","text":"<p>As of June 2023, the cost for main AWS account running this solution with the default settings in China (Ningxia) Region Operated by NWCD is approximately \u00a5901.82 CNY a month.</p> <ul> <li>Base cost for Infra (in admin account)</li> </ul> Service Usage Type Usage Quantity Monthly cost (CNY) Billing Type Amazon Relational Database Service for MySQL Community Edition CNY 0.8 per db.t3.medium Multi-AZ instance hour (or partial hour) running MySQL 720 Hrs 576 Reserved Amazon Relational Database Service Provisioned Storage CNY 1.5308 per GB-month of provisioned gp2 storage for Multi-AZ deployments running MySQL 20 GB-Mo 30.616 On Demand Amazon Elastic Compute Cloud NatGateway 0.37 CNY per GB Data Processed by NAT Gateways 30 GB-Mo 11.1 On Demand Amazon Elastic Compute Cloud NatGateway 0.37 CNY per NAT Gateway Hour 720 Hrs 266.4 Reserved Athena CNY34.34 per Terabytes for DataScannedInTB in China (Ningxia) 0.010 Terabytes 0.34 On Demand CloudWatch First 5GB-mo per month of logs storage is free. CNY 0.244 per GB archived per month 0.100 GB-Mo 0.244 On Demand EC2 Container Registry (ECR) 500MB-month Free Tier, CNY 0.69 per GB-month 0.003 GB-Mo 0.69 Reserved Elastic Load Balancing - Application 0.0 CNY per Application LoadBalancer-hour (or partial hour) under monthly free tier. CNY 0.156 per Application load balancer-hour 10 Hrs 1.56 On Demand Elastic Load Balancing - Application 0.0 CNY per used Application load balancer capacity unit-hour (or partial hour) under monthly free tier. CNY 0.072 per LCU-hour 0.105 LCU-Hrs 0 On Demand Lambda CNY 0 for first 400K GB-second usage of AWS Lambda - Total Compute - China (Ningxia), CNY 0.0000001135 Price per 1ms (Ningxia) 100,000.000 Lambda-GB-Second 11.35 On Demand Lambda CNY 0 for first 1M usage of AWS Lambda - Total Requests - China (Ningxia), CNY 1.36 per million requests 10,000 Requests 0 On Demand Simple Queue Service First 1,000,000 Amazon SQS Requests per month are free, CNY 3.33 (per Million requests) 100,000 Requests 3.33 On Demand Simple Storage Service First 2,000 PUTs free under free tier, CNY 0.00405 PUT, COPY, POST, LIST requests (per 1,000 requests) 2,000 Requests 0.0081 On Demand Simple Storage Service First 20,000 GETs free under free tier, CNY 0.0135 per 10,000 requests 4,000 Requests 0.0054 On Demand Simple Storage Service First 5 GB free under free tier, CNY 0.1755 per GB 0.032 GB-Mo 0.1755 On Demand Simple Storage Service CNY 0.00 0.1755 On Demand Total 901.819 <ul> <li>Base cost for Infra (in monitored account)</li> </ul> <p>The AWS services listed in the example cost tables below are billed on a monthly basis in a monitored account. Glue Crawler minimum cost is 0.5035 (CNY), the crawler will be launched in data source connection creation and PII detection job execution, below cost table includes both of the connection creation and job execution cost with the example data sources.</p> Service Usage Type Billing Type Glue 3.021 CNY per DPU-Hour, billed per second, with a 10-minute minimum per crawler run On Demand Glue 3.021 CNY per Data Processing Unit-Hour for Amazon Glue ETL job On Demand Glue 6.866 CNY per 1,000,000 requests for Amazon Glue Data Catalog request On Demand Step Functions CNY 0.00 for first 4,000 state transitions, CNY 0.0002102 per state transition thereafter On Demand CloudWatch First 5GB-mo per month of logs storage is free. CNY 0.244 per GB archived per month On Demand"},{"location":"plan-deployment/cost/#example","title":"Example","text":"<p>Based on typical usage patterns, we have indicated a few scenarios to provide an estimation of monthly costs. The AWS services listed in the example cost tables below are billed on a monthly basis.</p> <p>Here is a cost estimate with sample dataset:</p> Scenarios Service Running Cost (CNY) Monthly Cost (CNY) Account 1, scan frequency monthly, scan depth 1000 S3 Bucket A: 10000 CSV files, 1000 rows, 10 columns, total size 1.7GiBS3 Bucket B: 10000 JSON files, 1000 rows, 10 fields, total size 2.5GiBS3 Bucket C: 1000 PARQUET files, 1000 rows, 10 fields, total size 212Mb Glue Crawler: 1.007Glue Job: 2.3161 3.3231 Account 2, scan frequency weekly, scan depth 1000 RDS Aurora MySQL A: 10 tables, 10000 rows, 10 columns, instance type: db.r5.large(8vCPUs, 64GiB RAM Network: 4,750 Mbps)RDS MySQL B: 10 tables, 1,000,000 rows, 10 columns, instance type: db.m5.12xlarge(48 vCPU 192 GiB RAM Network:9500 Mbps) Glue Crawler: 2.5175Glue Job: 2.8196 5.3371 Account 3, scan frequency daily, scan depth 1000 S3 Bucket A: 10000 CSV files, 1000 rows, 10 columns, total size 1.7GiBS3 Bucket B: 10000 JSON files, 1000 rows, 10 fields, total size 2.5GiBS3 Bucket C: 1000 PARQUET files, 1000 rows, 10 fields, total size 212MbRDS Aurora MySQL A: 10 tables, 10000 rows, 10 columns, instance type: db.r5.large(8vCPUs, 64GiB RAM Network: 4,750 Mbps) Glue Crawler: 15.6085Glue Job: 70.9935 86.602 Total monthly cost in the above three accounts with different frequency (CNY) 95.2622"},{"location":"plan-deployment/regions/","title":"Supported regions","text":"<p>As of June 2023, this solution is supported in the following Amazon Web Services Regions:</p> Region ID Region Name us-east-1 US East (N. Virginia) us-east-2 US East (Ohio) us-west-2 US West (Oregon) ca-central-1 Canada sa-east-1 South America (Sao Paulo) eu-west-1 Europe (Ireland) eu-west-2 Europe (London) eu-west-3 Europe (Paris) eu-central-1 Europe (Frankfurt) ap-northeast-1 Asia Pacific (Tokyo) ap-northeast-2 Asia Pacific (Seoul) ap-southeast-1 Asia Pacific (Singapore) ap-southeast-2 Asia Pacific (Sydney) ap-south-1 Asia Pacific (Mumbai) cn-northwest-1 China (Ningxia) Region operated by NWCD cn-north-1 China (Beijing) Region operated by Sinnet"},{"location":"plan-deployment/security/","title":"Security","text":"<p>When you build systems on AWS infrastructure, security responsibilities are shared between you and AWS. This shared responsibility model reduces your operational burden because AWS operates, manages, and controls the components including the host operating system, the virtualization layer, and the physical security of the facilities in which the services operate. For more information about AWS security, visit AWS Cloud Security.</p>"},{"location":"plan-deployment/security/#iam-roles","title":"IAM roles","text":"<p>AWS Identity and Access Management (IAM) roles allow customers to assign granular access policies and permissions to services and users on the AWS Cloud. This solution creates IAM roles that grant the solution\u2019s access between the solution components.</p>"},{"location":"plan-deployment/security/#security-groups","title":"Security groups","text":"<p>The security groups created in this solution are designed to control and isolate network traffic between the solution components. We recommend that you review the security groups and further restrict access as needed once the deployment is up and running.</p>"},{"location":"solution-overview/concepts-and-definitions/","title":"Concepts and definitions","text":"<ul> <li>Data source: AWS resources where data is stored, such as Amazon S3, and Amazon RDS.</li> <li>Data catalog: A repository of metadata of a data source, allowing you to manage data at the column level. For example, you can view the table schema, sample data of a particular column, and add labels to specific data fields.</li> <li>Data identifier: The rule used to detect data. You can define custom data identifiers using RegEx and keywords.</li> <li>Classification template: A collection of data identifiers. Data identifiers are rules used to detect data.</li> <li>Sensitive data discovery job: A job that uses a template to detect sensitive data. The job automatically labels sensitive data in the data catalog.</li> <li>Glue job: A job that is triggered by a sensitive data discovery job to scan sensitive data using AWS Glue. One discovery job can trigger AWS Glue jobs in multiple AWS accounts in a distributed manner.</li> </ul>"},{"location":"solution-overview/features-and-benefits/","title":"Features and benefits","text":"<p>The solution includes the following features:</p> <p>Data discovery: supports various data sources, such as Amazon S3 and Amazon RDS, across multiple AWS accounts. The solution allows you to easily create a data catalog and run sensitive data discovery jobs. The solution leverages not only pattern-based discovery but also ML classification based on deep learning Natural Language Processing (NLP) and Named Entity Recognition (NER).</p> <p>Flexible data classification: defines data classification templates for detecting privacy data, such as personal information. The solution allows you to define custom sensitive data types or choose from over 200 built-in data types.</p> <p>Centralized data visualization: provides the dashboards with an overview of the data catalogs and sensitive data status, such as data location, and data sources.</p>"},{"location":"user-guide/appendix-built-in-identifiers/","title":"Appendix - Built-in data identifiers","text":""},{"location":"user-guide/appendix-organization/","title":"Add accounts via AWS Organization","text":"<p>You can use AWS Organizations to manage automated deployment of monitored accounts. In AWS CloudFormation, you can configure StackSet to deploy the Agent stack in the target Organizational Unit (OU). After you have configured the deployment, the Agent stack will be automatically deployed to the specified region of the account under the OU. Finally, you need to deploy the IT stack to the Organizations management account or the corresponding CloudFormation delegated account under Organizations, then, you can add member accounts via Organizations.</p>"},{"location":"user-guide/appendix-organization/#concepts","title":"Concepts","text":"<ol> <li>Deploy Admin CloudFormation stack in adminaccount</li> <li>Register delegated administrator in StackSets in Organization\u2019s management account. Register a delegated administrator</li> <li>Deploy IT CloudFormation Stack</li> <li>Create role for SDPS Admin API</li> <li>Create StackSet for Agent CloudFormation Stack</li> <li>Deploy to Organization/OU(s)</li> <li>Add member account via Organizations</li> <li>Retrieve deployment stacks and member accounts</li> </ol>"},{"location":"user-guide/appendix-permissions/","title":"Appendix: Permissions for agent CloudFormation stack","text":"<p>The solution follows the least privilege principle to only create needed permissions. High level concept for permission:</p>"},{"location":"user-guide/appendix-permissions/#solution-stack","title":"Solution Stack","text":""},{"location":"user-guide/appendix-permissions/#agent-stack","title":"Agent Stack","text":"<p>The Agent CloudFormation Stack automatically set AWS roles and policies to read data sources and setup necessary permissions in target accounts to authorize SDPS. </p> <ul> <li>(Data source) Amazon S3: read only permission for data source scanning.</li> <li>(Data source) Amazon RDS: read only permission for data source scanning. </li> <li>AWS SecretsManager: read only permission. If RDS database is secured with Secrets, we will read credentials from Secret Manager.</li> <li>AWS Glue: write permission. Glue data catalog, Glue crawler, Glue job are used. Glue is triggered by Step Functions.</li> <li>AWS StepFunctions: resource created. Step Function is used to orchestrate Glue jobs for data discovery.</li> <li>AWS Lambda: resource created.</li> <li>Amazon CloudWatch: write permission. Lambda logs will be stored in Cloudwatch. Note: AWS will provide the file of detailed IAM policy. [!!! TODO]</li> </ul>"},{"location":"user-guide/appendix-permissions/#organization-stack","title":"Organization Stack","text":""},{"location":"user-guide/appendix-supported-data-types/","title":"Appendix: Supported data types:","text":"<ul> <li> <p>Structured data and Semi-structured data are supported.The solution uses AWS Glue to crawl these data into data catalogs. For specific data format supported by AWS Glue, please refer to Built-in classifiers in AWS Glue.</p> </li> <li> <p>Unstructured data (such as Image and PDF) is not supported.</p> </li> </ul>"},{"location":"user-guide/data-catalog-create/","title":"Connect to data source","text":"<p>After onboarding AWS accounts, you can create data catalogs for the data source.</p>"},{"location":"user-guide/data-catalog-create/#create-data-catalogs-for-s3-manual","title":"Create data catalogs for S3 (manual)","text":"<ol> <li>On the Connect to data source page, click one account to open its details page.</li> <li>In the Amazon S3 tab, view a list of S3 buckets in the region where the solution is deployed. </li> <li>Select a S3 bucket, and choose Connect to create data catalog. </li> </ol> <p>After several minutes, you can see Catalog status is <code>ACTIVE</code>, which indicates data catalogs are created for the S3 bucket. </p> <p>You can also choose Connect All to quickly create data catalogs for all S3 buckets of this AWS account.</p>"},{"location":"user-guide/data-catalog-create/#create-data-catalogs-for-s3-automatic","title":"Create data catalogs for S3 (automatic)","text":"<p>In case you have no knowledge about the content stored in data source, you can skip the manual catalog creation step, and rely on sensitive data discovery job to create the data catalogs for you.</p> <p>When creating the job, you can choose either All buckets in all AWS accounts or All buckets in specific AWS accounts as the target for scanning. It will automatically create data catalogs at AWS account level (not for specific bucket). For more details, please refer to sensitive-data-discovery-job.</p>"},{"location":"user-guide/data-catalog-create/#create-data-catalogs-for-rds-manual","title":"Create data catalogs for RDS (manual)","text":"<p>Important<p>A RDS instance must meet one of the conditions to be successfully connected:</p> <ul> <li>It has VPC NAT Gateway.</li> <li>It has both VPC Endpoints for S3 and Glue Endpoint.</li> </ul> </p> <ol> <li>On the Connect to data source page, click one account to open its details page.</li> <li>Choose the Amazon RDS tab. You can see a list of RDS instances in the region where the solution is deployed. </li> <li> <p>Select a RDS instance, and choose Connect to open a pop-up window asking for credentials. There are two options to enter the credentials:</p> <ul> <li>Choose Username/Password and enter the username and password of the RDS instance.</li> <li>Choose Secret Manager and select the Secret of the RDS. It will list all the Secrets in Secret Manager of the same account of the RDS.</li> </ul> </li> <li> <p>Choose Connect. The solution will start testing connection, and it could take several minutes.</p> </li> </ol> <p>Once you see catalog status is <code>Active</code>, it indicates that data catalogs are created for the RDS instance. </p>"},{"location":"user-guide/data-catalog-delete/","title":"Delete data catalogs","text":"<p>You can delete data catalogs if you do not need them any more.</p>"},{"location":"user-guide/data-catalog-delete/#delete-data-catalogs-for-s3","title":"Delete data catalogs for S3","text":"<ol> <li>On the Connect to data source page, click one account to open its details page.</li> <li>In the S3 tab, select an S3 bucket, and choose Disconnect from the Actions list.</li> </ol>"},{"location":"user-guide/data-catalog-delete/#delete-data-catalogs-for-rds","title":"Delete data catalogs for RDS","text":"<ol> <li>On the Connect to data source page, click one account to open its details page.</li> <li>Choose the Amazon RDS tab.</li> <li>Select a RDS instance, and choose Disconnect from the Actions list.</li> </ol>"},{"location":"user-guide/data-catalog-labels/","title":"Label data catalog","text":"<p>Data catalog provides metadata for your data source. You can add/update labels for it to give more information of the metadata.</p>"},{"location":"user-guide/data-catalog-labels/#sensitive-data-labeling-automatic-or-manual","title":"Sensitive data labeling (automatic or manual)","text":"<p>After sensitive data job is completed, the \"Privacy field\" will be automatically tagged based on job result. Column-level data in data catalogs will be labeled with data identifiers.</p> <p>You can always manually update the Privacy field in data catalog. </p> <p>In the Browse data catalogs page: </p> <ul> <li>In the S3 tab, either on Bucket or Folder level, you can click the  to select Privacy label.</li> <li>In the RDS tab, either on Instance or Table level, you can click the  to select Privacy label.</li> </ul>"},{"location":"user-guide/data-catalog-labels/#custom-labeling-manual","title":"Custom labeling (manual)","text":"<p>You can use \"Custom label\" field in data catalog to add business related label (for example, line of business, department, team, etc). </p> <p>In the Browse data catalogs page: </p> <ul> <li>In S3 tab, either on Bucket or Folder level, you can click the  to select Custom label from dropdown list.</li> <li>In RDS tab, either on Instance or Table level, you can click the  to select Custom label from dropdown list. </li> </ul> <p>At the bottom of dropdown list, click Manage custom label link, there will be a pop-up window, in which you can Add/Edit/Delete a custom label. </p>"},{"location":"user-guide/data-catalog-sync/","title":"Synchronize data catalogs","text":""},{"location":"user-guide/data-catalog-sync/#what-is-data-catalog","title":"What is data catalog?","text":"<p>A data catalog is a repository of metadata of data source (S3, RDS). With data catalogs, you can view the column-level information of data. </p> <p>You can refer to Appendix: Supported data types for more details.</p>"},{"location":"user-guide/data-catalog-sync/#when-are-the-data-catalogs-synchronized-with-data-source","title":"When are the data catalogs synchronized with data source?","text":"<p>The solution synchronizes the data catalogs with data source in the following situations:</p> <ul> <li>when data catalog is created</li> <li>when sensitive data discovery job runs</li> </ul> AWS resource Data source change Synchronize when data catalog is created Synchronize when sensitive data discovery job runs S3 bucket created Y Y- Autosync S3 bucket deleted Y Y- Autosync S3 object created Y ? S3 object deleted Y ? S3 object updated (timestamp changed) Y Y RDS instance created Y Y- Autosync RDS instance deleted Y Y- Autosync RDS database created Y ? RDS database deleted Y ? RDS table created Y ? RDS table deleted Y ? RDS table updated Y Y <p>Note</p> <p>Synchronizing data catalog will not affect the label on an existing data catalog.</p>"},{"location":"user-guide/data-classification-template/","title":"Data classification template","text":"<p>A template is a collection of data identifiers. You can define sensitive data in the template by adding data identifiers. The template will be used in sensitive data discovery job. </p>"},{"location":"user-guide/data-classification-template/#add-data-identifier-to-template","title":"Add data identifier to template","text":"<ol> <li>Sign in to the solution's web portal.</li> <li>Choose Define classification template in the Summary area. Alternatively, from the left navigation pane, choose Define classification template under Classification settings to open the Define classification template page.</li> <li>Choose Add data identifier. You will see a list of data identifiers in window. </li> <li>Select one or multiple data identifiers and choose Add to template. </li> </ol>"},{"location":"user-guide/data-classification-template/#remove-data-identifier-from-template","title":"Remove data identifier from template","text":"<p>On the Define classification template page, select a data identifier in the template and choose Remove. </p>"},{"location":"user-guide/data-classification-template/#enabledisable-data-identifier","title":"Enable/disable data identifier","text":"<p>On the Define classification template page, for each identifier in the template, you can choose to enable or disable it. If you disable a data identifier, the related job will not detect data against the identifier. Disabling an identifier is usually for testing purposes.</p>"},{"location":"user-guide/data-identifiers/","title":"Manage data identifiers","text":"<p>A data identifier is a rule to detect sensitive data. The solution allows you to manage all the identifiers. You can view the built-in identifiers or create custom identifiers with RegEx or keywords.</p>"},{"location":"user-guide/data-identifiers/#view-built-in-data-identifiers","title":"View built-in data identifiers","text":"<p>In the Built-in data identifiers tab, you can see a list of built-in data identifiers. For a full list, please see Appendix - Built-in data identifiers.</p> <p>Note</p> <p>Some built-in data identifiers, such as person names and addresses, are defined based on AI, while others are defined based on Regex/keywords.</p>"},{"location":"user-guide/data-identifiers/#view-custom-data-identifiers","title":"View custom data identifiers","text":"<p>In the Custom data identifiers tab, you can see a list of custom data identifiers that you defined. The list is empty by default.</p>"},{"location":"user-guide/data-identifiers/#create-custom-data-identifier","title":"Create custom data identifier","text":"<p>In the Custom data identifiers tab, choose Create data identifier. </p>"},{"location":"user-guide/data-identifiers/#delete-custom-data-identifier","title":"Delete custom data identifier","text":"<p>In the Custom data identifiers tab, select a custom data identifier, choose Delete.</p>"},{"location":"user-guide/data-source/","title":"Connect to data source","text":"<p>The first step for using Sensitive Data Protection is to onboard AWS accounts. To do so, you need to deploy an \"Agent CloudFormation Stack\" in the AWS account.</p> <p>For the permissions required by the agent stack, refer to Appendix: Permissions for agent Cloudformation stack.</p>"},{"location":"user-guide/data-source/#add-aws-accounts-individually","title":"Add AWS accounts (individually)","text":"<ol> <li>Sign in to the solution's web portal.</li> <li>Choose Connect to data source in the Summary area. Alternatively, choose Connect to data source from the left navigation pane. </li> <li>Choose Add new account(s) to open the Individual account tab.</li> <li>Follow the instructions in Step1 and Step2 to install the Agent CloudFormation Stack.</li> <li>After successfully deploying the stack, fill in the account ID of the AWS account.</li> <li>Choose Add this account.</li> <li>Go back to the Connect to data source page. You will see that the AWS account has been added. </li> </ol> <p>You can also click one specific AWS account ID to check account details.</p>"},{"location":"user-guide/data-source/#add-aws-accounts-via-organization","title":"Add AWS accounts (via Organization)","text":"<p>For multiple AWS accounts, you can use AWS Organization to automatically install and uninstall the agent CloudFormation stacks. For more details, refer to Appendix: Add AWS accounts via Organization.</p> <p>After successfully deploying the stack, fill in the account ID of the Organization delegator account.</p> <p>Go back to the Connect to data source page, and you will see a list of AWS accounts added. You can click one specific AWS account ID to check account details.</p>"},{"location":"user-guide/discovery-job-create/","title":"Create job","text":"<p>You can create and manage jobs for detecting sensitive data. A discovery job consists of one or many AWS Glue jobs for actual data detection. For more information, refer to View job details.</p>"},{"location":"user-guide/discovery-job-create/#create-a-discovery-job","title":"Create a discovery job","text":"<ol> <li>Sign in to the solution's web portal.</li> <li>Choose Run sensitive data discovery jobs in the Summary area. Alternatively, from the left navigation pane, choose Run sensitive data discovery jobs to open its page. </li> <li> <p>Choose Create sensitive data discovery job. You'll need to go through the following steps to create a new discovery job.</p> <ul> <li>Step 1: Select the S3 data source</li> <li>Step 2: Select the RDS data source</li> <li>Step 3: Job settings (see the section Job setting details)</li> <li>Step 4: Job preview</li> </ul> </li> <li> <p>After previewing the job, choose Run job.</p> </li> </ol>"},{"location":"user-guide/discovery-job-create/#job-setting-details","title":"Job setting details","text":"Job setting Description Options Scan frequency Refers to the scan frequency of the discovery job. On-demand run Daily Weekly Monthly Scan depth Refers to the number of sampled rows. 1000(recommended) 100 Scan range Defines the overall scan range for the target data source.  \"Full scan\" means to scan all target data sources.\"Incremental scan\" means to skip those data sources that were not changed since the last data catalog update. Full scan Incremental scan (recommended) Detection threshold Defines the level of tolerance required for the job. If the scan depth is 1000 rows, a 10% threshold means that if over 100 rows (out of 1000) match the identifier rule, then the column will be labeled as sensitive. A lower threshold indicates that the job is less tolerant of sensitive data. 10% (recommended) 20% 30% 40% 50% 100% Override privacy labels that are updated manually Choose whether to allow the job to override the data catalog privacy label with the job result. Do not override (recommended) Override"},{"location":"user-guide/discovery-job-details/","title":"View job details","text":"<p>A sensitive data discovery job consists of Glue jobs that run in monitored AWS accounts (the same account as the data source). </p> <p>For instance, if you run a discovery job for a RDS instance (the data source) in Account A, and the solution is installed in Account B, the Glue job runs in Account A and returns results and masked sample data to Account B.</p>"},{"location":"user-guide/discovery-job-details/#view-job-details","title":"View job details","text":"<ol> <li>Sign in to the solution's web portal.</li> <li>Choose Run sensitive data discovery jobs in the Summary area. Alternatively, from the left navigation pane, choose Run sensitive data discovery jobs to open its page. </li> <li>Click the job that you want to view details. A window pops up. </li> <li>In the Job history tab, choose a specific job. If needed, you can choose Download report to download a report. For details, refer to discovery job report.</li> <li> <p>Click Job run details. You will be redirected to job details page, where you can see the job information and a list of Glue jobs. </p> <p>Note</p> <p>In case a Glue job failed, you can click the FAILED status to view its error log.</p> </li> </ol>"},{"location":"user-guide/discovery-job-details/#download-snapshot-of-classification-template-used-in-job","title":"Download snapshot of classification template used in job","text":"<p>You can download the template snapshot for the moment when the job starts to run. The snapshot shows what the job is using as data identifiers. </p> <p>On the job details page, choose Download snapshot to download the template snapshot in JSON format(.json). </p>"},{"location":"user-guide/discovery-job-pause-and-cancel/","title":"Pause/Continue job","text":"<p>You can only pause or resume a scheduled job. It does NOT mean to pause or resume a running discovery job.</p> <p>To pause a scheduled job, on the Run Sensitive Data Discovery Jobs page, click Actions and select Pause. For instance, if you scheduled a monthly job on the first day of every month and ran a job once in January, choosing Pause will prevent the discovery job from executing in February.</p> <p>To resume a paused job, on the Run Sensitive Data Discovery Jobs page,click Actions and select Continue.</p>"},{"location":"user-guide/discovery-job-report/","title":"Download job report","text":"<p>On the Run sensitive data discovery job page, click into a specific job to open a window. In the Job history tab, choose a specific job, and choose Download report to download report in Excel format (.xlsx). </p> <p>In the report, each data source is in a separate sheet (tab). For example, a sensitive data discovery job runs for S3 and RDS:</p> <p>The S3 sheet is:</p> account_id region s3_bucket s3_location column_name identifiers sample_data 177416885226 cn-northwest-1 sdps-beta-member s3://sdps-beta-member/ col1 [{identifier=CHINESE-NAME, score=0.6680557137733704}] [cn_*, \u9b4f, \u6881, , , , \u5c39, \u6c49, , \u9c81*] 177416885226 cn-northwest-1 sdps-beta-member s3://sdps-beta-member/ col2 [{identifier=ADDRESS, score=0.6929563446207209}] [cn_*, , \u6d77\u5357\u7701\u7701\u76f4\u8f96\u53bf**, \u6e56\u5317\u7701\u6b66\u6c49\u5e02**, , \u8d35\u5dde\u7701\u516d\u76d8*, \u6e56\u5317\u7701\u968f\u5dde\u5e02**, \u4e0a\u6d77\u5e02\u5e02\u8f96\u533a\u5609**, \u5c71\u897f\u7701\u5ffb\u5dde*, \u8d35\u5dde\u7701\u94dc\u4ec1\u5e02****] <p>The RDS sheet is:</p> account_id region rds_instance_id table_name column_name identifiers sample_data 640463273335 cn-northwest-1 db-instance-2 orderpaymentdb_orderpaymenttable cn_bank_card [{identifier=BANK-CARD-NUMBER, score=0.5269872423945045}] [62426**, 62061*, 627582**, 625612*, 624299**, 3462**, 627119*, 426927*, 620930**, 62385**] 640463273335 cn-northwest-1 db-instance-1-instance-1 shipmenttrackingdb_shipmenttable cn_car_license [{identifier=ADDRESS, score=0.10789832248611221}, {identifier=NUMBER-PLATE, score=1.0}] [\u6d25JE*, \u743cWM, \u8c6bRU, \u85cfGK, \u9ed1YR, \u6caaVK, \u4eacR7, \u664bD3, \u82cfT *, \u6e1dC1*]"},{"location":"user-guide/discovery-job-rerun-and-duplicate/","title":"Re-run/Duplicate job","text":""},{"location":"user-guide/discovery-job-rerun-and-duplicate/#re-run-a-discovery-job","title":"Re-run a discovery Job","text":"<p>On the Run Sensitive Data Discovery Jobs page, click Actions and select Execute once. You can create a new discovery job and run it with the same settings as the previous run.</p>"},{"location":"user-guide/discovery-job-rerun-and-duplicate/#duplicate-a-discovery-job","title":"Duplicate a Discovery Job","text":"<p>On the Run Sensitive Data Discovery Jobs page, click Actions and select Duplicate. You can duplicate a job setting and modify it to start a new job.</p>"},{"location":"user-guide/get-started/","title":"Overview","text":"<p>Once successfully deployed the solution, you can access the web portal to detect sensitive data. </p>"},{"location":"user-guide/get-started/#steps","title":"Steps","text":"<ul> <li>Step 1: Connect to data source. Add AWS accounts and create data catalogs.</li> <li>Step 2: Define classification template. Define sensitive data in templates by managing data identifiers.</li> <li>Step 3: Create and run sensitive data discovery jobs. Detect sensitive data by creating and managing data discovery jobs.</li> <li>Step 4: Browse data catalogs</li> <li>Step 5: Check result in dashboard</li> </ul>"},{"location":"user-guide/sensitive-data-discovery-job/","title":"Sensitive Data Discovery Job","text":"<p>On the \"Run Sensitive Data Discovery Jobs\" webpage, you can create and manage jobs for discovering sensitive data. A discovery job consists of one or many AWS Glue jobs for actual data inspection.</p> <p>Note: The Glue jobs run in the same account as the data source. For instance, if you run a discovery job against an RDS instance (the data source) in Account A, and the solution is installed in Account B, the Glue job runs in Account A and returns results and masked sample data to Account B.</p>"},{"location":"user-guide/sensitive-data-discovery-job/#create-a-discovery-job","title":"Create a discovery Job","text":"<p>Click the \"Create sensitive data discovery job\" button. You'll need to go through the job settings to run a new discovery job.</p> <ul> <li>Step 1: Select the S3 data source<ul> <li>All buckets in all AWS accounts</li> <li>All buckets in specific AWS accounts</li> <li>All buckets with created catalogs</li> <li>Specific buckets with created catalogs</li> <li>Skip scanning for S3</li> </ul> </li> <li>Step 2: Select the RDS data source<ul> <li>All RDS instances with created catalogs</li> <li>Specific instances with created catalogs</li> <li>Skip scanning for RDS</li> </ul> </li> <li>Step 3: Job settings</li> <li>Step 4: Job preview</li> </ul> <p>After previewing the job, click \"Run job\" button to start.</p>"},{"location":"user-guide/sensitive-data-discovery-job/#re-run-a-discovery-job","title":"Re-run a discovery Job","text":"<p>Click \"Actions\" and select \"Execute once\". You can create a new discovery job and run it with the same settings as the previous run.</p>"},{"location":"user-guide/sensitive-data-discovery-job/#pausecontinue-a-discovery-job","title":"Pause/Continue a discovery Job","text":"<p>\"Pause\"/\"Continue\" can only be applied to a scheduled job. It does NOT mean to stop/resume a running discovery job.</p> <p>To pause a scheduled job, click \"Actions\" and select \"Pause\". For instance, if you scheduled a monthly job on the first day of every month and ran a job once in January, choosing \"Pause\" will prevent the discovery job from executing in February.</p> <p>To resume a paused job, click \"Actions\" and select \"Continue\"**.</p>"},{"location":"user-guide/sensitive-data-discovery-job/#duplicate-a-discovery-job","title":"Duplicate a Discovery Job","text":"<p>Click \"Actions\" and select \"Duplicate\". You can duplicate a job setting and modify it to start a new job.</p>"},{"location":"user-guide/sensitive-data-discovery-job/#view-job-details","title":"View job details","text":""}]}