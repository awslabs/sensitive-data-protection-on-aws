{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>The Sensitive Data Protection on AWS solution allows enterprise customers to create data catalogs, discover, protect, and visualize sensitive data across multiple AWS accounts. The solution eliminates the need for manual tagging to track sensitive data such as Personal Identifiable Information (PII) and classified information. </p> <p>The solution provides an automated approach to data protection with a self-service web application. You can perform regular or on-demand sensitive data discovery jobs using your own data classification templates. Moreover, you can access metrics such as the total number of sensitive data entries stored in all your AWS accounts, which accounts contain the most sensitive data, and the data source\u00a0where the sensitive data is located. </p> <p></p> <p>The solution helps enterprise customers (such as companies with security or big data businesses) to implement the following data protection measures: </p> <ul> <li>centralized management over hundreds of AWS accounts</li> <li>automatic discovery of data assets</li> <li>sensitive data detection and automatic labeling</li> <li>integration with other AWS services or application</li> </ul> <p>This guide provides an overview of the solution, its reference architecture and components, considerations for planning the deployment, configuration steps for deploying the solution to the Amazon Web Services (AWS) Cloud. </p> <p>Use this navigation table to quickly find answers to these questions:</p> If you want to \u2026 Read\u2026 Know the cost for running this solution Cost Understand the security considerations for this solution Security Know which AWS Regions are supported for this solution Supported AWS Regions View or download the AWS CloudFormation template included in this solution to automatically deploy the infrastructure resources (the \u201cstack\u201d) for this solution AWS CloudFormation templates <p>The guide is intended for IT architects, developers, DevOps, data engineers with practical experience architecting on the AWS Cloud.</p> <p>This is a technical solution. You will be solely responsible for your compliance with all applicable laws and regulations on data protection.</p>"},{"location":"about-premium-edition/","title":"About Premium Edition","text":"<p>Sensitive Data Protection Premium Edition (SDP-PE) is a solution offered by AWS Professional Services (the Greater China Regions). It includes not only the features of SDP (this open-source project), but also features provided by AWS Professional Services.</p> <p>The features of SDP-PE include, but are not limited to, the following:</p>"},{"location":"about-premium-edition/#data-inspection","title":"Data inspection","text":"<ul> <li>Industry-specific classification templates (e.g. for automobile industry)</li> <li>Document inspection (PDF, TXT, etc.)</li> <li>Image inspection (face recognition, OCR, car license, etc.)</li> </ul>"},{"location":"about-premium-edition/#data-masking","title":"Data masking","text":"<ul> <li>Data masking rule configuration</li> <li>Data masking</li> <li>Image masking (face recognition, OCR, car license, etc.)</li> <li>Document data masking (PDF, TXT, etc.)</li> </ul>"},{"location":"about-premium-edition/#auditing-and-reporting","title":"Auditing and reporting","text":"<ul> <li>Auditing all API calls (data inspection, data masking) through the solution </li> <li>Viewing dashboard and downloading reports for all records of API calls</li> </ul> <p>For more details, please contact AWS Sales (the Greater China Regions) for further information and a price quote.</p>"},{"location":"contributors/","title":"Contributors","text":"<ul> <li>Chen, Haiyun</li> <li>Cui, Hubin</li> <li>Gu, George</li> <li>Hao, Liang</li> <li>Han, Xu</li> <li>Ji, Junxiang</li> <li>Jia, Ting</li> <li>Li, Xiujuan</li> <li>Lv, Ning</li> <li>Qin, Dehua</li> <li>Su, Fan</li> <li>Wang, Yu</li> <li>Yi, Ke </li> <li>Yi, Yan</li> <li>Zhang, Junzhong</li> </ul>"},{"location":"faq/","title":"FAQ","text":"<p>In this section, we listed frequently asked questions regarding solution architecture, technical design consideration, user guide details, etc. For specific error messages shown on UI, please refer to the Troubleshooting section.</p>"},{"location":"faq/#about-solution-installationupgradeuninstallation","title":"About Solution Installation/Upgrade/Uninstallation","text":"<p>After installing the solution, it uses an ALB address. Can I use a custom domain name?</p> <p>Yes, you can configure a custom domain name (such as a company's second-level domain) by configuring DNS. Specifically, you need to ask the DNS administrator to point the CName of the custom domain to the ALB address, and then update the parameters of Admin CloudFormation by filling in the custom domain name.</p> <p>Why does the Admin CloudFormation of the solution need a NAT Gateway in the VPC?</p> <p>The Admin program is in Lambda, which has no public IP. Lambda needs to access services such as S3, StepFunction, and Glue, so a NAT Gateway is needed.</p>"},{"location":"faq/#about-connecting-to-aws-accounts-and-data-sources","title":"About Connecting to AWS Accounts and Data Sources","text":"<p>What data types/file types does the solution support? Is there a specific list?</p> <p>The solution currently supports structured/semi-structured data mainly using the native capabilities of AWS Glue. For a specific list, please refer to Built-in classifiers in AWS Glue.</p> <p>I want to scan an RDS instance that is publicly accessible. How do I operate it?</p> <p>By default, the solution does not support scanning databases that are publicly accessible because production databases are usually not publicly accessible. To do so, you need to change the solution code. </p> <p>Open the Service.py file in the Lambda function with prefix <code>Sdp-admin</code> in the Lambda console and comment out the following code. This code change will allows the SDPS solution to access publicly accessible databases.</p> <pre><code>```\nif public_access:\n  raise BizException(...)\n```\n</code></pre> <p>Why do I need a NAT Gateway or Endpoint when connecting to RDS?</p> <p>When using the solution, the following operations need to be automated:</p> <ul> <li>Read secret information from SecretsManager during Glue Job execution (when using Secret to save RDS password);</li> <li>Read catalog information from Glue;</li> <li>Write the result to S3 after the check is completed.</li> </ul> <p>Because RDS is usually in a private subnet in the VPC, a NAT gateway or Glue endpoint, S3 endpoint, and SecretsManager endpoint are required (all three endpoints are required at the same time). If connecting to the database using a username and password, only the first two endpoints are needed. If Secrets are encrypted using KMS, the KMS endpoint is also required.</p>"},{"location":"faq/#about-data-catalog","title":"About Data Catalog","text":"<p>How the incremental scanning works? Will new tables be scanned? Will changes to fields be scanned?</p> <p>Please refer to the \"About Incremental Scanning\" section in the Create Job document.</p> <p>If one of the fields in a table is changed, will the other fields be rescanned? Will the previous identification be overwritten?</p> <p>Yes. If any field changes (i.e., the schema changes), the entire table will be rescanned by the job. After the job is completed, the overwrite is done at the column level (field). There are two cases:</p> <ul> <li> <p>If a column has been manually marked with an identifier (last updated by is not System), and the \"Do not overwrite manual marks\" setting is selected in the job, the previous identification will not be overwritten.</p> </li> <li> <p>If a column has not been modified by an identifier (last updated by is System), the previous identification will not be overwritten.</p> </li> </ul> <p>If a data field is not encrypted and is later encrypted, will it no longer be identified as sensitive data?</p> <p>In general, if a field is encrypted and does not contain plaintext sensitive data, it will not be identified as sensitive data, which means job will not mark any identifiers after scanning. </p> <p>There is only one exception: if the field has previously been manually marked with an identifier (last updated by is not System), and the \"Do not overwrite manual marks\" setting is selected in the job, the job will not modify the field after scanning, and the identifier will still exist.</p>"},{"location":"faq/#about-data-classification-templates","title":"About Data Classification Templates","text":"<p>What are the built-in identifiers, can you provide detailed rules of how they are defined?</p> <p>The list of built-in data identifiers can be found in Appendix-Built-in Identifiers. As they are provided by the AWS Glue Service, the specific matching regular expressions, keywords, and AI models used for these built-in identifiers are not included in this open source project.</p>"},{"location":"faq/#about-sensitive-data-discovery-jobs","title":"About Sensitive Data Discovery Jobs","text":"<p>What does the \"score\" in the exported data mean? Does it represent the proportion of sensitive data to 1000 records?</p> <p>Yes. The definition of Score is the number of occurrences of the identifier in the sample divided by the total number of scanned rows (by default, 1000 rows). Only fields with a Score greater than the sensitivity threshold configured in the job (by default, 10% sensitivity threshold) will be defined as sensitive data. The value of Score will be included in the job report, and using this value, you can visually understand the percentage of sensitive data in the column.</p> <ul> <li>Example 1: In the total sample of 1000 rows, the scheme detected 120 sensitive data, so Score = 120/1000 = 0.12. Since 0.12 &gt; 10%, this column is marked as a sensitive column and the corresponding data identifier is automatically labeled.</li> <li>Example 2: In the total sample of 1000 rows, the scheme detected 80 sensitive data, so Score = 120/1000 = 0.08. Since 0.08 &lt; 10%, this column is marked as non-sensitive and the data identifier is not automatically labeled.</li> </ul>"},{"location":"notices/","title":"Notices","text":"<p>Customers are responsible for making their own independent assessment of the information in this document. This document: (a) is for informational purposes only, (b) represents Amazon Web Services current product offerings and practices, which are subject to change without notice, and (c) does not create any commitments or assurances from Amazon Web Services and its affiliates, suppliers or licensors. Amazon Web Services products or services are provided \u201cas is\u201d without warranties, representations, or conditions of any kind, whether express or implied. Amazon Web Services responsibilities and liabilities to its customers are controlled by Amazon Web Services agreements, and this document is not part of, nor does it modify, any agreement between Amazon Web Services and its customers.</p> <p>The Sensitive Data Protection on AWS solution is licensed under the terms of the Apache License Version 2.0 available at The Apache Software Foundation.</p>"},{"location":"revisions/","title":"Revisions","text":"Date Modifications June 2023 Initial release version. Support for AWS data sources: S3 and Amazon RDS for scanning sensitive data in structured data December 2023 Release v1.1.0.  1) Support for unstructured data scanning in S3. 2) Support for new AWS data sources: self-hosted databases in EC2 and Glue data catalogs. 3) Support for databases in other clouds (Tencent/Google): databases that can be connected via JDBC."},{"location":"troubleshooting/","title":"Troubleshooting","text":""},{"location":"troubleshooting/#about-connecting-to-aws-accounts-and-data-sources","title":"About Connecting to AWS Accounts and Data Sources","text":"<p>When connecting to RDS, an error occurred: <code>At least one security group must open all ingress ports.</code></p> <p>Assign the default security group of the VPC where the RDS is located. This is a requirement of Glue, see details at https://docs.aws.amazon.com/glue/latest/dg/glue-troubleshooting-errors.html.</p> <p>After assigning the security group to the RDS, you need to delete the corresponding data catalog in SDP, and then click \"Sync to Data Catalog\" again. This operation will allow the backend to retrieve the latest configuration of the RDS and create the data catalog successfully.</p> <p>When connecting to RDS, an error occurred <code>VPC S3 endpoint validation failed for SubnetId: subnet-000000. VPC: vpc-111111. Reason: Could not find S3 endpoint or NAT gateway for subnetId: subnet-000000 in Vpc</code></p> <p>This error occurs after manually deleting the NAT Gateway or S3 endpoint (type is Gateway). To fix it, try to add a NAT Gateway or S3 endpoint and configure the route to resolve the issue.</p> <p>After deleting the Agent CloudFormation in the monitored, an error occurred when trying to delete the AWS account in the main account: <code>An error occurred (AccessDenied) when calling the AssumeRole operation: User: arn:aws:sts::5566xxxxxxx:assumed-role/SDPSAPIRole-us-east-1/SDPS-Admin-APIAPIFunction719F975A-yEQ3iOlIYK1F is not authorized to perform: sts:AssumeRole on resource: arn:aws:iam::8614xxxxxxx:role/SDPSRoleForAdmin-us-east-1</code></p> <p>You need to wait for 5-8 minutes after deleting the Agent CloudFormation before attempting to delete the AWS account in SDP UI, otherwise this error may occur. If the error occurs, please re-try to delete the AWS account on UI after 10 minutes.</p>"},{"location":"troubleshooting/#about-sensitive-data-discovery-jobs","title":"About Sensitive data discovery jobs","text":"<p>When using the solution in China Regions, it is not possible to download the template snapshot and report files.</p> <p>This is because the template snapshot and report are downloaded from S3 using pre-signed URLs, so the account where Admin is located must have ICP filing or ICP Exception.</p>"},{"location":"uninstall/","title":"Uninstall the solution","text":"<p>To uninstall the solution, you must delete the AWS CloudFormation stack. </p> <p>You can use either the AWS Management Console or the AWS Command Line Interface (AWS CLI) to delete the CloudFormation stack.</p> <p>Time to uninstall: Approximately 60 minutes</p>"},{"location":"uninstall/#uninstall-the-stack-using-the-aws-management-console","title":"Uninstall the stack using the AWS Management Console","text":"<ol> <li>Sign in to the AWS CloudFormation console.</li> <li>Select this solution\u2019s installation parent stack.</li> <li>Choose Delete.</li> </ol>"},{"location":"uninstall/#uninstall-the-stack-using-aws-command-line-interface","title":"Uninstall the stack using AWS Command Line Interface","text":"<p>Determine whether the AWS Command Line Interface (AWS CLI) is available in your environment. For installation instructions, refer to What Is the AWS Command Line Interface in the AWS CLI User Guide. After confirming that the AWS CLI is available, run the following command.</p> <pre><code>aws cloudformation delete-stack --stack-name &lt;installation-stack-name&gt; --region &lt;aws-region&gt;\n</code></pre>"},{"location":"architecture-details/architecture-details/","title":"How the solution works","text":"<p>his section describes the components and AWS services that make up this solution and the high level system design.</p> <p> Sensitive Data Protection on AWS high level system design</p> <p>As shown in the diagram, the centralized sensitive data governance account is the admin account. Solution users, typically security auditors, can access the solution via a web portal after deploying the Admin stack. Users can browse the data catalog and execute sensitive data detection jobs in the monitored account(s) after deploying the Agent stack and logging into the web portal. </p> <p>Multiple monitored accounts are connected to the admin account with data source access and job execution privileges, so that the admin account can invoke the Job Processor model in the specified monitored account for sensitive data detection.</p>"},{"location":"architecture-details/architecture-details/#modules-in-admin-account","title":"Modules in Admin Account","text":"<ul> <li> <p>Web Portal (UI): The solution administrator or normal users can access the solution through the web portal. It provides secure user access management and a web UI for the solution.</p> </li> <li> <p>Data Source Management (DSM): The DSM is responsible for retrieving the data sources from monitored accounts by the Data Source Detector and storing the data source structure. Users can explore the data storage in the monitored accounts, such as S3 buckets and RDS instances.</p> </li> <li> <p>Data Catalog Management (DCM): The DCM can discover the latest schema (normally called metadata) of the data sources in DSM. The schema includes information such as table columns in RDS databases and the sensitive data detection results after the detection job has run.</p> </li> <li> <p>Job Controller (JC): The Job Controller is responsible for executing the detection job in the monitored account and collecting the detection results back to the admin account. It can configure the job to run on a user-defined schedule or as needed.</p> </li> <li> <p>Template Configuration (TC): The detection templates are stored in the TC model. It contains built-in templates and custom-defined templates. The JC can retrieve the templates for running the job processor.</p> </li> <li> <p>Account Management (AM): The monitored AWS account(s) are managed by the AM model.</p> </li> </ul>"},{"location":"architecture-details/architecture-details/#modules-in-monitored-accounts","title":"Modules in Monitored Account(s)","text":"<ul> <li> <p>Job Processor: The Job Processor is the running container for sensitive data detection, invoked by the Job Controller. The Job Processor reads the raw data to the detection engine for detection and sends the analysis results and running state to the Job Controller.</p> </li> <li> <p>Detection Engine: The Detection Engine model is the core sensitive data detection engine with AI/ML support features. It receives the data from the Job Processor to identify the sensitive data type using a pre-trained ML model or pattern.</p> </li> </ul>"},{"location":"architecture-details/services-in-the-solution/","title":"AWS services in this solution","text":"<p>The following AWS services are included in this solution:</p> AWS service Description Application Load Balancer Core. To distribute the frontend web UI assets. Amazon ECR Core. To store Docker images. AWS Lambda Core. To serve as a target for the application load balancer. AWS Step Functions Supporting.\u00a0To control job processing. AWS Glue Supporting.\u00a0To take inventory of data sources and to be invoked for sensitive data detection. Amazon RDS Supporting. To set up, operate, and scale a relational database in the cloud with just a few clicks. Amazon SQS Supporting.\u00a0To allow the Step Functions to send messages to the detection job queue. Amazon SageMaker Supporting. To pre-process unstructured data."},{"location":"architecture-overview/architecture/","title":"Architecture diagram","text":"<p>Deploying this solution with the default parameters builds the following environment in the AWS Cloud.</p> <p> Sensitive Data Protection on AWS architecture</p> <ol> <li>The Application Load Balancer distributes the solution's frontend web UI assets hosted in AWS Lambda. </li> <li>Identity provider for user authentication. </li> <li>The AWS Lambda function is packaged as Docker images and stored in the Amazon ECR (Elastic Container Registry). </li> <li>The backend Lambda function is a target for the Application Load Balancer. </li> <li>The backend Lambda function invokes AWS Step Functions in monitored accounts for sensitive data detection. </li> <li>In AWS Step Functions workflow, the AWS Glue Crawler runs to take inventory of the structured data sources and is stored in the Glue Database as metadata tables. Amazon SageMaker processing job is used to pre-process unstructured file in S3 buckets, and store metadata in the Glue database. AWS Glue Job is used to detect sensitive data.</li> <li>The Step Functions send Amazon SQS messages to the detection job queue after the Glue job has run. </li> <li>Lambda function processes messages from Amazon SQS.</li> <li>The Amazon Athena query detection results and save to MySQL instance in Amazon RDS.</li> </ol> <p>The solution uses the AWS Glue service as a core for building data catalog in the monitored account(s) and for invoking the Glue Job to detect sensitive data Personal Identifiable Information (PII). The distributed Glue job runs in each monitored account, and the admin account contains a centralized data catalog of data sources across AWS accounts. This is an implementation of the Data Mesh concept recommended by AWS.</p> <p>To be more specific, the solution introduces an event-driven process and uses AWS IAM roles to trigger and communicate between the admin account and the monitored account(s) for sensitive data discovery jobs. The admin account can start PII detection jobs and retrieve data catalogs. All monitored AWS accounts are permitted to be connected to the admin account, which is able to distinguish and access the monitored accounts.</p>"},{"location":"architecture-overview/well-architected-pillars/","title":"AWS Well-Architected pillars","text":"<p>This solution was designed with best practices from the AWS Well-Architected Framework which helps customers design and operate reliable, secure, efficient, and cost-effective workloads in the cloud.</p> <p>This section describes how the design principles and best practices of the Well-Architected Framework were applied when building this solution.</p>"},{"location":"architecture-overview/well-architected-pillars/#operational-excellence","title":"Operational excellence","text":"<p>This section describes how the principles and best practices of the operational excellence pillar were applied when designing this solution.</p> <p>The Sensitive Data Protection on AWS solution pushes metrics, logs and traces to Amazon CloudWatch at various stages to provide observability into the infrastructure, Elastic load balancer, Lambda functions, Step Function workflow and the rest of the solution components.</p>"},{"location":"architecture-overview/well-architected-pillars/#security","title":"Security","text":"<p>This section describes how the principles and best practices of the security pillar were applied when designing this solution.</p> <ul> <li>Sensitive Data Protection on AWS solution web console users are authenticated and authorized with Amazon Cognito or OpenID Connect.</li> <li>All inter-service communications use AWS IAM roles.</li> <li>All roles used by the solution follows least-privilege access. That is, it only contains minimum permissions required so the service can function properly.</li> </ul>"},{"location":"architecture-overview/well-architected-pillars/#reliability","title":"Reliability","text":"<p>This section describes how the principles and best practices of the reliability pillar were applied when designing this solution.</p> <ul> <li>Using AWS serverless services wherever possible (for example, Lambda, Step Functions, Amazon S3, and Amazon SQS) to ensure high availability and recovery from service failure.</li> <li>Scan result metadata is stored in Amazon RDS with multiple Availability Zones (AZs).</li> </ul>"},{"location":"architecture-overview/well-architected-pillars/#performance-efficiency","title":"Performance efficiency","text":"<p>This section describes how the principles and best practices of the performance efficiency pillar were applied when designing this solution.</p> <ul> <li>The ability to launch this solution in any Region that supports AWS services in this solution such as: Amazon S3, Elastic load balancer.</li> <li>Use Serverless architectures remove the need for you to run and maintain physical servers for traditional compute activities.</li> <li>Automatically testing and deploying this solution daily. Reviewing this solution by solution architects and subject matter experts for areas to experiment and improve.</li> </ul>"},{"location":"architecture-overview/well-architected-pillars/#cost-optimization","title":"Cost optimization","text":"<p>This section describes how the principles and best practices of the cost optimization pillar were applied when designing this solution.</p> <ul> <li>Use Autoscaling Group so that the compute costs are only related to how much data is ingested and processed.</li> <li>Using serverless services such as Amazon S3, Amazon Kinesis Data Streams, Amazon EMR Serverless and Amazon Redshift Serverless so that customers only get charged for what they use.</li> </ul>"},{"location":"architecture-overview/well-architected-pillars/#sustainability","title":"Sustainability","text":"<p>This section describes how the principles and best practices of the sustainability pillar were applied when designing this solution.</p> <ul> <li>The solution\u2018s use of managed services (such as Amazon Glue, Amazon Step Functions, Amazon Lambda) are aimed at reducing carbon footprint compared to the footprint of continually operating on-premises servers.</li> <li>The solution decouples senders and receivers of asynchronous messages by Amazon SQS.</li> </ul>"},{"location":"deployment/deployment/","title":"Deployment","text":"<p>Before you launch the solution, review the architecture, supported regions, and other considerations discussed in this guide. Follow the step-by-step instructions in this section to configure and deploy the solution into your account.</p> <p>Time to deploy: Approximately 30 minutes</p>"},{"location":"deployment/deployment/#deployment-overview","title":"Deployment overview","text":"<p>Use the following steps to deploy this solution on AWS. </p> <ul> <li> <p>Step 1. Create an OpenID Connector (OIDC) application(Skip this step by using the template with an identity provider)</p> </li> <li> <p>Step 2. Deploy the Admin stack</p> </li> <li> <p>Step 3. Configure the OIDC application(Skip this step by using the template with an identity provider)</p> </li> <li> <p>Step 4. Configure custom domain name</p> </li> <li> <p>Step 5. Launch the solution console</p> </li> <li> <p>Step 6. Deploy the Agent stack</p> </li> </ul>"},{"location":"deployment/deployment/#deployment-steps","title":"Deployment steps","text":""},{"location":"deployment/deployment/#step-1-create-an-oidc-application","title":"Step 1. Create an OIDC application","text":"<p>Remind</p> <p>Skip this step by using the template with an identity provider</p> <p>You can use different kinds of OIDC providers. This section introduces Option 1 to Option 3.</p> <ul> <li> <p>Option 1: Cognito, which uses Amazon Cognito as OIDC provider</p> </li> <li> <p>Option 2: Authing, which is an example of third-party authentication providers</p> </li> <li> <p>Option 3: OKTA, which is an example of third-party authentication providers</p> </li> </ul>"},{"location":"deployment/deployment/#option-1-cognito","title":"Option 1: Cognito","text":"<p>You can leverage the Cognito User Pool as the OIDC provider in supported AWS Regions. </p> <ol> <li> <p>Go to the Amazon Cognito console.</p> </li> <li> <p>Set up the hosted UI with the Amazon Cognito console based on this guide.</p> </li> <li> <p>When creating a user pool, from Step 1 to Step 4, choose the options according to your needs. </p> </li> <li> <p>In Step 5 Integrate your app, make sure you have selected Use the Cognito Hosted UI in the Hosted authentication pages area. For App type, choose Public client. Choose Don't generate a client secret under Client secret. </p> </li> <li> <p>In Advanced app client settings, select OpenID, Email and Profile for OpenID Connect scopes. </p> </li> <li> <p>Confirm that the Hosted UI status is Available, and confirm that the OpenID Connect scopes includes email, openid, and profile. </p> </li> <li> <p>Save the App Client ID, User pool ID and the AWS Region to a file, which will be used later. In Step 2. Deploy Admin template, the Client ID is the App Client ID, and Issuer URL is <code>https://cognito-idp.${REGION}.amazonaws.com/${USER_POOL_ID}</code></p> </li> </ol> <p>Note: The Allowed callback URLs and Allowed sign-out URLs will be configured again later in Step 3 Configure the OIDC application.</p> <p> </p>"},{"location":"deployment/deployment/#option-2-authing","title":"Option 2: Authing","text":"<ol> <li> <p>Go to the Authing console.</p> </li> <li> <p>On the left navigation bar, select Self-built App under Applications.</p> </li> <li> <p>Choose Create.</p> </li> <li> <p>Enter the Application Name, and Subdomain.</p> </li> <li> <p>Save the App ID (that is, Client ID) and Issuer (Issuer URL) to a text file from Endpoint Information, which will be used later. </p> </li> <li> <p>Set the Authorization Configuration in Protocol Configuration tab. </p> </li> <li>On the Access Authorization tab, select the accessible users.</li> </ol>"},{"location":"deployment/deployment/#option-3-okta","title":"Option 3: OKTA","text":"<ol> <li> <p>Go to the OKTA console.</p> </li> <li> <p>Choose Applications, and then choose Create App Integration. </p> </li> <li> <p>Choose OIDC - OpenID Connect, then Single-Page Application, and Next. </p> </li> <li> <p>For Controlled access, choose the options that suit your needs and choose Save. </p> </li> <li>Save the Client ID and Issuer URL to a text file from Endpoint Information, which will be used later. The Issuer URL is in your profile. The full Issuer URL is \u201chttps://dev-xxx.okta.com\u201d.  </li> </ol>"},{"location":"deployment/deployment/#step-2-deploy-admin-stack","title":"Step 2. Deploy Admin stack","text":"<p>Deploy the AWS CloudFormation Admin template into your AWS admin account.</p> <ol> <li> <p>Sign in to the AWS Management Console and use the button below to launch the AWS CloudFormation template.</p> Launch in AWS Console Deploy the Admin template with a new VPC in AWS Regions Deploy the Admin template with an existing VPC in AWS Regions Deploy the Admin template with a new VPC in AWS China Regions Deploy the Admin template with an existing VPC in AWS China Regions <p>Important</p> <p>If you choose deployment with an existing VPC, make sure to meet the following requirements:</p> <ul> <li>At least two public subnets and two private subnets.</li> <li>The existing VPC must have NAT gateway. </li> <li>The two private subnets must have routes pointing to NAT gateway.        </li> </ul> </li> <li> <p>To launch this solution in a different AWS Region, use the Region selector in the console navigation bar.</p> </li> <li>On the Create stack page, verify that the correct template URL is shown in the Amazon S3 URL text box and choose Next.</li> <li>On the Specify stack details page, assign a valid and account level unique name to your solution stack.</li> <li> <p>Under Parameters, review the parameters for the template and modify them as necessary. This solution uses the following default values.</p> Parameter Default Description Issuer URL Specify the secure OpenID Connect URL. Maximum 255 characters. URL must begin with <code>https://</code>.The template with identity provider does not have this parameter. Client ID Specify the client ID issued by the identity provider. Maximum 255 characters. Use alphanumeric or ?:_.-/? characters. The template with identity provider does not have this parameter. Public Access Yes If you choose No, the portal website can be accessed ONLY in the VPC. If you want to access the portal website over Internet, you need to choose Yes. The template that uses only private subnets does not have this parameter. Port 80 If an ACM certificate ARN has been added, we recommend using port 443 as the default port for HTTPS protocol. Otherwise, port 80 can be set as an alternative option ACM Certificate ARN (Optional) To enable secure communication through encryption and enhancing the security of the solution, you can add a public certificate ARN from ACM to create the portal website URL based on the HTTPS protocol Custom Domain Name (Optional) By adding your own domain name, such as sdps.example.com, you can directly access the portal website by adding a CNAME record to that domain name after deploying the stack. You only need to enter the domain name, excluding <code>http(s)://</code> </li> <li> <p>Choose Next.</p> </li> <li>On the Configure stack options page, choose Next.</li> <li>On the Review page, review and confirm the settings. Select 3 checkboxes that I acknowledge.</li> <li>Choose Create stack to deploy the stack. Wait for about 20 minutes to ensure that all related resource are created. You can choose the Resource and Event tab to see the status of the stack.</li> <li>Select the Outputs tab to check the <code>POrtalUrlHTTP(S)</code> and <code>SigninRedirectUriHTTP(S)</code>, which will be used to configure the OIDC application in Step 3. Configure the OIDC application. </li> </ol>"},{"location":"deployment/deployment/#step-3-configure-the-oidc-application","title":"Step 3. Configure the OIDC application","text":"<p>Note</p> <p>Skip this step by using the template with an identity provider.</p> <p>Enter the values of SigninRedirectUriHTTP(S) and PortalUrlHTTP(S) into the login callback url and logout callback url in your OIDC application, respectively.</p>"},{"location":"deployment/deployment/#option-1-cognito_1","title":"Option 1: Cognito","text":"<ol> <li>Go to your user pools.</li> <li>Choose App integration.</li> <li>Choose Your App.</li> <li>Make the following configurations. </li> </ol>"},{"location":"deployment/deployment/#option-2-authing_1","title":"Option 2: Authing","text":""},{"location":"deployment/deployment/#option-3-otka","title":"Option 3: OTKA","text":""},{"location":"deployment/deployment/#step-4-configure-custom-domain-name","title":"Step 4. Configure custom domain name","text":"<p>If you entered a custom domain name when deploying the Admin stack, set the CName of the custom domain name to LoadBalancerDnsNameHTTP(S) value on the Output tab of CloudFormat.</p> <ol> <li>Obtain the LoadBalancerDnsNameHTTP(S) as the endpoint from the Outputs tab. </li> <li>Create a CNAME record in DNS resolver, which points to the endpoint address.</li> </ol>"},{"location":"deployment/deployment/#step-5-launch-the-solution-console","title":"Step 5. Launch the solution console","text":"<ol> <li>Obtain the value of PortalUrlHTTP(S) from the Outputs tab.</li> <li>Launch the solution's console by entering the value in the browser. </li> </ol>"},{"location":"deployment/deployment/#step-6-deploy-the-agent-stack","title":"Step 6. Deploy the Agent stack","text":"<p>Deploy the AWS CloudFormation Agent template into your AWS monitored account.</p> <p>Important</p> <p>You can deploy the Agent stack on one or multiple accounts to be monitored.</p> <ol> <li> <p>Sign in to the AWS Management Console and use the button below to launch the AWS CloudFormation template.</p> Launch in AWS Console Deploy the Agent template in AWS Regions Deploy the Agent template in AWS China Regions </li> <li> <p>To launch this solution in a different AWS Region, use the Region selector in the console navigation bar.</p> </li> <li>On the Create stack page, verify that the correct template URL is shown in the Amazon S3 URL text box and choose Next.</li> <li>On the Specify stack details page, enter a Stack name.</li> <li>In the Admin Account ID field, enter the Account ID (12 digits) of the Admin account. This means that this account will be a monitored account by the specified Admin account.</li> <li>Follow the remaining steps described in Step 2. Deploy the Admin stack to complete deploying the Agent stack.</li> </ol>"},{"location":"deployment/template/","title":"AWS CloudFormation template","text":"<p>To automate deployment, this solution uses the following AWS CloudFormation templates, which you can download before deployment:</p> <p>To use the solution, you need to deploy the Admin template on an admin account and the Agent template on one or multiple monitored accounts. Only the accounts which are deployed with the Agent template can be monitored, which are also called monitored account.</p> <p>If all your accounts are within AWS Organizations, you need to deploy the IT template on an IT account. The AWS organization root user needs to first register the IT account as a delegated administrator.</p>"},{"location":"deployment/template/#aws-global-regions","title":"AWS Global Regions","text":"<ul> <li>Template for Admin account with IdP (Admin template) in New VPC</li> <li>Template for Admin account With IdP (Admin template) in Existing VPC</li> <li>Template for Admin account With IdP (Admin template) in Only Private Subnets Existing VPC</li> <li>Template for Admin account (Admin template) in New VPC</li> <li>Template for Admin account (Admin template) in Existing VPC</li> <li>Template for Admin account (Admin template) in Only Private Subnets Existing VPC</li> <li>Template for Monitored account (Agent template)</li> <li>Template for IT account (IT template)</li> </ul>"},{"location":"deployment/template/#aws-china-regions","title":"AWS China Regions","text":"<ul> <li>Template for Admin account (Admin template) in New VPC</li> <li>Template for Admin account (Admin template) in Existing VPC</li> <li>Template for Monitored account (Agent template)</li> <li>Template for IT account (IT template)</li> </ul>"},{"location":"developer-guide/api-access/","title":"API Access Restriction","text":"<p>If you need to call the API to perform data detection, please configure the ApiKey in the environment variables of Lambda first. The system will verify the authenticity of the ApiKey through the specified field in the request header. For detailed instructions, please refer to this document.</p>"},{"location":"developer-guide/api-access/#custom-security-key","title":"Custom Security Key","text":"<ul> <li>Step 1: Log in to the AWS console, select the Lambda option, and enter the Lambda console.</li> </ul> <ul> <li>Step 2: Select the corresponding deployment region in the upper right corner, and enter the Functions list page.</li> <li>Step 3: Enter \"APIAPIFunction\" in the search box, and select the corresponding Function from the drop-down list.</li> </ul> <ul> <li>Step 4: Click on the Function name link to enter the Function page, then select the Configuration tab.</li> <li>Step 5: Select the Environment variables tab on the left side, click the edit button on the upper right corner of the right panel, and add environment variables.</li> </ul> <ul> <li>Step 6: On the environment variable editing page, add a key-value pair, with the key as the fixed value \"ApiKey\", and the value as the user-defined content. This value will be used for validation when accessing the API later.</li> </ul>"},{"location":"developer-guide/api-access/#security-verification","title":"Security Verification","text":"<p>All API requests require security verification using an API key. Your API key should be included in the HTTP header of all API requests as x-api-key as follows:</p> <pre><code>x-api-key: xxxxxxxxxxxxxxxxxxxx\n</code></pre>"},{"location":"developer-guide/api-preview/","title":"API overview","text":"<p>This document only lists the main operation interfaces for each module. For more details, please refer to OPENAPI</p>"},{"location":"developer-guide/api-preview/#apis-related-to-data-sources","title":"APIs Related to Data Sources","text":"url Function Rate Limit (requests per second) /data-source/add_account Add Account 10 /data-source/delete_account Delete Account 10 /data-source/add-jdbc-conn Add Data Source 10 /data-source/update-jdbc-conn Edit Data Source 10 /data-source/delete-jdbc Delete Data Source 10"},{"location":"developer-guide/api-preview/#apis-related-to-job","title":"APIs Related to Job","text":"url Function Rate Limit (requests per second) /discovery-jobs Create Job 10 /discovery-jobs/{job_id}/start Start Job 10"},{"location":"developer-guide/api-preview/#apis-related-to-identifier","title":"APIs Related to Identifier","text":"url Function Rate Limit (requests per second) /template/identifiers Create Recognition Rule 10 /template/template-mappings Add Recognition Rule to Template 10"},{"location":"developer-guide/datasource/","title":"Datasource API","text":""},{"location":"developer-guide/datasource/#add-account","title":"Add Account","text":"<p>POST /data-source/add_account</p> <p>Create an account under the specified provider.</p>"},{"location":"developer-guide/datasource/#request","title":"Request","text":"Parameter Name Required Type Description account_provider Yes integer Provider ID account_id Yes string region Yes string <p>Response Examples</p> <p>Success</p> <pre><code>{\n  \"status\": \"success\",\n  \"code\": 1001,\n  \"message\": \"Operation succeeded\"\n}\n</code></pre>"},{"location":"developer-guide/datasource/#responses","title":"Responses","text":"HTTP Status Code Meaning Description Data schema 1001 OK Success Inline"},{"location":"developer-guide/datasource/#responses-data-schema","title":"Responses Data Schema","text":"<p>HTTP Status Code 1001</p> Name Type Required Restrictions Title description \u00bb message string true none none \u00bb code integer true none none"},{"location":"developer-guide/datasource/#delete-account","title":"Delete Account","text":"<p>POST /data-source/delete_account</p> <p>Delete an account under the specified provider.</p>"},{"location":"developer-guide/datasource/#request_1","title":"Request","text":"Parameter Name Required Type Description account_provider Yes integer Provider ID account_id Yes string - region Yes string - <p>Response Examples</p> <p>Success</p> <pre><code>{\n  \"status\": \"success\",\n  \"code\": 1001,\n  \"message\": \"Operation succeeded\"\n}\n</code></pre>"},{"location":"developer-guide/datasource/#responses_1","title":"Responses","text":"HTTP Status Code Meaning Description Data schema 1001 OK Success Inline"},{"location":"developer-guide/datasource/#responses-data-schema_1","title":"Responses Data Schema","text":"<p>HTTP Status Code 1001</p> Name Type Required Restrictions Title description \u00bb message string true none none \u00bb code integer true none none"},{"location":"developer-guide/datasource/#add-datasource","title":"Add Datasource","text":"<p>POST /data-source/add-jdbc-conn</p> <p>Add a data source under the specified account.</p>"},{"location":"developer-guide/datasource/#request_2","title":"Request","text":"Parameter Name Required Type Description instance_id Yes string - account_provider_id Yes integer Provider ID account_id Yes string - region Yes string - description No string - jdbc_connection_url Yes string - jdbc_connection_schema Yes string - jdbc_enforce_ssl No string true/false kafka_ssl_enabled No string true/false master_username Yes string * password Yes string * secret Yes string * skip_custom_jdbc_cert_validation No string - custom_jdbc_cert No string - custom_jdbc_cert_string No string - network_availability_zone Yes string - network_subnet_id Yes string - network_sg_id Yes string - glue_crawler_last_updated No datetime - creation_time No string - last_updated_time No string - jdbc_driver_class_name No string - jdbc_driver_jar_uri No string - create_type Yes integer - <p>*: master_username/password and secret cannot be empty at the same time.</p> <p>Response Examples</p> <p>Success</p> <pre><code>{\n  \"status\": \"success\",\n  \"code\": 1001,\n  \"message\": \"Operation succeeded\"\n}\n</code></pre>"},{"location":"developer-guide/datasource/#responses_2","title":"Responses","text":"HTTP Status Code Meaning Description Data schema 1001 OK Success Inline"},{"location":"developer-guide/datasource/#responses-data-schema_2","title":"Responses Data Schema","text":"<p>HTTP Status Code 1001</p> Name Type Required Restrictions Title description \u00bb message string true none none \u00bb code integer true none none"},{"location":"developer-guide/datasource/#edit-datasource","title":"Edit Datasource","text":"<p>POST /data-source/delete_account</p> <p>Edit a data source under the specified account.</p>"},{"location":"developer-guide/datasource/#request_3","title":"Request","text":"Parameter Name Required Type Description instance_id Yes string - account_provider_id Yes integer Provider ID account_id Yes string - region Yes string - description No string - jdbc_connection_url Yes string - jdbc_connection_schema Yes string - jdbc_enforce_ssl No string true/false kafka_ssl_enabled No string true/false master_username Yes string * password Yes string * secret Yes string * skip_custom_jdbc_cert_validation No string - custom_jdbc_cert No string - custom_jdbc_cert_string No string - network_availability_zone Yes string - network_subnet_id Yes string - network_sg_id Yes string - glue_crawler_last_updated No datetime - creation_time No string - last_updated_time No string - jdbc_driver_class_name No string - jdbc_driver_jar_uri No string - create_type Yes integer - <p>*: master_username/password and secret cannot be empty at the same time.</p> <p>Response Examples</p> <p>Success</p> <pre><code>{\n  \"status\": \"success\",\n  \"code\": 1001,\n  \"message\": \"Operation succeeded\"\n}\n</code></pre>"},{"location":"developer-guide/datasource/#responses_3","title":"Responses","text":"HTTP Status Code Meaning Description Data schema 1001 OK Success Inline"},{"location":"developer-guide/datasource/#responses-data-schema_3","title":"Responses Data Schema","text":"<p>HTTP Status Code 1001</p> Name Type Required Restrictions Title description \u00bb message string true none none \u00bb code integer true none none"},{"location":"developer-guide/datasource/#delete-datasource","title":"Delete Datasource","text":"<p>POST /data-source/delete-jdbc</p> <p>Delete a data source under the specified account.</p>"},{"location":"developer-guide/datasource/#request_4","title":"Request","text":"Parameter Name Required Type Description account_provider Yes integer Provider ID account_id Yes string region Yes string instances Yes list <p>Response Examples</p> <p>Success</p> <pre><code>{\n  \"status\": \"success\",\n  \"code\": 1001,\n  \"message\": \"Operation succeeded\"\n}\n</code></pre>"},{"location":"developer-guide/datasource/#responses_4","title":"Responses","text":"HTTP Status Code Meaning Description Data schema 1001 OK Success Inline"},{"location":"developer-guide/datasource/#responses-data-schema_4","title":"Responses Data Schema","text":"<p>HTTP Status Code 1001</p> Name Type Required Restrictions Title description \u00bb message string true none none \u00bb code integer true none none"},{"location":"developer-guide/identifier/","title":"Identifier API","text":""},{"location":"developer-guide/identifier/#create-identifier","title":"Create Identifier","text":"<p>POST /template/identifiers</p> <p>Create Identifier.</p>"},{"location":"developer-guide/identifier/#request","title":"Request","text":"Parameter Name Required Type Description description No string - type No integer Default:1 name Yes string - classification No integer Default:1 privacy No integer Default:0 rule No string * header_keywords No string * max_distance No integer - min_occurrence No integer - props No string - exclude_keywords No string - <p>Response Examples</p> <p>Success</p> <pre><code>{\n  \"status\": \"success\",\n  \"code\": 1001,\n  \"message\": \"Operation succeeded\"\n}\n</code></pre>"},{"location":"developer-guide/identifier/#responses","title":"Responses","text":"HTTP Status Code Meaning Description Data schema 1001 OK Success Inline"},{"location":"developer-guide/identifier/#responses-data-schema","title":"Responses Data Schema","text":"<p>HTTP Status Code 1001</p> Name Type Required Restrictions Title description \u00bb message string true none none \u00bb code integer true none none"},{"location":"developer-guide/identifier/#add-identifier-into-template","title":"Add identifier into template","text":"<p>POST /template/template-mappings</p> <p>Add identifier into template.</p>"},{"location":"developer-guide/identifier/#request_1","title":"Request","text":"Parameter Name Required Type Description template_id Yes integer Template ID identifier_ids Yes array eg: [1,2,3] status No string Default:0 <p>Response Examples</p> <p>Success</p> <pre><code>{\n  \"status\": \"success\",\n  \"code\": 1001,\n  \"message\": \"Operation succeeded\"\n}\n</code></pre>"},{"location":"developer-guide/identifier/#responses_1","title":"Responses","text":"HTTP Status Code Meaning Description Data schema 1001 OK Success Inline"},{"location":"developer-guide/identifier/#responses-data-schema_1","title":"Responses Data Schema","text":"<p>HTTP Status Code 1001</p> Name Type Required Restrictions Title description \u00bb message string true none none \u00bb code integer true none none"},{"location":"developer-guide/job/","title":"Job API","text":""},{"location":"developer-guide/job/#create-job","title":"Create Job","text":"<p>POST /discovery-jobs</p> <p>Create job.</p>"},{"location":"developer-guide/job/#request","title":"Request","text":"Parameter Name Required Type Description name Yes string - template_id No integer Default: 1 schedule No string Default: OnDemand description No string - range No integer Default:1 depth_structured No integer Default:100 depth_unstructured No integer Default:10 detection_threshold No number Default:0.2 all_s3 No integer - all_rds No integer - all_ddb No integer - all_emr No integer - all_glue No integer - all_jdbc No integer - overwrite No integer - exclude_keywords No string - include_keywords No string - exclude_file_extensions No string - include_file_extensions No string - provider_id No integer - database_type No string - databases Yes array          The structure of each element is as follows         {           \u2003\"account_id\": \"string\",           \u2003\"region\": \"string\",           \u2003\"database_type\": \"string\",           \u2003\"database_name\": \"string\",           \u2003\"table_name\": \"string\"         }         <p>Response Examples</p> <p>Success</p> <pre><code>{\n  \"status\": \"success\",\n  \"code\": 1001,\n  \"message\": \"Operation succeeded\"\n}\n</code></pre>"},{"location":"developer-guide/job/#responses","title":"Responses","text":"HTTP Status Code Meaning Description Data schema 1001 OK Success Inline"},{"location":"developer-guide/job/#responses-data-schema","title":"Responses Data Schema","text":"<p>HTTP Status Code 1001</p> Name Type Required Restrictions Title description \u00bb message string true none none \u00bb code integer true none none"},{"location":"developer-guide/job/#start-job","title":"Start Job","text":"<p>POST /discovery-jobs/{job_id}/start</p> <p>start job.</p>"},{"location":"developer-guide/job/#request_1","title":"Request","text":"Parameter Name Required Type Description job_id Yes integer - <p>Response Examples</p> <p>Success</p> <pre><code>{\n  \"status\": \"success\",\n  \"code\": 1001,\n  \"message\": \"Operation succeeded\"\n}\n</code></pre>"},{"location":"developer-guide/job/#responses_1","title":"Responses","text":"HTTP Status Code Meaning Description Data schema 1001 OK Success Inline"},{"location":"developer-guide/job/#responses-data-schema_1","title":"Responses Data Schema","text":"<p>HTTP Status Code 1001</p> Name Type Required Restrictions Title description \u00bb message string true none none \u00bb code integer true none none"},{"location":"developer-guide/source/","title":"Source code","text":"<p>Visit our GitHub repository to download the templates and scripts for this solution. The Sensitive Data Protection on AWS Solution template is generated using the AWS Cloud Development Kit (CDK). Refer to the README.md file for additional information.</p>"},{"location":"plan-deployment/cost/","title":"Cost","text":"<p>Important</p> <p>The following cost estimations are examples and may vary depending on your environment.</p> <p>You will be responsible for the cost of the AWS services used when running the solution. As of this revision, the following examples are cost estimations for running this solution with default configurations in the US East (N. Virginia) Region and and the China (Ningxia) Region Operated by NWCD. The total cost includes the fees for sensitive data detection and solution console.</p>"},{"location":"plan-deployment/cost/#sensitive-data-detection","title":"Sensitive data detection","text":"<p>The solution cost is mainly composed of the sensitive data detection jobs, which may vary depending on the duration of the detection jobs and each specific task. The longer a detection job runs, the higher the costs are. There are three primary factors that influence the duration of the detection jobs:</p> <ul> <li> <p>Number of identifiers for sensitive information detection: The solution detects sensitive information using predefined detection identifiers. Before performing the detection, you need to select the required detection identifiers. The more identifiers you load, the longer the sensitive information detection process will take, resulting in an increased overall runtime and higher costs.</p> </li> <li> <p>Connected data source types: The efficiency of sensitive information detection jobs is influenced by the throughput or network bandwidth of different data sources. For example, the detection task may be affected by the network bandwidth of an RDS database. Data sources with higher performance typically lead to faster completion of sensitive information detection jobs.</p> </li> <li> <p>Data volume and content: Larger data volumes generally require more time for processing. In addition to the data volume in the data source, factors such as data format, sampling size, and detection depth within the detection task also impact the task duration.</p> </li> </ul> <p>The following examples illustrate cost estimations in two scenarios for a single detection task, with monthly billing for AWS services. In case of a periodic automatic execution of detection jobs without significant changes in data volume, the resulting costs will accumulate based on the actual number of executions per month.</p>"},{"location":"plan-deployment/cost/#detecting-sensitive-data-in-the-database-amazon-rds","title":"Detecting sensitive data in the database (Amazon RDS)","text":"<ul> <li>US East (N. Virginia) Region (us-east-1)</li> </ul> Database Type Data Volume Identifier Count Detection Time Detection Data Cost (USD) Scan Data Cost (USD) Total Cost (USD) Amazon RDS Aurora db.r5.large 10 tables, each with 10 columns, 5000 rows of text data 10 Approximately 6 minutes $0.07 $0.05 $0.12 Amazon RDS MySQL db.m5.xlarge 1000 tables, each with 100 columns, 1000 rows of text data 10 Approximately 11 hours $0.07 $4.84 $4.91 <ul> <li>China (Ningxia) Region Operated by NWCD (cn-northwest-1)</li> </ul> Database Type Data Volume Identifier Count Detection Time Detection Data Cost (CNY) Scan Data Cost (CNY) Total Cost (CNY) Amazon RDS Aurora db.r5.large 10 tables, each with 10 columns, 5000 rows of text data 10 Approximately 6 minutes $0.5 $0.3 $0.8 Amazon RDS MySQL db.m5.xlarge 1000 tables, each with 100 columns, 1000 rows of text data 10 Approximately 11 hours $0.5 $33.23 $33.73"},{"location":"plan-deployment/cost/#detecting-sensitive-data-in-the-s3-bucket","title":"Detecting sensitive data in the S3 bucket","text":"<ul> <li>US East (N. Virginia) Region (us-east-1)</li> </ul> File Total Size Identifier Count Detection Time Detection Data Cost (USD) Scan Data Cost (USD) Total Cost (USD) Total 5,000 files, including PDF, WORD, JPG, TXT, etc. 4GB 10 Approximately 8 hours $0.10 $1.29 $1.39 Total 1,000 files, log files 24GB 13 Approximately 22 hours $0.1 9.97 9.98 Total 20,000 files, structured data such as CSV, JSON, etc. 5GB 20 Approximately 1 hour $0.15 $0.34 $0.39 <ul> <li>China (Ningxia) Region Operated by NWCD (cn-northwest-1)</li> </ul> File Total Size Identifier Count Detection Time Detection Data Cost (CNY) Scan Data Cost (CNY) Total Cost (CNY) Total 5,000 files, including PDF, WORD, JPG, TXT, etc. 4GB 10 Approximately 8 hours $0.61 $23.80 $24.41 Total 1,000 files, log files 24GB 13 Approximately 22 hours $0.42 $68.43 $68.85 Total 20,000 files, structured data such as CSV, JSON, etc. 5GB 20 Approximately 1 hour $1 $2.32 $3.32"},{"location":"plan-deployment/cost/#aws-service-pricing","title":"AWS service pricing","text":"<p>Sensitive data detection jobs utilize AWS Glue and Amazon SageMaker services together to perform sensitive information detection. Therefore, the primary cost of sensitive data detection jobs comes from the runtime costs of these two services. You can refer to the following links to view the specific pricing of these two services in your Region.</p> <ul> <li> <p>Amazon SageMaker Pricing</p> </li> <li> <p>Amazon Glue Pricing</p> </li> </ul>"},{"location":"plan-deployment/cost/#solution-console","title":"Solution console","text":"<p>The deployment of the solution automatically creates a web-based console accessible through a browser. As of this revision, with default settings and an estimated access count of 1000, the following costs will be incurred:</p> <ul> <li>US East (N. Virginia) Region (us-east-1) for one month (calculated as 30 days):</li> </ul> Service Usage Monthly Cost (USD) Amazon Relational Database Service for MySQL Community Edition 720 hours $97.92 Amazon Relational Database Service Provisioned Storage 20 GB-month $4.6 Amazon Elastic Compute Cloud NatGateway 30 GB-month $1.35 720 hours $32.4 Athena 0.010 TB $0.05 CloudWatch 0.100 GB-month $0.05 Elastic Load Balancing - Application 10 hours 0 0.105 LCU-hours 0 Lambda 50000 Lambda-GB-sec $0.08333 10000 requests 0 Simple Queue Service (SQS) 100000 requests $0.04 Simple Storage Service (S3) 2000 requests $0.01 4000 requests $0.0002 1 GB $0.023 Total $136.53 <ul> <li>China (Ningxia) Region Operated by NWCD (cn-northwest-1) for one month (calculated as 30 days):</li> </ul> Service Usage Monthly Cost (CNY) Amazon Relational Database Service for MySQL Community Edition 720 hours $576 Amazon Relational Database Service Provisioned Storage 20 GB-Month $30.62 Amazon Elastic Compute Cloud NatGateway 30 GB-Month $11.1 720 hours $266.4 Athena 0.010 TB $0.34 CloudWatch 0.100 GB-Month $0.24 EC2 Container Registry (ECR) 0.003 GB-Month $0.69 Elastic Load Balancing - Application 10 hours $1.56 0.105 LCU-Hours 0 Lambda 100,000.000 Lambda-GB-Second $11.35 10,000 requests 0 Simple Queue Service 100,000 requests $3.33 Simple Storage Service 2,000 requests $0.01 4,000 requests $0.01 0.032 GB-Month $0.176 Total $901.82"},{"location":"plan-deployment/cost/#cost-optimization-recommendations","title":"Cost Optimization Recommendations","text":"<p>The basic cost of performing detection jobs is billed on-demand, which means you only pay for the resources you actually use without needing to purchase or reserve any capacity in advance. Additionally, AWS provides some free usage credits to help you understand and evaluate your service usage. Therefore, we recommend using the AWS Cost Explorer feature in the solution to help manage costs. All resources in this solution include a tag with the name Owner and the value SDPS. You can query the costs generated by the solution by applying user-defined cost allocation tags. Prices may vary. For more information, please refer to the pricing pages for each AWS service used in this solution. Here are some cost optimization recommendations that you can implement when creating sensitive data detection jobs to reduce costs:</p> <ul> <li> <p>Before performing sensitive information detection jobs, carefully select appropriate identifiers for sensitive information instead of selecting all.   Having too many detection identifiers typically requires more time to complete the sensitive information detection. Before executing the detection task, adjust the identifiers using the add/remove sensitive information identifiers feature in the detection template to reduce the execution time of the detection task.</p> </li> <li> <p>Set an appropriate detection depth and sample size based on the actual data volume to accurately detect sensitive information.   In detection scenarios with large data volumes and primarily structured information, such as RDS databases, selecting a smaller sample size can detect sensitive information and achieve cost optimization.</p> </li> <li> <p>Adjust the execution frequency of scheduled detection jobs based on the actual situation.   The frequency of detection task execution has a significant impact on costs. If the data changes are not significant, you can choose on-demand scanning instead of scheduled scanning when creating sensitive information detection jobs to reduce the frequency of task execution.</p> </li> </ul>"},{"location":"plan-deployment/regions/","title":"Regional Deployment","text":"<p>This solution uses services which may not be currently available in all AWS Regions. Launch this solution in an AWS Region where required services are available. For the most current availability by Region, refer to the AWS Regional Services List.</p>"},{"location":"plan-deployment/regions/#supported-regions-for-deployment-in-aws-global-regions","title":"Supported regions for deployment in AWS Global Regions","text":"Region Name Region ID US East (N. Virginia) Region us-east-1 US East (Ohio) Region us-east-2 US West (N. California) Region us-west-1 US West (Oregon) Region us-west-2 Asia Pacific (Mumbai) Region ap-south-1 Asia Pacific (Tokyo) Region ap-northeast-1 Asia Pacific (Seoul) Region ap-northeast-2 Asia Pacific (Singapore) Region ap-southeast-1 Asia Pacific (Sydney) Region ap-southeast-2 Canada (Central) Region ca-central-1 Europe (Ireland) Region eu-west-1 Europe (London) Region eu-west-2 Europe (Paris) Region eu-west-3 Europe (Frankfurt) Region eu-central-1 South America (Sao Paulo) Region sa-east-1"},{"location":"plan-deployment/regions/#supported-regions-for-deployment-in-aws-china-regions","title":"Supported regions for deployment in AWS China Regions","text":"Region Name Region ID AWS China (Beijing) Region operated by Sinnet cn-north-1 AWS China (Ningxia) Region operated by NWCD cn-northwest-1"},{"location":"plan-deployment/security/","title":"Security Information","text":"<p>When you build systems on AWS infrastructure, security responsibilities are shared between you and AWS. This Shared Responsibility Model reduces your operational burden because AWS operates, manages, and controls the components including the host operating system, the virtualization layer, and the physical security of the facilities in which the services operate. For more information about AWS security, visit AWS Cloud Security.</p>"},{"location":"plan-deployment/security/#iam-roles","title":"IAM roles","text":"<p>AWS Identity and Access Management (IAM) roles allow customers to assign fine-grained access policies and permissions to services and users on AWS. This solution creates IAM roles that grant access between components of the solution.</p>"},{"location":"plan-deployment/security/#monitoring-services-using-amazon-cloudwatch-alarms","title":"Monitoring services using Amazon CloudWatch alarms","text":"<p>You can set up alarms to monitor and receive alerts about your AWS resources on the alarms dashboard in Amazon CloudWatch. Generally, we recommend configuring your alarms to notify you whenever a metric starts excessively over or under-utilizes a particular resource, such as high CPU or memory usage. This can be an indicator that your service is experiencing a DoS-style attack. Additionally, it may be necessary to set alarms when your data storage container, such as RDS, approaches near 100% capacity utilization, because this could indicate a resource starvation or exhaustion-style attack.</p> <p>Warning</p> <p>There could be additional cost for CloudWatch alarms.</p> <p>In AWS China Regions (cn-north-1 and cn-northwest-1), you can create RDS and NAT Gateway metrics in alarms. </p> <p>In AWS Global Regions, you can enable more services metrics like Lambda, SQS, Application Load Balancer.</p> <p>For example, if you want to create alarms to monitor ActiveConnectionCount in NATGateway using the CloudWatch console, follow the steps below.</p> <ol> <li>Sign in to the AWS Management Console.</li> <li>Access the CloudWatch console.</li> <li>In the navigation pane, choose All alarms under Alarms, and then choose Create alarm.</li> <li>Choose Select metric, and choose NATGateway in metrics.</li> <li>Search for the metric ActiveConnectionCount, click it and select it.</li> <li>Choose Select metric.</li> <li>In Conditions, define the alarm condition whenever ActiveConnectionCount is greater than 100. Then choose Next.</li> <li>In the Notification, configure CloudWatch to send you an email when the alarm state is triggered.</li> <li>Choose Next.</li> <li>Enter a name and description for the alarm and Create alarm.</li> </ol>"},{"location":"plan-deployment/security/#enabling-access-logging-for-the-admin-account-s3-bucket","title":"Enabling access logging for the Admin account S3 bucket","text":"<p>After deploying the solution, you can enable access logs for the Admin account's S3 bucket to detect and prevent security issues. Enabling logging allows Amazon S3 to deliver access logs to a destination bucket of your choice. The destination bucket must be within the same AWS Region and AWS account as the Admin account's S3 bucket.</p> <p>If you want to enable access logging for the Admin account S3 bucket using the S3 console, follow the steps below.</p> <ol> <li>Sign in to the AWS Management Console.</li> <li>Access the S3 console.</li> <li>Find the S3 bucket after the CloudFormation stack deployment, the bucket name is start with the CloudFormation stack name.</li> <li>Click on the \"Properties\" tab.</li> <li>Scroll down to the \"Server access logging\" section and click on \"Edit\".</li> <li>Choose the option \"Enable\" to enable access logging for the S3 bucket.</li> <li>In the \"Target bucket\" field, select the bucket where you want to store the access logs.</li> <li>Click on \"Save\" to enable access logging for the S3 bucket.</li> </ol>"},{"location":"solution-overview/concepts-and-definitions/","title":"Concepts and definitions","text":"<ul> <li>Data source: AWS resources where data is stored, such as Amazon S3, and Amazon RDS.</li> <li>Data catalog: A repository of metadata of a data source, allowing you to manage data at the column level. For example, you can view the table schema, sample data of a particular column, and add labels to specific data fields.</li> <li>Data identifier: The rule used to detect data. You can define custom data identifiers using RegEx and keywords.</li> <li>Classification template: A collection of data identifiers. Data identifiers are rules used to detect data.</li> <li>Sensitive data discovery job: A job that uses a template to detect sensitive data. The job automatically labels sensitive data in the data catalog.</li> <li>Glue job: A job that is triggered by a sensitive data discovery job to scan sensitive data using AWS Glue. One discovery job can trigger AWS Glue jobs in multiple AWS accounts in a distributed manner.</li> </ul>"},{"location":"solution-overview/features-and-benefits/","title":"Features and benefits","text":"<p>The solution includes the following features:</p> <p>Data discovery: supports various data sources, such as Amazon S3 and Amazon RDS, across multiple AWS accounts. The solution allows you to easily create a data catalog and run sensitive data discovery jobs. The solution leverages not only pattern-based discovery but also ML classification based on deep learning Natural Language Processing (NLP) and Named Entity Recognition (NER).</p> <p>Flexible data classification: defines data classification templates for detecting privacy data, such as personal information. The solution allows you to define custom sensitive data types or choose from over 200 built-in data types.</p> <p>Centralized data visualization: provides the dashboards with an overview of the data catalogs and sensitive data status, such as data location, and data sources.</p>"},{"location":"update/update/","title":"Update","text":"<p>Time to update: Approximately 20 minutes</p>"},{"location":"update/update/#update-overview","title":"Update overview","text":"<p>Important</p> <p>Currently, upgrading from 1.0 to 1.1 is not supported. Please remove 1.0 and redeploy 1.1. If there is indeed a need, please contact us.</p> <p>Important</p> <p>Please confirm that no jobs are running before updating.</p> <p>This page is used to guide how to update to the latest version after deploying an old version. Use the following steps to update this solution on AWS. </p> <ul> <li>Step 1.Update the Admin stack in admin account</li> <li>Step 2.Update the Agent stack in monitored account(s)</li> </ul>"},{"location":"update/update/#update-steps","title":"Update steps","text":""},{"location":"update/update/#step-1-update-the-admin-stack","title":"Step 1. Update the admin stack","text":"<p>Important</p> <p>The new template url must be consistent with the previous template url, otherwise the update will fail. That is, the previous stack used the new VPC template, and the new stack must also use the new VPC template.Similarly, the previous stack used the existing VPC template, and the new stack must also use the existing VPC template.</p> <ol> <li>Sign in to the AWS management console, enter the CloudFormation service, select the previously deployed stack, and then click the Update button. </li> <li>Select Replace Current Template, then enter the template url in the Amazon S3 URL input box, and then click Next. Template Address Reference CloudFormation template. </li> <li>On the specified stack details page, click Next.  </li> <li>On the Configure stack options page, click Next.</li> <li>On the Review page, review and confirm the settings. Select 3 checkboxes that I acknowledge.Choose Create stack to deploy the stack.</li> <li>When upgrading from 1.1.0 to subsequent versions, please manually delete 4 ENIs with a status of Available after 5 minutes of CloudFormat update: 2 descriptions contain PortalConfigFunction and 2 descriptions contain PortalFunction\u3002 </li> <li>Wait for about 20 minutes to ensure that all related resource are updated. You can choose the Resource and Event tab to see the status of the stack. After the update is successful, you can reopen the administrator page.</li> </ol> <p>Important</p> <p>You need to manually delete ENI, otherwise the upgrade will fail.</p>"},{"location":"update/update/#step-2-update-the-agent-stack","title":"Step 2. Update the agent stack","text":"<p>The operation steps are the same as updating the admin stack. Please note that when entering the template url, simply enter the agent template url.</p> <p>Important</p> <p>The agent must be updated at the same time as the admin version, otherwise when the version does not match, the job(s) will run with an error.</p>"},{"location":"user-guide/appendix-build-in-identifiers-eu-gdpr/","title":"Appx.EU PII identifiers(GDPR reference)","text":"<p>Having a continuous data categorization process is essential for adjusting secure data processing according to the nature of the data. If your organization manages sensitive data, please monitor where the data is stored, protect it appropriately, and provide evidence that you are implementing mandatory requirements for data security and privacy to meet regulatory compliance needs.</p> <p>About GDPR</p> <p>The General Data Protection Regulation (GDPR) is a significant data protection regulation enacted by the European Union (EU), which came into effect on May 25, 2018. GDPR aims to strengthen and unify the protection of personal data (PII) within all EU member states, while also affecting companies outside the EU, as long as they process the data of EU citizens.</p>"},{"location":"user-guide/appendix-build-in-identifiers-eu-gdpr/#general-data-identifiers","title":"General Data Identifiers","text":"Data Identifier Description PERSON_NAME Personal Name (English/Latin specific) EMAIL Email Address CREDIT_CARD Credit Card Number"},{"location":"user-guide/appendix-build-in-identifiers-eu-gdpr/#data-identifiers-categorized-by-european-countries-andor-regions","title":"Data Identifiers Categorized by European Countries and/or Regions","text":"<p>(The following are listed in alphabetical order of countries and/or regions)</p> Country Data Identifier Description Austria AUSTRIA_DRIVING_LICENSE Driving License Number (Specific to Austria) Austria AUSTRIA_PASSPORT_NUMBER Passport Number (Specific to Austria) Austria AUSTRIA_SSN Austrian Social Security Number (For Austrian citizens) Austria AUSTRIA_TAX_IDENTIFICATION_NUMBER Tax Identification Number (Specific to Austria) Austria AUSTRIA_VALUE_ADDED_TAX Value Added Tax (Specific to Austria) Belgium BELGIUM_DRIVING_LICENSE Driving License Number (Specific to Belgium) Belgium BELGIUM_NATIONAL_IDENTIFICATION_NUMBER Belgian National Number (BNN) Belgium BELGIUM_PASSPORT_NUMBER Passport Number (Specific to Belgium) Belgium BELGIUM_TAX_IDENTIFICATION_NUMBER Tax Identification Number (Specific to Belgium) Belgium BELGIUM_VALUE_ADDED_TAX Value Added Tax (Specific to Belgium) Bosnia BOSNIA_UNIQUE_MASTER_CITIZEN_NUMBER Unique Master Citizen Number (JMBG) for Bosnia-Herzegovina Citizens Bulgaria BULGARIA_DRIVING_LICENSE Driving License Number (Specific to Bulgaria) Bulgaria BULGARIA_UNIFORM_CIVIL_NUMBER Uniform Civil Number (EGN) used as a national identification number Bulgaria BULGARIA_VALUE_ADDED_TAX Value Added Tax (Specific to Bulgaria) Croatia CROATIA_DRIVING_LICENSE Driving License Number (Specific to Croatia) Croatia CROATIA_IDENTITY_NUMBER National Identification Number (Specific to Croatia) Croatia CROATIA_PASSPORT_NUMBER Passport Number (Specific to Croatia) Croatia CROATIA_PERSONAL_IDENTIFICATION_NUMBER Personal Identification Number (OIB) Cyprus CYPRUS_DRIVING_LICENSE Driving License Number (Specific to Cyprus) Cyprus CYPRUS_NATIONAL_IDENTIFICATION_NUMBER Cyprus Identity Card Cyprus CYPRUS_PASSPORT_NUMBER Passport Number (Specific to Cyprus) Cyprus CYPRUS_TAX_IDENTIFICATION_NUMBER Tax Identification Number (Specific to Cyprus) Cyprus CYPRUS_VALUE_ADDED_TAX Value Added Tax (Specific to Cyprus) Czechia CZECHIA_DRIVING_LICENSE Driving License Number (Specific to Czechia) Czechia CZECHIA_PERSONAL_IDENTIFICATION_NUMBER Personal Identification Number (Specific to Czechia) Czechia CZECHIA_VALUE_ADDED_TAX Value Added Tax (Specific to Czechia) Denmark DENMARK_DRIVING_LICENSE Driving License Number (Specific to Denmark) Denmark DENMARK_PERSONAL_IDENTIFICATION_NUMBER Personal Identification Number (Specific to Denmark) Denmark DENMARK_TAX_IDENTIFICATION_NUMBER Tax Identification Number (Specific to Denmark) Denmark DENMARK_VALUE_ADDED_TAX Value Added Tax (Specific to Denmark) Estonia ESTONIA_DRIVING_LICENSE Driving License Number (Specific to Estonia) Estonia ESTONIA_PASSPORT_NUMBER Passport Number (Specific to Estonia) Estonia ESTONIA_PERSONAL_IDENTIFICATION_CODE Personal Identification Code (Specific to Estonia) Estonia ESTONIA_VALUE_ADDED_TAX Value Added Tax (Specific to Estonia) Finland FINLAND_DRIVING_LICENSE Driving License Number (Specific to Finland) Finland FINLAND_HEALTH_INSURANCE_NUMBER Health Insurance Number (Specific to Finland) Finland FINLAND_NATIONAL_IDENTIFICATION_NUMBER National Identification Number (Specific to Finland) Finland FINLAND_PASSPORT_NUMBER Passport Number (Specific to Finland) Finland FINLAND_VALUE_ADDED_TAX Value Added Tax (Specific to Finland) France FRANCE_BANK_ACCOUNT Bank Account Number (Specific to France) France FRANCE_DRIVING_LICENSE Driving License Number (Specific to France) France FRANCE_HEALTH_INSURANCE_NUMBER French Health Insurance France FRANCE_INSEE_CODE French Social Security or National Identity Card Number France FRANCE_NATIONAL_IDENTIFICATION_NUMBER French National Identification Number (CNI) France FRANCE_PASSPORT_NUMBER Passport Number (Specific to France) France FRANCE_TAX_IDENTIFICATION_NUMBER Tax Identification Number (Specific to France) France FRANCE_VALUE_ADDED_TAX Value Added Tax (Specific to France) Germany GERMANY_BANK_ACCOUNT German Bank Account Number Germany GERMANY_DRIVING_LICENSE German Driving License Number Germany GERMANY_PASSPORT_NUMBER German Passport Number Germany GERMANY_PERSONAL_IDENTIFICATION_NUMBER German Personal Identification Number Germany GERMANY_TAX_IDENTIFICATION_NUMBER German Tax Identification Number Germany GERMANY_VALUE_ADDED_TAX German Value Added Tax Greece GREECE_DRIVING_LICENSE Greek Driving License Number Greece GREECE_PASSPORT_NUMBER Greek Passport Number Greece GREECE_SSN Greek Social Security Number Greece GREECE_TAX_IDENTIFICATION_NUMBER Greek Tax Identification Number Greece GREECE_VALUE_ADDED_TAX Greek Value Added Tax Hungary HUNGARY_DRIVING_LICENSE Hungarian Driving License Number Hungary HUNGARY_PASSPORT_NUMBER Hungarian Passport Number Hungary HUNGARY_SSN Hungarian Social Security Number Hungary HUNGARY_TAX_IDENTIFICATION_NUMBER Hungarian Tax Identification Number Hungary HUNGARY_VALUE_ADDED_TAX Hungarian Value Added Tax Iceland ICELAND_NATIONAL_IDENTIFICATION_NUMBER Icelandic National Identity Card Number Iceland ICELAND_PASSPORT_NUMBER Icelandic Passport Number Iceland ICELAND_VALUE_ADDED_TAX Icelandic Value Added Tax Ireland IRELAND_DRIVING_LICENSE Irish Driving License Number Ireland IRELAND_PASSPORT_NUMBER Irish Passport Number Ireland IRELAND_PERSONAL_PUBLIC_SERVICE_NUMBER Irish Personal Public Service Number Ireland IRELAND_TAX_IDENTIFICATION_NUMBER Irish Tax Identification Number Ireland IRELAND_VALUE_ADDED_TAX Irish Value Added Tax Italy ITALY_BANK_ACCOUNT Italian Bank Account Number Italy ITALY_DRIVING_LICENSE Italian Driving License Number Italy ITALY_FISCAL_CODE Italian Fiscal Code Italy ITALY_PASSPORT_NUMBER Italian Passport Number Italy ITALY_VALUE_ADDED_TAX Italian Value Added Tax Latvia LATVIA_DRIVING_LICENSE Latvian Driving License Number Latvia LATVIA_PASSPORT_NUMBER Latvian Passport Number Latvia LATVIA_PERSONAL_IDENTIFICATION_NUMBER Latvian Personal Identification Number Latvia LATVIA_VALUE_ADDED_TAX Latvian Value Added Tax Liechtenstein LIECHTENSTEIN_NATIONAL_IDENTIFICATION_NUMBER Liechtenstein National Identification Number Liechtenstein LIECHTENSTEIN_PASSPORT_NUMBER Liechtenstein Passport Number Liechtenstein LIECHTENSTEIN_TAX_IDENTIFICATION_NUMBER Liechtenstein Tax Identification Number Lithuania LITHUANIA_DRIVING_LICENSE Lithuanian Driving License Number Lithuania LITHUANIA_PERSONAL_IDENTIFICATION_NUMBER Lithuanian Personal Identification Number Lithuania LITHUANIA_TAX_IDENTIFICATION_NUMBER Lithuanian Tax Identification Number Lithuania LITHUANIA_VALUE_ADDED_TAX Lithuanian Value Added Tax Luxembourg LUXEMBOURG_DRIVING_LICENSE Luxembourgish Driving License Number Luxembourg LUXEMBOURG_NATIONAL_INDIVIDUAL_NUMBER Luxembourgish National Individual Number Luxembourg LUXEMBOURG_PASSPORT_NUMBER Luxembourgish Passport Number Luxembourg LUXEMBOURG_TAX_IDENTIFICATION_NUMBER Luxembourgish Tax Identification Number Luxembourg LUXEMBOURG_VALUE_ADDED_TAX Luxembourgish Value Added Tax Montenegro MONTENEGRO_UNIQUE_MASTER_CITIZEN_NUMBER Montenegrin Unique Master Citizen Number Netherlands NETHERLANDS_BANK_ACCOUNT Dutch Bank Account Number Netherlands NETHERLANDS_CITIZEN_SERVICE_NUMBER Dutch Citizen Service Number (BSN) Netherlands NETHERLANDS_DRIVING_LICENSE Dutch Driving License Number Netherlands NETHERLANDS_PASSPORT_NUMBER Dutch Passport Number Netherlands NETHERLANDS_TAX_IDENTIFICATION_NUMBER Dutch Tax Identification Number Netherlands NETHERLANDS_VALUE_ADDED_TAX Dutch Value Added Tax Norway NORWAY_BIRTH_NUMBER Norwegian Birth Number Norway NORWAY_DRIVING_LICENSE Norwegian Driving License Number Norway NORWAY_HEALTH_INSURANCE_NUMBER Norwegian Health Insurance Number Norway NORWAY_NATIONAL_IDENTIFICATION_NUMBER Norwegian National Identification Number Norway NORWAY_VALUE_ADDED_TAX Norwegian Value Added Tax Poland POLAND_DRIVING_LICENSE Polish Driving License Number Poland POLAND_IDENTIFICATION_NUMBER Polish Identification Number Poland POLAND_PASSPORT_NUMBER Polish Passport Number Poland POLAND_REGON_NUMBER Polish Statistical Identification Number (REGON) Poland POLAND_SSN Polish Social Security Number Poland POLAND_TAX_IDENTIFICATION_NUMBER Polish Tax Identification Number Poland POLAND_VALUE_ADDED_TAX Polish Value Added Tax Portugal PORTUGAL_DRIVING_LICENSE Portuguese Driving License Number Portugal PORTUGAL_NATIONAL_IDENTIFICATION_NUMBER Portuguese National Identification Number Portugal PORTUGAL_PASSPORT_NUMBER Portuguese Passport Number Portugal PORTUGAL_TAX_IDENTIFICATION_NUMBER Portuguese Tax Identification Number Portugal PORTUGAL_VALUE_ADDED_TAX Portuguese Value Added Tax Romania ROMANIA_DRIVING_LICENSE Romanian Driving License Number Romania ROMANIA_NUMERICAL_PERSONAL_CODE Romanian Numerical Personal Code Romania ROMANIA_PASSPORT_NUMBER Romanian Passport Number Romania ROMANIA_VALUE_ADDED_TAX Romanian Value Added Tax Serbia SERBIA_UNIQUE_MASTER_CITIZEN_NUMBER Serbian Unique Master Citizen Number Serbia SERBIA_VALUE_ADDED_TAX Serbian Value Added Tax Vojvodina VOJVODINA_UNIQUE_MASTER_CITIZEN_NUMBER Vojvodina Unique Master Citizen Number (Specific to Serbia) Slovakia SLOVAKIA_DRIVING_LICENSE Slovakian Driving License Number Slovakia SLOVAKIA_NATIONAL_IDENTIFICATION_NUMBER Slovakian National Identification Number Slovakia SLOVAKIA_PASSPORT_NUMBER Slovakian Passport Number Slovakia SLOVAKIA_VALUE_ADDED_TAX Slovakian Value Added Tax Slovenia SLOVENIA_DRIVING_LICENSE Slovenian Driving License Number Slovenia SLOVENIA_PASSPORT_NUMBER Slovenian Passport Number Slovenia SLOVENIA_TAX_IDENTIFICATION_NUMBER Slovenian Tax Identification Number Slovenia SLOVENIA_UNIQUE_MASTER_CITIZEN_NUMBER Slovenian Unique Master Citizen Number (JMBG) Slovenia SLOVENIA_VALUE_ADDED_TAX Slovenian Value Added Tax Spain SPAIN_DNI Spanish National Identity Document (DNI) Spain SPAIN_DRIVING_LICENSE Spanish Driving License Number Spain SPAIN_PASSPORT_NUMBER Spanish Passport Number Spain SPAIN_SSN Spanish Social Security Number Spain SPAIN_BANK_ACCOUNT Spanish bank account Spain SPAIN_VALUE_ADDED_TAX Spanish Value Added Tax Spain SPAIN_NIE Spanish foreigner ID (N\u00famero de Identificaci\u00f3n del Extranjero) Spain SPAIN_NIF Spanish local tax number (N\u00famero de Identificaci\u00f3n Fiscal) Sweden SWEDEN_DRIVING_LICENSE Swedish Driving License Number Sweden SWEDEN_PASSPORT_NUMBER Swedish Passport Number Sweden SWEDEN_PERSONAL_IDENTIFICATION_NUMBER Swedish Personal Identification Number Sweden SWEDEN_TAX_IDENTIFICATION_NUMBER Swedish Tax Identification Number Sweden SWEDEN_VALUE_ADDED_TAX Swedish Value Added Tax Switzerland SWITZERLAND_AHV Swiss AHV Number (Social Security Number) Switzerland SWITZERLAND_HEALTH_INSURANCE_NUMBER Swiss Health Insurance Number Switzerland SWITZERLAND_PASSPORT_NUMBER Swiss Passport Number Switzerland SWITZERLAND_VALUE_ADDED_TAX Swiss Value Added Tax UK UK_BANK_ACCOUNT UK Bank Account Number UK UK_BANK_SORT_CODE UK Bank Sort Code UK UK_DRIVING_LICENSE UK Driving License Number UK UK_ELECTORAL_ROLL_NUMBER UK Electoral Roll Number UK UK_NATIONAL_HEALTH_SERVICE_NUMBER UK National Health Service Number UK UK_NATIONAL_INSURANCE_NUMBER UK National Insurance Number UK UK_PASSPORT_NUMBER UK Passport Number UK UK_PHONE_NUMBER UK Phone Number UK UK_UNIQUE_TAXPAYER_REFERENCE_NUMBER UK Unique Taxpayer Reference Number UK UK_VALUE_ADDED_TAX UK Value Added Tax"},{"location":"user-guide/appendix-built-in-identifiers/","title":"Appendix - Built-in data identifiers","text":"<p>The solution uses machine learning and pattern matching to detect sensitive data. These are 200+ built-in data identifiers for general and country-specific data types. </p> <p>For more information</p> <p>In this solution, some built-in data identifiers, such as person names and addresses, are defined based on AI, while others are defined based on Regex/keywords.</p>"},{"location":"user-guide/appendix-built-in-identifiers/#general-data-identifiers","title":"General data identifiers","text":"Data identifier Description PERSON_NAME Person name (English/Latin specific) EMAIL The email address DATE The date in general format PHONE_NUMBER The phone number (mainly US) BANK_ACCOUNT The bank card (mainly US and Canada specific) CREDIT_CARD The credit card number (mainly US/universal format) IP_ADDRESS The IP address MAC_ADDRESS The Mac address"},{"location":"user-guide/appendix-built-in-identifiers/#country-specific-data-identifiers","title":"Country-specific data identifiers","text":"Country Data identifier Description Argentina ARGENTINA_TAX_IDENTIFICATION_NUMBER Argentina Tax Identification Number, also known as CUIT or CUIL Australia AUSTRALIA_BUSINESS_NUMBER Australia Business Number (ABN). A unique identifier issued by the Australian Business Register (ABR) to identify businesses to the government and community. Australia AUSTRALIA_COMPANY_NUMBER Australia Company Number (ACN). Unique identifier issued by the Australian Securities and Investments Commission. Australia AUSTRALIA_DRIVING_LICENSE The driver license number (Australia specific) Australia AUSTRALIA_MEDICARE_NUMBER Australian Medicare Number. Personal identifier issued by the Australian Health Insurance Commission. Australia AUSTRALIA_PASSPORT_NUMBER The passport number (Australia specific) Australia AUSTRALIA_TAX_FILE_NUMBER Australia Tax File Number (TFN). Issued by the Australian Taxation Office (ATO) to taxpayers (individual, company, etc) for tax dealings. Austria AUSTRIA_DRIVING_LICENSE The driver license number (Austria specific) Austria AUSTRIA_PASSPORT_NUMBER The passport number (Austria specific) Austria AUSTRIA_SSN The social security number (for Austria persons) Austria AUSTRIA_TAX_IDENTIFICATION_NUMBER Tax identification number (Austria specific) Austria AUSTRIA_VALUE_ADDED_TAX Value-Added Tax (Austria specific) Belgium BELGIUM_DRIVING_LICENSE The driver license number (Belgium specific) Belgium BELGIUM_NATIONAL_IDENTIFICATION_NUMBER The Belgian National Number (BNN) Belgium BELGIUM_PASSPORT_NUMBER The passport number (Belgium specific) Belgium BELGIUM_TAX_IDENTIFICATION_NUMBER Tax identification number (Belgium specific) Belgium BELGIUM_VALUE_ADDED_TAX Value-Added Tax (Belgium specific) Bosnia BOSNIA_UNIQUE_MASTER_CITIZEN_NUMBER Unique master citizen number (JMBG) for Bosnia-Herzegovina citizens Brazil BRAZIL_BANK_ACCOUNT The bank account number (Brazil specific) Brazil BRAZIL_NATIONAL_IDENTIFICATION_NUMBER The national identifier (Brazil specific) Brazil BRAZIL_NATIONAL_REGISTRY_OF_LEGAL_ENTITIES_NUMBER The identification number issued to companies (Brazil specific), also known as the CNPJ Brazil BRAZIL_NATURAL_PERSON_REGISTRY_NUMBER The identification number issued to person (Brazil specific) Bulgaria BULGARIA_DRIVING_LICENSE The driver license number (Bulgaria specific) Bulgaria BULGARIA_UNIFORM_CIVIL_NUMBER Unified Civil Number (EGN) that serves as a national identification number Bulgaria BULGARIA_VALUE_ADDED_TAX Value-Added Tax (Bulgaria specific) Canada CANADA_DRIVING_LICENSE The driver license number (Canada specific) Canada CANADA_GOVERNMENT_IDENTIFICATION_CARD_NUMBER The national identifier (Canada specific) Canada CANADA_PASSPORT_NUMBER The passport number (Canada specific) Canada CANADA_PERMANENT_RESIDENCE_NUMBER Permanent residence number (PR Card number) Canada CANADA_PERSONAL_HEALTH_NUMBER The unique identifier for healthcare (PHN number) Canada CANADA_SOCIAL_INSURANCE_NUMBER The social insurance number (SIN) in Canada Chile CHILE_DRIVING_LICENSE The driver license number (Chile specific) Chile CHILE_NATIONAL_IDENTIFICATION_NUMBER The Chile national identifier, also known as RUT or RUN China and territories CHINA_IDENTIFICATION The identification card id (China specific) China and territories CHINA_PHONE_NUMBER The mobile or landland phone number (China specific) China and territories CHINA_PASSPORT_NUMBER The passport number (China specific) China and territories CHINA_LICENSE_PLATE_NUMBER The car license plate number (China specific) China and territories CHINA_MAINLAND_TRAVEL_PERMIT_ID_TAIWAN A China mainland travel permit id number for Taiwan residents China and territories CHINA_MAINLAND_TRAVEL_PERMIT_ID_HONG_KONG_MACAU A China mainland travel permit id number for Hong Kong and Macau residents China and territories HONG_KONG_IDENTITY_CARD The Hong Kong Identification card id China and territories MACAU_RESIDENT_IDENTITY_CARD The Macau Identification card id China and territories TAIWAN_NATIONAL_IDENTIFICATION_NUMBER The Taiwan Identification card id China and territories TAIWAN_PASSPORT_NUMBER The Taiwan passport number China and territories CHINA_CHINESE_NAME AI-based identifier for detecting Chinese name. (Built-in) China and territories CHINA_ADDRESS AI-based identifier for detecting Chinese address. (Built-in) China and territories CHINA_NATIONALITY Regex-based identifier for detecting Chinese nationality (Built-in) China and territories CHINA_BANK_CARD Regex-based identifier for detecting Chinese bank card Id (Built-in) China and territories CHINA_MARITAL_STATUS Regex-based identifier for detecting Marital status (Built-in) China and territories CHINA_POLITICAL_PARTY Regex-based identifier for detecting Chinese Political party (Built-in) China and territories CHINA_PROVINCE Regex-based identifier for detecting China province (Built-in) China and territories CHINA_GENDER Regex-based identifier for detecting Gender (Built-in) China and territories CHINA_ID_TYPE Regex-based identifier for detecting for Id Card type (Built-in) Colombia COLOMBIA_PERSONAL_IDENTIFICATION_NUMBER Unique identifier assigned to Colombians at birth Colombia COLOMBIA_TAX_IDENTIFICATION_NUMBER Tax identification number (Colombia specific) Croatia CROATIA_DRIVING_LICENSE The driver license number (Croatia specific) Croatia CROATIA_IDENTITY_NUMBER The national identifier (Croatia specific) Croatia CROATIA_PASSPORT_NUMBER The passport number (Croatia specific) Croatia CROATIA_PERSONAL_IDENTIFICATION_NUMBER The personal identifier number (OIB) Cyprus CYPRUS_DRIVING_LICENSE The driver license number (Cyprus specific) Cyprus CYPRUS_NATIONAL_IDENTIFICATION_NUMBER The Cypriot identity card Cyprus CYPRUS_PASSPORT_NUMBER The passport number (Cyprus specific) Cyprus CYPRUS_TAX_IDENTIFICATION_NUMBER Tax identification number (Cyprus specific) Cyprus CYPRUS_VALUE_ADDED_TAX Value-Added Tax (Cyprus specific) Czechia CZECHIA_DRIVING_LICENSE The driver license number (Czechia specific) Czechia CZECHIA_PERSONAL_IDENTIFICATION_NUMBER The personal identifier number (Czechia specific) Czechia CZECHIA_VALUE_ADDED_TAX Value-Added Tax (Czechia specific) Denmark DENMARK_DRIVING_LICENSE The driver license number (Denmark specific) Denmark DENMARK_PERSONAL_IDENTIFICATION_NUMBER The personal identifier number (Denmark specific) Denmark DENMARK_TAX_IDENTIFICATION_NUMBER Tax identification number (Denmark specific) Denmark DENMARK_VALUE_ADDED_TAX Value-Added Tax (Denmark specific) Estonia ESTONIA_DRIVING_LICENSE The driver license number (Estonia specific) Estonia ESTONIA_PASSPORT_NUMBER The passport number (Estonia specific) Estonia ESTONIA_PERSONAL_IDENTIFICATION_CODE The personal identifier number (Estonia specific) Estonia ESTONIA_VALUE_ADDED_TAX Value-Added Tax (Estonia specific) Finland FINLAND_DRIVING_LICENSE The driver license number (Finland specific) Finland FINLAND_HEALTH_INSURANCE_NUMBER The health insurance number (Finland specific) Finland FINLAND_NATIONAL_IDENTIFICATION_NUMBER The national identifier number (Finland specific) Finland FINLAND_PASSPORT_NUMBER The passport number (Finland specific) Finland FINLAND_VALUE_ADDED_TAX Value-Added Tax (Finland specific) France FRANCE_BANK_ACCOUNT The bank account number (France specific) France FRANCE_DRIVING_LICENSE The driver license number (France specific) France FRANCE_HEALTH_INSURANCE_NUMBER France health insurance number France FRANCE_INSEE_CODE France social security, SSN, or NIR number France FRANCE_NATIONAL_IDENTIFICATION_NUMBER France national identifier number (CNI) France FRANCE_PASSPORT_NUMBER The passport number (France specific) France FRANCE_TAX_IDENTIFICATION_NUMBER Tax identification number (France specific) France FRANCE_VALUE_ADDED_TAX Value-Added Tax (France specific) Germany GERMANY_BANK_ACCOUNT The bank account number (Germany specific) Germany GERMANY_DRIVING_LICENSE The driver license number (Germany specific) Germany GERMANY_PASSPORT_NUMBER The passport number (Germany specific) Germany GERMANY_PERSONAL_IDENTIFICATION_NUMBER The personal identifier number (Germany specific) Germany GERMANY_TAX_IDENTIFICATION_NUMBER Tax identification number (Germany specific) Germany GERMANY_VALUE_ADDED_TAX Value-Added Tax (Germany specific) Greece GREECE_DRIVING_LICENSE The driver license number (Greece specific) Greece GREECE_PASSPORT_NUMBER The passport number (Greece specific) Greece GREECE_SSN The social security number (for Greece persons) Greece GREECE_TAX_IDENTIFICATION_NUMBER Tax identification number (Greece specific) Greece GREECE_VALUE_ADDED_TAX Value-Added Tax (Greece specific) Hungary HUNGARY_DRIVING_LICENSE The driver license number (Hungary specific) Hungary HUNGARY_PASSPORT_NUMBER The passport number (Hungary specific) Hungary HUNGARY_SSN The social security number (for Hungary persons) Hungary HUNGARY_TAX_IDENTIFICATION_NUMBER Tax identification number (Hungary specific) Hungary HUNGARY_VALUE_ADDED_TAX Value-Added Tax (Hungary specific) Iceland ICELAND_NATIONAL_IDENTIFICATION_NUMBER The national identifier (Iceland specific) Iceland ICELAND_PASSPORT_NUMBER The passport number (Iceland specific) Iceland ICELAND_VALUE_ADDED_TAX Value-Added Tax (Iceland specific) India INDIA_AADHAAR_NUMBER Aadhaar identification number issued by the UIDAI India INDIA_PERMANENT_ACCOUNT_NUMBER India Permanent Account Number (PAN) Indonesia INDONESIA_IDENTITY_CARD_NUMBER The national identifier (Indonesia specific) Ireland IRELAND_DRIVING_LICENSE The driver license number (Ireland specific) Ireland IRELAND_PASSPORT_NUMBER The passport number (Ireland specific) Ireland IRELAND_PERSONAL_PUBLIC_SERVICE_NUMBER Ireland personal public service number (PPS) Ireland IRELAND_TAX_IDENTIFICATION_NUMBER Tax identification number (Ireland specific) Ireland IRELAND_VALUE_ADDED_TAX Value-Added Tax (Ireland specific) Israel ISRAEL_IDENTIFICATION_NUMBER The national identifier (Israel specific) Italy ITALY_BANK_ACCOUNT The bank account number (Italy specific) Italy ITALY_DRIVING_LICENSE The driver license number (Italy specific) Italy ITALY_FISCAL_CODE The identifier number, also known as Italian Codice Fiscale Italy ITALY_PASSPORT_NUMBER The passport number (Italy specific) Italy ITALY_VALUE_ADDED_TAX Value-Added Tax (Italy specific) Japan JAPAN_BANK_ACCOUNT The bank account number (Japan specific) Japan JAPAN_DRIVING_LICENSE The driver license number (Japan specific) Japan JAPAN_MY_NUMBER The identifier number (Japan specific) Japan JAPAN_PASSPORT_NUMBER The passport number (Japan specific) Korea KOREA_PASSPORT_NUMBER The passport number (Korea specific) Korea KOREA_RESIDENCE_REGISTRATION_NUMBER_FOR_CITIZENS Korea residence registrant number for citizens Korea KOREA_RESIDENCE_REGISTRATION_NUMBER_FOR_FOREIGNERS Korea residence registrant number for foreigners Kosovo KOSOVO_UNIQUE_MASTER_CITIZEN_NUMBER The unique citizen number (Kosovo specific) Latvia LATVIA_DRIVING_LICENSE The driver license number (Latvia specific) Latvia LATVIA_PASSPORT_NUMBER The passport number (Latvia specific) Latvia LATVIA_PERSONAL_IDENTIFICATION_NUMBER The personal identifier number (Latvia specific) Latvia LATVIA_VALUE_ADDED_TAX Value-Added Tax (Latvia specific) Liechtenstein LIECHTENSTEIN_NATIONAL_IDENTIFICATION_NUMBER The national identifier (Liechtenstein specific) Liechtenstein LIECHTENSTEIN_PASSPORT_NUMBER The passport number (Liechtenstein specific) Liechtenstein LIECHTENSTEIN_TAX_IDENTIFICATION_NUMBER Tax identification number (Liechtenstein specific) Lithuania LITHUANIA_DRIVING_LICENSE The driver license number (Lithuania specific) Lithuania LITHUANIA_PERSONAL_IDENTIFICATION_NUMBER The personal identifier number (Lithuania specific) Lithuania LITHUANIA_TAX_IDENTIFICATION_NUMBER Tax identification number (Lithuania specific) Lithuania LITHUANIA_VALUE_ADDED_TAX Value-Added Tax (Lithuania specific) Luxembourg LUXEMBOURG_DRIVING_LICENSE The driver license number (Luxembourg specific) Luxembourg LUXEMBOURG_NATIONAL_INDIVIDUAL_NUMBER The national identifier (Luxembourg specific) Luxembourg LUXEMBOURG_PASSPORT_NUMBER The passport number (Luxembourg specific) Luxembourg LUXEMBOURG_TAX_IDENTIFICATION_NUMBER Tax identification number (Luxembourg specific) Luxembourg LUXEMBOURG_VALUE_ADDED_TAX Value-Added Tax (Luxembourg specific) Macedonia MACEDONIA_UNIQUE_MASTER_CITIZEN_NUMBER The unique citizen number (Macedonia specific) Malaysia MALAYSIA_MYKAD_NUMBER The national identifier (Malaysia specific) Malaysia MALAYSIA_PASSPORT_NUMBER The passport number (Malaysia specific) Malta MALTA_DRIVING_LICENSE The driver license number (Malta specific) Malta MALTA_NATIONAL_IDENTIFICATION_NUMBER The national identifier (Malta specific) Malta MALTA_TAX_IDENTIFICATION_NUMBER Tax identification number (Malta specific) Malta MALTA_VALUE_ADDED_TAX Value-Added Tax (Malta specific) Mexico MEXICO_CLABE_NUMBER Mexico CLABE (Clave Bancaria Estandarizada) bank number Mexico MEXICO_DRIVING_LICENSE The driver license number (Mexico specific) Mexico MEXICO_PASSPORT_NUMBER The passport number (Mexico specific) Mexico MEXICO_TAX_IDENTIFICATION_NUMBER Tax identification number (Mexico specific) Mexico MEXICO_UNIQUE_POPULATION_REGISTRY_CODE The Clave \u00danica de Registro de Poblaci\u00f3n (CURP) unique identity code for Mexico Montenegro MONTENEGRO_UNIQUE_MASTER_CITIZEN_NUMBER The unique citizen number (Montenegro specific) Netherlands NETHERLANDS_BANK_ACCOUNT The bank account number (Netherlands specific) Netherlands NETHERLANDS_CITIZEN_SERVICE_NUMBER Netherlands citizen number (BSN, burgerservicenummer) Netherlands NETHERLANDS_DRIVING_LICENSE The driver license number (Netherlands specific) Netherlands NETHERLANDS_PASSPORT_NUMBER The passport number (Netherlands specific) Netherlands NETHERLANDS_TAX_IDENTIFICATION_NUMBER Tax identification number (Netherlands specific) Netherlands NETHERLANDS_VALUE_ADDED_TAX Value-Added Tax (Netherlands specific) New Zealand NEW_ZEALAND_DRIVING_LICENSE The driver license number (New Zealand specific) New Zealand NEW_ZEALAND_NATIONAL_HEALTH_INDEX_NUMBER New Zealand health insurance number New Zealand NEW_ZEALAND_TAX_IDENTIFICATION_NUMBER Tax identification number, also known as inland revenue number Norway NORWAY_BIRTH_NUMBER Norwegian national identity number Norway NORWAY_DRIVING_LICENSE The driver license number (Norway specific) Norway NORWAY_HEALTH_INSURANCE_NUMBER Norway health insurance number Norway NORWAY_NATIONAL_IDENTIFICATION_NUMBER The national identifier number (Norway specific) Norway NORWAY_VALUE_ADDED_TAX Value-Added Tax (Norway specific) Philippines PHILIPPINES_DRIVING_LICENSE The driver license number (Philippines specific) Philippines PHILIPPINES_PASSPORT_NUMBER The passport number (Philippines specific) Poland POLAND_DRIVING_LICENSE The driver license number (Poland specific) Poland POLAND_IDENTIFICATION_NUMBER The Poland identifier Poland POLAND_PASSPORT_NUMBER The passport number (Poland specific) Poland POLAND_REGON_NUMBER The REGON identifier number, also known as the Statistical ID Poland POLAND_SSN The social security number (for Poland persons) Poland POLAND_TAX_IDENTIFICATION_NUMBER Tax identification number (Poland specific) Poland POLAND_VALUE_ADDED_TAX Value-Added Tax (Poland specific) Portugal PORTUGAL_DRIVING_LICENSE The driver license number (Portugal specific) Portugal PORTUGAL_NATIONAL_IDENTIFICATION_NUMBER The national identifier number (Portugal specific) Portugal PORTUGAL_PASSPORT_NUMBER The passport number (Portugal specific) Portugal PORTUGAL_TAX_IDENTIFICATION_NUMBER Tax identification number (Portugal specific) Portugal PORTUGAL_VALUE_ADDED_TAX Value-Added Tax (Portugal specific) Romania ROMANIA_DRIVING_LICENSE The driver license number (Romania specific) Romania ROMANIA_NUMERICAL_PERSONAL_CODE The personal identifier number (Romania specific) Romania ROMANIA_PASSPORT_NUMBER The passport number (Romania specific) Romania ROMANIA_VALUE_ADDED_TAX Value-Added Tax (Romania specific) Serbia SERBIA_UNIQUE_MASTER_CITIZEN_NUMBER The unique citizen number (Serbia specific) Serbia SERBIA_VALUE_ADDED_TAX Value-Added Tax (Serbia specific) Serbia VOJVODINA_UNIQUE_MASTER_CITIZEN_NUMBER The unique citizen number for Vojvodina (Serbia specific) Singapore SINGAPORE_DRIVING_LICENSE The driver license number (Singapore specific) Singapore SINGAPORE_NATIONAL_REGISTRY_IDENTIFICATION_NUMBER The national registration identity card for Singapore Singapore SINGAPORE_PASSPORT_NUMBER The passport number (Singapore specific) Singapore SINGAPORE_UNIQUE_ENTITY_NUMBER The unique entity number (Singapore specific) Slovakia SLOVAKIA_DRIVING_LICENSE The driver license number (Slovakia specific) Slovakia SLOVAKIA_NATIONAL_IDENTIFICATION_NUMBER The national identifier number (Slovakia specific) Slovakia SLOVAKIA_PASSPORT_NUMBER The passport number (Slovakia specific) Slovakia SLOVAKIA_VALUE_ADDED_TAX Value-Added Tax (Slovakia specific) Slovenia SLOVENIA_DRIVING_LICENSE The driver license number (Slovenia specific) Slovenia SLOVENIA_PASSPORT_NUMBER The passport number (Slovenia specific) Slovenia SLOVENIA_TAX_IDENTIFICATION_NUMBER Tax identification number (Slovenia specific) Slovenia SLOVENIA_UNIQUE_MASTER_CITIZEN_NUMBER Unique master citizen number (JMBG) for Slovenia citizens Slovenia SLOVENIA_VALUE_ADDED_TAX Value-Added Tax (Slovenia specific) South Africa SOUTH_AFRICA_PERSONAL_IDENTIFICATION_NUMBER The personal identifier number (South Africa specific) Spain SPAIN_DNI The national identity card (Documento Nacional de Identidad) of Spain Spain SPAIN_DRIVING_LICENSE The driver license number (Spain specific) Spain SPAIN_PASSPORT_NUMBER The passport number (Spain specific) Spain SPAIN_SSN The social security number (for Spain persons) Spain SPAIN_TAX_IDENTIFICATION_NUMBER Tax identification number (Spain specific) Spain SPAIN_VALUE_ADDED_TAX Value-Added Tax (Spain specific) Sri Lanka SRI_LANKA_NATIONAL_IDENTIFICATION_NUMBER The national identifier (Sri Lanka specific) Sweden SWEDEN_DRIVING_LICENSE The driver license number (Sweden specific) Sweden SWEDEN_PASSPORT_NUMBER The passport number (Sweden specific) Sweden SWEDEN_PERSONAL_IDENTIFICATION_NUMBER The national identifier number (Sweden specific) Sweden SWEDEN_TAX_IDENTIFICATION_NUMBER Sweden Tax Identification Number (personnummer) Sweden SWEDEN_VALUE_ADDED_TAX Value-Added Tax (Sweden specific) Switzerland SWITZERLAND_AHV The social security number for Swiss persons (AHV) Switzerland SWITZERLAND_HEALTH_INSURANCE_NUMBER Swiss health insurance number Switzerland SWITZERLAND_PASSPORT_NUMBER The passport number (Switzerland specific) Switzerland SWITZERLAND_VALUE_ADDED_TAX Value-Added Tax (Switzerland specific) Thailand THAILAND_PASSPORT_NUMBER The passport number (Thailand specific) Thailand THAILAND_PERSONAL_IDENTIFICATION_NUMBER The personal identifier number (Thailand specific) Turkey TURKEY_NATIONAL_IDENTIFICATION_NUMBER The national identifier number (Turkey specific) Turkey TURKEY_PASSPORT_NUMBER The passport number (Turkey specific) Turkey TURKEY_VALUE_ADDED_TAX Value-Added Tax (Turkey specific) United Kingdom UK_BANK_ACCOUNT The bank account number (UK specific) United Kingdom UK_BANK_SORT_CODE A sort code is a 6 digit number that identifies bank. (UK specific) United Kingdom UK_DRIVING_LICENSE The driver license number (UK specific) United Kingdom UK_ELECTORAL_ROLL_NUMBER The electroral register roll number United Kingdom UK_NATIONAL_HEALTH_SERVICE_NUMBER The national health service number United Kingdom UK_NATIONAL_INSURANCE_NUMBER The national insurance number (UK specific) United Kingdom UK_PASSPORT_NUMBER The passport number (UK specific) United Kingdom UK_PHONE_NUMBER The phone number (UK specific) United Kingdom UK_UNIQUE_TAXPAYER_REFERENCE_NUMBER The unique tax payer number (UK specific) United Kingdom UK_VALUE_ADDED_TAX Value-Added Tax (UK specific) Ukraine UKRAINE_INDIVIDUAL_IDENTIFICATION_NUMBER The unique identifier (Ukraine specific) Ukraine UKRAINE_PASSPORT_NUMBER_DOMESTIC The domestic passport number (Ukraine specific) Ukraine UKRAINE_PASSPORT_NUMBER_INTERNATIONAL The international passport number (Ukraine specific) United Arab Emirates (UAE) UNITED_ARAB_EMIRATES_PERSONAL_NUMBER The personal identifier number (UAE specific) United States of America (USA) USA_SSN The social security number (USA specific) United States of America (USA) USA_ATIN The Adoption Taxpayer Identification Number (USA specific) United States of America (USA) USA_ITIN The Individual Taxpayer Identification Number (USA specific) United States of America (USA) USA_PTIN The Preparer Tax Identification Number (USA specific) United States of America (USA) USA_PASSPORT_NUMBER The passport number (USA specific) United States of America (USA) USA_DRIVING_LICENSE The driver license card ID (USA specific) United States of America (USA) USA_DEA_NUMBER The DEA number (DEA Registration Number) assigned to a healthcare provider (USA specific) United States of America (USA) USA_HCPCS_CODE The Healthcare Common Procedure Coding System (HCPCS) Codes (USA specific) United States of America (USA) USA_NATIONAL_PROVIDER_IDENTIFIER The NPI is a unique identification number for covered healthcare providers (USA specific) United States of America (USA) USA_NATIONAL_DRUG_CODE The universal product identifier for human drugs in the United States (USA specific) United States of America (USA) USA_HEALTH_INSURANCE_CLAIM_NUMBER The identifier assigned to a health insurance claim submitted by a healthcare provider (USA specific) United States of America (USA) USA_MEDICARE_BENEFICIARY_IDENTIFIER The Medicare beneficiary's identification number (USA specific) United States of America (USA) USA_CPT_CODE The Current Procedural Terminology (USA specific) Venezuela VENEZUELA_DRIVING_LICENSE The driver license number (Venezuela specific) Venezuela VENEZUELA_NATIONAL_IDENTIFICATION_NUMBER The national identifier number (Venezuela specific) Venezuela VENEZUELA_VALUE_ADDED_TAX Value-Added Tax (Venezuela specific)"},{"location":"user-guide/appendix-built-in-supported-datatypes/","title":"Supported data types","text":""},{"location":"user-guide/appendix-built-in-supported-datatypes/#structuredsemi-structured-data-scanning","title":"Structured/Semi-structured data scanning","text":"Classifier type Classification string Notes Apache Avro avro Reads the schema at the beginning of the file to determine format. Apache ORC orc Reads the file metadata to determine format. Apache Parquet parquet Reads the schema at the end of the file to determine format. JSON json Reads the beginning of the file to determine format. Binary JSON bson Reads the beginning of the file to determine format. XML xml Reads the beginning of the file to determine format. AWS Glue determines the table schema based on XML tags in the document. For information about creating a custom XML classifier to specify rows in the document, see Writing XML custom classifiers. Amazon Ion ion Reads the beginning of the file to determine format. Combined Apache log combined_apache Determines log formats through a grok pattern. Apache log apache Determines log formats through a grok pattern. Linux kernel log linux_kernel Determines log formats through a grok pattern. Microsoft log microsoft_log Determines log formats through a grok pattern. Ruby log ruby_logger Reads the beginning of the file to determine format. Squid 3.x log squid Reads the beginning of the file to determine format. Redis monitor log redismonlog Reads the beginning of the file to determine format. Redis log redislog Reads the beginning of the file to determine format. CSV csv Checks for the following delimiters: comma (,), pipe ( Amazon Redshift redshift Uses JDBC connection to import metadata. MySQL mysql Uses JDBC connection to import metadata. PostgreSQL postgresql Uses JDBC connection to import metadata. Oracle database oracle Uses JDBC connection to import metadata. Microsoft SQL Server sqlserver Uses JDBC connection to import metadata. Amazon DynamoDB dynamodb Reads data from the DynamoDB table. Compressed Formats Files in the following compressed formats can be classified: ZIP Supported for archives containing only a single file. Note that Zip is not well-supported in other services (because of the archive). BZIP GZIP LZ4 Snappy Supported for both standard and Hadoop native Snappy formats. <p>Note: The solution uses AWS Glue to crawl these data into data catalogs. For specific data format supported by AWS Glue, please refer to Built-in classifiers in AWS Glue.</p>"},{"location":"user-guide/appendix-built-in-supported-datatypes/#unstructured-data-scanning-s3-only","title":"## Unstructured data scanning (S3 only)","text":"File Type Extensions Document \".docx\", \".pdf\" Webpage \".htm\", \".html\" Email \".eml\" Code \".java\", \".py\", \".cpp\", \".c\", \".h\", \".html\", \".css\", \".js\", \".php\", \".rb\", \".swift\", \".go\", \".sql\" Text \".txt\", \".md\", \".log\" Image \u201c.jpg\u201d, \u201c.jpeg\u201d, \u201c.png\u201d, \u201c.gif\u201d, \u201c.bmp\u201d, \u201c.tiff\u201d, \u201c.tif\u201d - (ID cards/Business licenses/Driver's licenses/Faces)"},{"location":"user-guide/appendix-database-proxy/","title":"Appx.Create and config database proxy","text":""},{"location":"user-guide/appendix-database-proxy/#configuring-database-proxy-on-ec2","title":"Configuring Database Proxy on EC2","text":""},{"location":"user-guide/appendix-database-proxy/#create-and-log-in-to-the-proxy-ec2-machine-configure-forwarding-ports","title":"Create and log in to the proxy EC2 machine, configure forwarding ports","text":"<p>Some users' database Security Groups have restrictions that only allow fixed IP access. In this case, users need an EC2 as a Proxy to provide a fixed IP.</p> <p>Next, we will create an EC2 instance as a database proxy, install Nginx software, and set up port forwarding. When making a data source connection, SDPS connects to EC2 and makes a JDBC connection to the database through this EC2.</p>"},{"location":"user-guide/appendix-database-proxy/#step-1-create-an-ec2-instance","title":"Step 1: Create an EC2 Instance","text":"<ul> <li>In the EC2 console. Create an EC2 in the VPC where SDP is located, to be used as a proxy server.</li> <li>Configure the EC2 Security Group: Add an Inbound Rule, allowing all TCP entries from the following two security groups: SDPS-CustomDB, Stack Name-RDSRDSClient</li> </ul>"},{"location":"user-guide/appendix-database-proxy/#step-2-install-nginx-software-on-ec2","title":"Step 2: Install Nginx software on EC2","text":"<ul> <li>Copy the EC2's .pem file to the Bastion host for logging into the proxy server.</li> <li>From your Bastion host, log in to EC2 using SSH, for example:   <code>ssh -i /path/to/your/key.pem ec2-user@ec2-private-ip</code></li> <li>Run the following commands in sequence to install and start Nginx.</li> <li>Installation: <code>sudo yum install nginx nginx-mod-stream</code></li> <li>Start: <code>sudo systemctl start nginx</code></li> <li>Check status: <code>systemctl status nginx</code></li> </ul>"},{"location":"user-guide/appendix-database-proxy/#step-3-configure-nginx-software","title":"Step 3: Configure Nginx software","text":"<ul> <li>Open the configuration file: <code>vim /etc/nginx/nginx.conf</code></li> <li>Edit the configuration file: <pre><code># Replace the default nginx.conf file content with code. You need to make necessary adjustments.\nstream {\n    upstream backend1 {\n        server 10.0.34.171:3306  max_fails=3 fail_timeout=30s; \n        # You need to modify the server address to the IP:Port of your target database, you can also use DomainName:Port format.\n    }\n    server {\n        listen 7001; # This EC2 port is used for forwarding requests (Port)\n        proxy_connect_timeout 2s;\n        proxy_timeout 3s;\n        proxy_pass backend1;\n    }\n}\n</code></pre> !!! Info How to edit the configuration file when there are many databases?     If you need to configure multiple port forwarding, you can use the SDP batch create data source feature, and create the Nginx configuration file through the template. See below Appendix.</li> </ul>"},{"location":"user-guide/appendix-database-proxy/#step-5-reload-the-configuration-file","title":"Step 5: Reload the configuration file","text":"<p>Save the configuration file and reload it to take effect: <code>sudo nginx -s reload</code></p>"},{"location":"user-guide/appendix-database-proxy/#step-7-test-if-the-proxy-ec2-port-forwarding-is-effective-optional","title":"Step 7: Test if the proxy EC2 port forwarding is effective (Optional)","text":"<p>On EC2, install telnet, and test if the local 7001 port can be pinged. <code>sudo yum install telnet</code> <code>telnet 127.0.0.1 7001</code> If configured correctly, you should see the following log: <pre><code>    Trying 127.0.0.1...\n    Connected to 127.0.0.1.\n</code></pre> Now, you have completed the configuration of the proxy server, you can go back to the SDP UI to manually add or batch add data sources.</p>"},{"location":"user-guide/appendix-database-proxy/#appendix-batch-create-data-sources-forwarded-from-the-proxy-server","title":"Appendix: Batch create data sources forwarded from the proxy server","text":""},{"location":"user-guide/appendix-database-proxy/#step-1-download-the-template","title":"Step 1: Download the template","text":"<p>From the SDP UI, download the template for batch creating data sources.</p>"},{"location":"user-guide/appendix-database-proxy/#step-2-edit-the-excel-file","title":"Step 2: Edit the excel file","text":"<p>Fill in the data sources you need to scan.</p> InstanceName SSL Description JDBC_URL JDBC_Databases SecretARN Username Password AccountID Region ProviderID test-instance-7001 1 xxxx1.sql.db.com:23297 jdbc:mysql://172.31.48.6:7001 root Temp123456! 123456789 ap-guangzhou-1 4 test-instance-7002 1 xxxx2.sql.db.com:3306 jdbc:mysql://172.31.48.6:7002 root Temp123456! 123456789 ap-guangzhou-1 4"},{"location":"user-guide/appendix-database-proxy/#step-3-generate-the-nginx-softwares-config-file","title":"Step 3: Generate the Nginx software's config file","text":"<p>(On your local machine) Open the Excel software, in the menu bar click Tools \u2192 Macro \u2192 Visual Basic Editor.</p> <p>Click the run button, and a config.txt file will be generated in the directory where the Excel file is located.</p> <pre><code>// This is a sample.\n// Forward through EC2's 7001 port to xxxx1.sql.db.com:23297 database.\n// Forward through EC2's 7002 port to xxxx2.sql.xxdb.com:3306 database.\nstream {\n    upstream backend1 {\n        server xxxx1.sql.db.com:23297 max_fails=3 fail_timeout=30s;\n    }\n    server {\n        listen 7001; \n        proxy_connect_timeout 2s;\n        proxy_pass backend1;\n    }\n    upstream backend2 {\n        server xxxx2.sql.db.com:3306 max_fails=3 fail_timeout=30s;\n    }\n    server {\n        listen 7002; \n        proxy_connect_timeout 2s;\n        proxy_pass backend2;\n    }\n}\n</code></pre>"},{"location":"user-guide/appendix-organization/","title":"Appx.Add accounts via AWS Organization","text":"<p>You can use AWS Organizations to manage automated deployment of monitored accounts. In AWS CloudFormation, you can configure StackSet to deploy the Agent stack in the target Organizational Unit (OU). After you have configured the deployment, the Agent stack will be automatically deployed to the specified region of the account under the OU. Finally, you need to deploy the IT stack to the Organizations management account or the corresponding CloudFormation delegated account under Organizations, then, you can add member accounts via Organizations.</p> <p></p>"},{"location":"user-guide/appendix-organization/#steps","title":"Steps","text":"<ol> <li>Deploy Admin CloudFormation stack in the Admin account.</li> <li>Register delegated administrator in StackSets in Organization\u2019s management account. For more information, refer to Register a delegated administrator.</li> <li>Deploy IT CloudFormation Stack.</li> <li>Create a role for the solution Admin API.</li> <li>Create StackSet for Agent CloudFormation Stack.</li> <li>Deploy to Organization/OU(s).</li> <li>Add member account via Organizations.</li> <li>Retrieve deployment stacks and member accounts.</li> </ol>"},{"location":"user-guide/appendix-permissions/","title":"Appendix: Permissions for agent CloudFormation stack","text":"<p>The solution follows the least privilege principle to grant permissions to monitored account(s) when deploying the CloudFormation template.</p> <p>The permissions can be described at a high-level:</p> <ul> <li>(Data source) Amazon S3: read only permission for data source scanning.</li> <li>(Data source) Amazon RDS: read only permission for data source scanning. </li> <li>AWS SecretsManager: read only permission. If RDS database is secured with Secrets, the solution will read credentials from Secret Manager.</li> <li>AWS Glue: write permission. Glue data catalog, Glue crawler, Glue job are used. Glue is triggered by Step Functions.</li> <li>AWS StepFunctions: resource created. Step Function is used to orchestrate Glue jobs for data discovery.</li> <li>AWS Lambda: resource created.</li> <li>Amazon CloudWatch: write permission. Lambda logs will be stored in CloudWatch.</li> </ul> <p>For more information</p> <p>You can view specific permission details in Template for Monitored account (Agent template)</p>"},{"location":"user-guide/check-result-dashboard/","title":"Review the Results","text":""},{"location":"user-guide/check-result-dashboard/#view-summary-statistics","title":"View Summary Statistics","text":"<p>In the left-side menu, select Overview. You can see data statistics as shown in the following image.</p> <p> </p>"},{"location":"user-guide/check-result-dashboard/#query-sensitive-data","title":"Query Sensitive Data","text":"<p>In the statistics panel, in the Popular Data Identifiers section, you can perform a reverse search on sensitive data. This allows for quick localization of the data sources containing sensitive data.  </p>"},{"location":"user-guide/data-catalog-create-glue/","title":"Connect to Data Sources","text":"<p>After adding an AWS account, you can connect to AWS Glue Data Catalogs to scan data sources that use AWS Glue as a data catalog (metadata catalog).</p>"},{"location":"user-guide/data-catalog-create-glue/#connect-to-aws-glue-data-source","title":"Connect to AWS Glue Data Source","text":"<p>Supported Big Data Data Types</p> <p>For specific data formats supported by AWS Glue, please refer to Built-in Classifiers in AWS Glue.</p> <p>Additionally, the solution also supports Glue Hudi tables.</p> <ol> <li>On the Connect Data Sources page, click an account to open its details page.</li> <li>On the Glue Data Catalogs tab, select a Glue connection, then choose Sync to Data Catalog.</li> <li>You will see the catalog status turn to gray <code>PENDING</code>, indicating the connection is starting (about 3 minutes).</li> <li>When you see the catalog status turn to green <code>ACTIVE</code>, it means the Glue Data Catalog has been synchronized to the SDP platform's data catalog.</li> </ol> <p>At this point, you have successfully connected to the Glue Data Catalog and can proceed to the next steps.</p>"},{"location":"user-guide/data-catalog-create-jdbc/","title":"Connect to Data Sources - JDBC","text":"<p>When you wish to perform sensitive data scans on a particular type of database, you can use DB instances or databases as your data sources.</p> <p>First, please ensure that when you add an AWS account, you select the CloudFormation method. If you added the account using the JDBC method, please proceed to Connect to the Database via EC2 Proxy.</p> <p>Currently supported JDBC data sources:</p> Supported Database Types Amazon Redshift Amazon Aurora Microsoft SQL Server MySQL Oracle PostgreSQL Snowflake Amazon RDS for MariaDB"},{"location":"user-guide/data-catalog-create-jdbc/#prerequisites-ensure-network-connectivity","title":"Prerequisites - Ensure Network Connectivity","text":"<ol> <li>Please ensure that the inbound rule for the database you want to scan has self-reference in the security group. For detailed steps, refer to the official documentation.</li> <li>Have the database connection credentials ready (username/password).</li> </ol> <p>How to get JDBC Credentials</p> <p>DBA or the business unit creates a read-only user (User) for security auditing purposes. Grant this user read-only permissions: <code>GRANT SHOW VIEW, SELECT ON *.* TO 'reader'@'%'</code>;</p>"},{"location":"user-guide/data-catalog-create-jdbc/#connect-to-a-single-jdbc-data-source","title":"Connect to a Single JDBC Data Source","text":"<ol> <li>From the left menu, select Connect Data Source.</li> <li>Choose the AWS Cloud tab.</li> <li>Click on an AWS account to open the detailed page.</li> <li>Select the Custom Database (JDBC) tab.</li> <li>Click Actions, Add Data Source.</li> <li> <p>In the pop-up window, enter the database credential information. (If you choose the Secret Manager method, you need to host the username/password in Secret Manager beforehand.)</p> Parameter Required Parameter Description Instance Name Yes Database name Enable SSL No Whether to connect via SSL Description (Optional) No Instance description Database Type Yes Choose between MySQL or other. If MySQL is selected, the solution supports automatic querying of databases in the instance. If other, you need to manually add the DB list. JDBC URL (Required) Yes Fill in a database to connect and scan. See the \"JDBC URL Format and Examples\" section at the bottom of this article for specific format. JDBC Databases No If you want to display multiple databases in a data catalog, enter a list of databases. For example, if one data catalog corresponds to one database instance, you can enter multiple databases under instance. If you only want to scan one database under this instance, keep it blank. Credentials Yes Choose username/password or SecretManager. Fill in the database's username/password. VPC Yes Select the VPC where the database is located Subnet Yes Select the VPC subnet where the database is located Security Group Yes Select the VPC security group where the database is located </li> <li> <p>Click Authorize. You can close this window after waiting for 10s.</p> </li> <li>You'll see the directory status change to blue <code>AUTHORIZED</code>. This also means that in the SDP backend, AWS Glue has successfully created a Crawler.</li> </ol> <p>You have now connected to this data source via JDBC \ud83c\udf89. You can proceed to the next step to Define Classification and Grading Templates.</p> <p>Once you have configured the classification template and completed the sensitive data discovery task:</p> <ul> <li>If the task is successful: You will see the directory status on this data source page turn green <code>ACTIVE</code>, indicating that the data directory has been created for this data.</li> <li>If the task fails: You will see the directory status on this data source page turn gray <code>Error message</code>, and you can hover over the error to see the specific information.</li> </ul>"},{"location":"user-guide/data-catalog-create-jdbc/#bulk-automatic-creation-of-jdbc-data-sources","title":"Bulk Automatic Creation of JDBC Data Sources","text":"<p>If you have many data sources and adding them one by one in the UI is not convenient, you can use this bulk creation feature.</p>"},{"location":"user-guide/data-catalog-create-jdbc/#step-1-download-template","title":"Step 1: Download Template","text":"<p>On the AWS account management page, click on the Bulk Create button. On the bulk operation page, first download the \"Bulk Create Data Sources\" template (.xlsm).</p>"},{"location":"user-guide/data-catalog-create-jdbc/#step-2-edit-the-template-file","title":"Step 2: Edit the Template File","text":"<p>Open this file with Microsoft Excel. Excel software will prompt, \"Do you need Enabled Macros?\" Choose Enable. </p> <p>Enter the data sources you need to scan, and it is recommended to do it in small batches (for easier error checking).</p> Instance Name SSL Description JDBC URL JDBC Databases SecretARN Username Password AccountID Region ProviderID test-instance-7001 1 xxxx1.sql.db.com:23297 jdbc:mysql://172.31.48.6:7001 root Temp123456! 123456789 ap-guangzhou-1 1 test-instance-7002 1 xxxx2.sql.db.com:3306 jdbc:mysql://172.31.48.6:7002 root Temp123456! 123456789 ap-guangzhou-1 1"},{"location":"user-guide/data-catalog-create-jdbc/#connect-to-data-sources-via-database-proxy","title":"Connect to Data Sources via Database Proxy","text":"<p>When your RDS/database is in a private network and strict IP restrictions apply (only allowing fixed IPs to access), you need to connect to data sources this way.</p> <ol> <li>Create a database proxy: Create an EC2 as a proxy machine in the VPC where the solution is located. Refer to the detailed steps in Appendix: Create and Configure Database Proxy.</li> <li>When configuring the Proxy, configure the Nginx steps. Refer to the detailed steps in Appendix: Create and Configure Database Proxy.</li> <li>When creating the JDBC data source,<ul> <li>For the Description field, it is recommended to fill in the actual database address.</li> <li>For the JDBC URL field, fill in <code>jdbc:mysql://ec2_public_ip:port/databasename</code>.</li> <li>Fill in the Provider field with 4. (Required for batch creation template)</li> </ul> </li> </ol> <p>--</p>"},{"location":"user-guide/data-catalog-create-jdbc/#parameters-for-creating-data-sources","title":"Parameters for Creating Data Sources","text":"<p>JDBC URL Format and Examples</p> <pre><code>| JDBC URL                                        | Example                                                                                      |\n|-------------------------------------------------|----------------------------------------------------------------------------------------------|\n| Amazon Redshift                                 | `jdbc:redshift://xxx.us-east-1.redshift.amazonaws.com:8192/dev`                              |\n| Amazon RDS for MySQL                            | `jdbc:mysql://xxx-cluster.cluster-xxx.us-east-1.rds.amazonaws.com:3306/employee`             |\n| Amazon RDS for PostgreSQL                       | `jdbc:postgresql://xxx-cluster.cluster-xxx.us-east-1.rds.amazonaws.com:5432/employee`        |\n| Amazon RDS for Oracle                           | `jdbc:oracle:thin://@xxx-cluster.cluster-xxx.us-east-1.rds.amazonaws.com:1521/employee`      |\n| Amazon RDS for Microsoft SQL Server             | `jdbc:sqlserver://xxx-cluster.cluster-xxx.us-east-1.rds.amazonaws.com:1433;databaseName=employee` |\n| Amazon Aurora PostgreSQL                        | `jdbc:postgresql://employee_instance_1.xxxxxxxxxxxx.us-east-2.rds.amazonaws.com:5432/employee` |\n| Amazon RDS for MariaDB                          | `jdbc:mysql://xxx-cluster.cluster-xxx.aws-region.rds.amazonaws.com:3306/employee`            |\n| Snowflake (Standard Connection)                 | `jdbc:snowflake://account_name.snowflakecomputing.com/?user=user_name&amp;db=sample&amp;role=role_name&amp;warehouse=warehouse_name` |\n| Snowflake (AWS PrivateLink Connection)          | `jdbc:snowflake://account_name.region.privatelink.snowflakecomputing.com/?user=user_name&amp;db=sample&amp;role=role_name&amp;warehouse=warehouse_name` |\n</code></pre> <p>Provider Parameter (used for batch creation):</p> Provider Provider Id Description AWS 1 AWS (Installed method: CloudFormation) Tencent 2 Tencent account Google 3 Google account AWS(JDBC Only) 4 AWS (Installed method:JDBC Only)"},{"location":"user-guide/data-catalog-create-rds/","title":"Connect to Data Sources - RDS","text":""},{"location":"user-guide/data-catalog-create-rds/#prerequisites-maintain-network-connectivity","title":"Prerequisites - Maintain Network Connectivity","text":"<ol> <li>Please confirm that when you add an AWS account, you choose the CloudFormation method. If you added the account using the JDBC method, please go to Connect via EC2 Proxy for operations.</li> <li>Ensure that the inbound rule of the RDS to be scanned includes a self-reference of its security group. See official documentation for details.</li> <li>Ensure that the Amazon RDS instance's VPC has at least one private subnet.</li> <li>Ensure that the RDS's VPC meets one of the following conditions: 1) It has a VPC NAT Gateway. 2) It has VPC Endpoints for S3, Glue, KMS, and Secret Manager services. (See official documentation).</li> <li>Prepare RDS connection credentials (username/password).</li> </ol> <p>How to Obtain RDS Credentials</p> <p>DBAs or business teams create a read-only user for security audits. This user only needs database SELECT (read-only) permissions.</p>"},{"location":"user-guide/data-catalog-create-rds/#connect-to-amazon-rds-data-source","title":"Connect to Amazon RDS Data Source","text":"<ol> <li>From the left menu, select Connect Data Source</li> <li>Choose the AWS Cloud tab</li> <li>Click to enter an AWS account and open its detail page</li> <li>Select the Amazon RDS tab. You can see the list of RDS instances in the solution deployment region</li> <li>Choose an RDS instance and click the button Sync to Data Catalog</li> <li> <p>In the popup window, enter RDS credential information. (If you choose the Secret Manager method, you need to manage the username/password for this RDS in Secret Manager in advance.)</p> Parameter Required Description Credentials Yes Choose username and password or SecretManager. Enter the database username/password. </li> <li> <p>Click Connect. You can wait 10 seconds before closing this window.</p> </li> <li>You will see the catalog status turn to gray <code>PENDING</code>, indicating the connection is starting (about 3 minutes).</li> <li>You will see the catalog status turn to blue <code>CRAWLING</code>. (about 15 minutes for 200 tables)</li> <li>When you see the catalog status turn green <code>ACTIVE</code>, it means a data catalog has been created for the RDS instance.</li> </ol> <p>At this point, you have successfully connected the RDS data source and can proceed to the next step \ud83d\udc49 Define Classification and Grading Templates.</p>"},{"location":"user-guide/data-catalog-create-s3/","title":"Connect to Data Sources - S3","text":"<p>After adding a cloud account, you can connect to S3 data sources for sensitive data scanning, which also involves an authorization process.</p> <p>Supported Data/File Types for Scanning</p> <p>Please refer to Appendix: Supported Data Types for Scanning.</p>"},{"location":"user-guide/data-catalog-create-s3/#prerequisites","title":"Prerequisites","text":"<p>If you need to scan unstructured data (such as documents, code, emails, images, etc.), please increase the Service quota.</p> <ul> <li>Global regions: Increase the SageMaker Processing Job instance quota for the region to be scanned through the Service Quota service.</li> <li>China regions: Please contact AWS sales to open a \"Quota Increase Ticket\" with the following content: 'Hello, please increase the parallel running number of SageMaker Processing Job ml.m5.2xlarge instances in this account's certain region (e.g., cn-northwest-1) to 100'.</li> </ul>"},{"location":"user-guide/data-catalog-create-s3/#connect-to-s3-data-source","title":"Connect to S3 Data Source","text":"<ol> <li>From the left menu, select Connect Data Source</li> <li>Choose the AWS Cloud tab</li> <li>Click to enter an AWS account and open its detail page.</li> <li>In the Amazon S3 tab, view the list of S3 buckets in the solution deployment region.</li> <li>Choose an S3 bucket and click Authorize. Alternatively, you can also select Bulk Authorize from the Action list to quickly authorize all S3 buckets.  </li> <li>About half a minute later, you will see the Authorization Status turn green <code>ACTIVE</code>.</li> </ol> <p>At this point, you have successfully connected to the S3 data source and can proceed to the next step \ud83d\udc49 Define Classification and Grading Templates.</p>"},{"location":"user-guide/data-catalog-delete/","title":"Data catalog delete","text":"<p>You can delete data catalogs if you do not need them any more.</p>"},{"location":"user-guide/data-catalog-delete/#delete-data-catalogs-for-s3","title":"Delete data catalogs for S3","text":"<ol> <li>On the Connect to data source page, click one account to open its details page.</li> <li>In the S3 tab, select an S3 bucket, and choose Delete data catalog from the Actions list.</li> </ol>"},{"location":"user-guide/data-catalog-delete/#delete-data-catalogs-for-rds","title":"Delete data catalogs for RDS","text":"<ol> <li>On the Connect to data source page, click one account to open its details page.</li> <li>Choose the Amazon RDS tab.</li> <li>Select a RDS instance, and choose Delete data catalog from the Actions list.</li> </ol>"},{"location":"user-guide/data-catalog-export/","title":"Data catalog export","text":"<p>You can export the latest data catalogs</p> <ol> <li>On the Browse data catalogs page, click button Export data catalogs button.</li> <li>Choose either Download .xlsx file for Microsoft Excel file or Download . csv file</li> </ol> <p>The exported files contains all the details at column level of current data catalogs, the file schema is shown as below. </p> S3 RDS account_id account_id region region s3_bucket rds_instance_id folder_name table_name column_name column_name identifiers identifiers sample_data sample_data bucket_catalog_label instance_catalog_label folder_catalog_label table_catalog_label comment comment"},{"location":"user-guide/data-catalog-labels/","title":"Labeling Data Catalog","text":"<p>The data catalog provides metadata for your data sources. You can add/update labels to provide more metadata information.</p>"},{"location":"user-guide/data-catalog-labels/#sensitive-data-labeling-automatic-or-manual","title":"Sensitive Data labeling (Automatic or Manual)","text":"<p>After the completion of a sensitive data job, \"Privacy Fields\" will be automatically labelged based on the job results. Column-level data in the data catalog will be labelged using data identifiers.</p> <p>You can always manually update the privacy fields in the data catalog.</p> <p>On the Browse Data Catalog page:</p> <ul> <li>In the S3 tab, at either the bucket or folder level, you can click  to select privacy labels from a dropdown list.</li> <li>In the RDS/Glue/JDBC tabs, at either the instance or table level, you can click  to select privacy labels from a dropdown list.</li> </ul>"},{"location":"user-guide/data-catalog-labels/#custom-labeling-manual","title":"Custom Labeling (Manual)","text":"<p>You can add business-related labels (such as business line, department, team, etc.) using the \"Custom labels\" field in the data catalog.</p> <p>On the Browse Data Catalog page:</p> <ul> <li>In the S3 tab, at either the bucket or folder level, you can click  to select custom labels from a dropdown list.</li> <li>In the RDS/Glue/JDBC tabs, at either the instance or table level, you can click  to select custom labels from a dropdown list.</li> </ul> <p>At the bottom of the dropdown list, click the Manage Custom Labels link, which will open a window where you can Add/Edit/Delete custom lables.</p>"},{"location":"user-guide/data-catalog-sync/","title":"Data catalog sync","text":""},{"location":"user-guide/data-catalog-sync/#what-is-data-catalog","title":"What is data catalog?","text":"<p>A data catalog is a repository of metadata of data source (Amazon S3, Amazon RDS). With data catalogs, you can view the column-level information of data. </p>"},{"location":"user-guide/data-catalog-sync/#when-are-the-data-catalogs-synchronized-with-data-source","title":"When are the data catalogs synchronized with data source?","text":"<p>The solution synchronizes the data catalogs with data source in the following situations: - Sync to data catalog (manual). Please refer to Connect to data source - Run sensitive data discovery job (automatic)</p> <p>For more information</p> <p>Synchronizing data catalog will not affect the labels on an existing data catalog.</p> AWS resource Data source change Sync to data catalog Run sensitive data discovery jobs S3 bucket created Y Y S3 bucket deleted Y Y S3 object created Y Y S3 object deleted Y Y S3 object(in bucket root) created Y Y S3 object(in bucket root) deleted N N S3 object updated (timestamp changed) Y Y RDS instance created Y Y RDS instance deleted Y Y RDS instance updated Y Y RDS database created Y Y RDS database deleted Y Y RDS table created Y Y RDS table deleted Y Y RDS table updated Y Y RDS column created Y Y RDS column deleted Y Y RDS column updated Y Y"},{"location":"user-guide/data-identifiers/","title":"Step 2: Define Classification and Grading Templates","text":""},{"location":"user-guide/data-identifiers/#concept","title":"Concept","text":"<ul> <li>Data identifiers are specific rules for detecting certain sensitive data, such as identifiers for ID cards, email addresses, names, etc.</li> <li>A template is a collection of data identifiers. Templates will be used in sensitive data discovery jobs.</li> </ul> <p>Best Practices</p> <p>You need to understand what kind of data is defined as sensitive in your company and the identification rules for these sensitive data. Only those sensitive data that can be defined by regular expressions or AI can be identified by technical means (e.g., using the SDP solution). After defining data identifiers, you need to add them to a template. When running a sensitive data scanning task, it will match the data in the data source against the rules in the template and mark them in the data catalog.</p>"},{"location":"user-guide/data-identifiers/#view-and-edit-built-in-data-identifiers","title":"View and Edit Built-in Data Identifiers","text":"<p>The solution provides built-in data identifiers, which are mainly based on national privacy data rules and provides a reference for you to decide how to classify your data.</p> <p>On the Manage Data Identification Rules page, in the Built-in Data Identifiers tab, you can see a list of built-in data identifiers. For the complete list, please refer to Appendix - Built-in Data Identifiers.</p> <p>You can click  to edit and adjust the data grading classification. By default, these data identifiers have a <code>PERSONAL</code> category attribute and <code>S2</code>/<code>S3</code>/<code>S4</code> identifier label attributes. You can update these attributes according to your sensitive data.</p> <p> </p>"},{"location":"user-guide/data-identifiers/#create-and-edit-custom-data-identifiers","title":"Create and Edit Custom Data Identifiers","text":"<p>On the Manage Data Identification Rules page, in the Custom Data Identifiers tab, you can see your defined list of custom data identifiers. By default, this list is empty. You can create or delete data identifiers based on business-sensitive data.</p> <p>You can click  to edit and adjust the data grading classification. For example, you can define the category as <code>FINANCE</code>/<code>AUTO</code>/<code>GENERAL</code>, and set the data sensitivity level to <code>Level1</code>/<code>Level2</code>/<code>Level3</code>/<code>Level4</code>.</p> <p>To create a new data identifier, select Create Text-based Data Identifier. </p> <p>On the data identifier creation page, you can define rules for sensitive data scanning, as detailed in the following table.  </p> Parameter Required Description Name Yes The name of the data identifier, used for automatic marking when sensitive data is scanned. Description Optional Additional explanation of the identifier, helpful for understanding its use and context. Identification Rules Yes Defines the rules for identifying data, which can be based on column name keywords, regular expressions, or a combination of both. Identifier Attributes Optional Allows for classification and grading of identifiers, e.g., by industry (Finance, Game, Personal, etc.) or security level (S1, S2, S3, etc.). Advanced Rules: Exclude Keywords Optional Defines column name keywords that should not be marked as sensitive data. Advanced Rules: Unstructured Data Optional Applicable for specific security levels (like S3), includes settings for the frequency of rule occurrence and the number of characters between keywords and regular expressions."},{"location":"user-guide/data-identifiers/#add-data-identifiers-to-template","title":"Add Data Identifiers to Template","text":"<ol> <li>In the left menu, select Define Classification Template.</li> <li>Choose Add Data Identifier. You will see a sidebar displaying all data identifiers.</li> <li>Select one or more data identifiers and choose Add to Template.  </li> </ol>"},{"location":"user-guide/data-identifiers/#example-how-data-identifiers-are-labeled-in-data-catalog-after-sensitive-data-discovery-job","title":"Example: how data identifiers are labeled in data catalog after sensitive data discovery job.","text":"<p>Assume we want to detect sensitive data in this table named \"PizzaOrderTable\"</p> id user_name email_address order_id 1 aaa_frankzhu frankzhu@mail.com 12344536 2 aaa_zheng zhm@mail.com 12344536 3 aaa_patrickpark ppark@example.com 12344536 4 aaa_kyle kyle@qq.com 1230000 <p>For example, we define 5 custom data identifiers:</p> Identifier Name Regex Keyword OrderInfo1 OrderInfo1 order OrderInfo2 (disabled) _id UserEmail <code>^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$</code> (disabled) EmailAddress (disabled) themail, email-address, email_address UserPrefix aaa_ user <p>Assume all the above identifiers were added in classification template, and then we started a discovery job. \"PizzaOrderTable\" data catalog result is as below:</p> Column Identifiers Privacy id N/A Non-PII user_name UserPrefix Contain-PII email_address UserEmail, EmailAddress Contain-PII order_id OrderInfo2 Contain-PII <p>Explanation for Identifiers:</p> <ul> <li>The identifier \"OrderInfo1\" is not matched because the regex does not match the data pattern.</li> <li>The identifier \"OrderInfo2\" is labeled on the \"order_id\" column because the keyword \"_id\" partially matches the column name \"order_id\".</li> <li>The identifier \"UserEmail\" is labeled on the \"email_address\" column because the regex matches the data pattern of the \"email_address\" column values.</li> <li>The identifier \"EmailAddress\" is labeled on the \"email_address\" column because one of the keywords \"email_address\" matches the column name.</li> <li>The identifier \"UserPrefix\" is labeled on the \"user_name\" column because both the regex and keywords match.</li> </ul> <p>Explanation for Privacy labels:</p> <ul> <li>Columns \"user_name\", \"email_address\", and \"order_id\" are labeled as (Contain-PII) privacy label because identifiers are matched.</li> <li>Column \"id\" is labeled as (Non-PII) privacy label because no identifier is matched.</li> </ul>"},{"location":"user-guide/data-source/","title":"Connect to Data Sources","text":""},{"location":"user-guide/data-source/#add-aws-account","title":"Add AWS Account","text":"<ol> <li>In the left menu, select Connect Data Source.</li> <li>Choose the AWS Cloud tab.</li> <li>Click the button Add New Account.   </li> <li> <p>Choose one of the following methods to add an account:</p> </li> </ol>"},{"location":"user-guide/data-source/#method-1-authorization-via-cloudformation-applicable-for-the-same-aws-region-automatically-discovers-s3-rds-aws-glue","title":"Method 1: Authorization via CloudFormation (Applicable for the same AWS region, automatically discovers S3, RDS, AWS Glue)","text":"<ol> <li>Open the Standalone AWS Account tab</li> <li>Follow the instructions in Step 1 and Step 2 on this page to install the Agent CloudFormation stack (about 3 mins), which is used for authorizing the account to be scanned. For detailed permission information, please refer to Appendix: Permissions of the Agent CloudFormation Stack</li> <li>Fill in the AWS account ID to be scanned and select the region where the data source is located.</li> <li>Click the button Add This Account.</li> </ol>"},{"location":"user-guide/data-source/#method-2-connect-to-databases-via-jdbc-applicable-for-multiple-aws-regions-manually-add-databases-such-as-redshift-ec2-self-built-databases-multi-cloud-databases","title":"Method 2: Connect to Databases via JDBC (Applicable for multiple AWS regions, manually add databases, such as Redshift, EC2 self-built databases, multi-cloud databases)","text":"<ol> <li>Open the JDBC Only tab.</li> <li>Select regions</li> <li>Fill in the AWS account ID</li> <li>Click the button Add This Account</li> </ol>"},{"location":"user-guide/data-source/#method-3-batch-add-via-cloudformation-authorization-applicable-for-the-same-aws-region-automatically-discovers-s3-rds-aws-glue-manage-accounts-in-bulk-through-aws-organization","title":"Method 3: Batch Add via CloudFormation Authorization (Applicable for the same AWS region, automatically discovers S3, RDS, AWS Glue. Manage accounts in bulk through AWS Organization)","text":"<ol> <li>Open the JDBC Only tab</li> <li>Follow the instructions in Step 1, Step 2, and Step 3 on this page to install the Agent CloudFormation stack. For more information, see Appendix: Adding AWS Accounts via Organization.</li> <li>Fill in the AWS Organization's agent account ID</li> <li>Click the button Add This Account</li> </ol>"},{"location":"user-guide/data-source/#add-other-cloud-accounts","title":"Add Other Cloud Accounts","text":"<ol> <li>In the left menu, select Connect Data Source.</li> <li>Choose the tab for the Cloud Provider you need to add (such as Tencent, Google).</li> <li> <p>Click the button Add New Account.</p> </li> </ol>"},{"location":"user-guide/data-source/#connect-to-databases-via-jdbc-applicable-for-multi-cloud-or-idc-manually-add-databases","title":"Connect to Databases via JDBC (Applicable for multi-cloud or IDC, manually add databases)","text":"<ol> <li>Fill in the account ID</li> <li>Select regions</li> <li>Click the button Add This Account</li> </ol>"},{"location":"user-guide/discovery-job-create/","title":"Create job","text":"<p>You can create and manage jobs for detecting sensitive data. Discovery jobs consist of one or more AWS Glue jobs used for actual data detection. For more information, please see View Job Details.</p>"},{"location":"user-guide/discovery-job-create/#create-discovery-job","title":"Create Discovery Job","text":"<p>From the left menu, select Run Sensitive Data Discovery Job. Click Create Sensitive Data Discovery Job.</p> <p>Step 1: Choose Data Source</p> Provider Data source AWS S3, RDS, Glue, Custom databases\uff0cProxy databases Tencent JDBC Google JDBC <p>What are AWS CustomDB and ProxyDB?</p> <ul> <li>If you are scanning within your own account and connecting to a JDBC data source, select CustomDB</li> <li>If you added your account using CloudFormation and connected to a JDBC data source, select Custom databases</li> <li>If you added your account using JDBC Only and connected to a JDBC data source, select Proxy databases</li> </ul> <p>Step 2: Select specific database to be scanned</p> <p>Step 3: Job Settings</p> Job Setting Description Options Scan Frequency Indicates the scan frequency of the discovery job. On-demand Daily Weekly Monthly Sampling Depth Indicates the number of sampled rows. 100 (Recommended) 10, 30, 60, 100, 300, 500, 1000 Sampling Depth - Unstructured Data Applicable only to S3, the number of unstructured files sampled per folder. Skip, 10 files, 30 files, All files Scan Scope Defines the overall scan scope of the target data source.  \"Comprehensive Scan\" means scanning all target data sources.  \"Incremental Scan\" means skipping data sources unchanged since the last data directory update. Comprehensive Scan Incremental Scan (Recommended) Detection Threshold Defines the tolerance level required for the job. If the sampling depth is 1000 rows, a threshold of 10% means if more than 100 rows (out of 1000 rows) match the identifier rules, the column will be flagged as sensitive. A lower threshold indicates lower tolerance. 10% (Recommended) 20% 30% 40% 50% 100% Overwrite Privacy Tags Manually Updated Choose whether to allow the job to overwrite data directory privacy tags with job results. Do Not Overwrite (Recommended) Overwrite <p>Step 4: Advanced Configuration Items Step 5: Job Preview     After previewing the job, select Run Job.</p>"},{"location":"user-guide/discovery-job-create/#about-incremental-scan","title":"About Incremental Scan:","text":"<p>When choosing \"Incremental Scan\" in the job settings, the scan logic for S3 and RDS differs slightly as follows:</p> <p>S3: When any changes occur in S3 objects, the incremental scan scans at the folder level.</p> <ul> <li> <p>Example: With 1 bucket and 3 folders, each containing a CSV file (with different schemas), if the schema of one folder's file changes, the job will only scan the CSV files in that folder during incremental scanning, skipping the other 2 folders.</p> </li> <li> <p>Example: With 1 bucket and 3 folders, each containing a CSV file (with different schemas), if there are no schema changes but additional rows are added or any file updates occur in one folder, the job will only scan the CSV files in that folder during incremental scanning, skipping the other 2 folders.</p> </li> </ul> <p>RDS: Only when there are column-level changes in RDS tables does the incremental scan scan the table.</p> <ul> <li>Example: With 1 RDS instance and 3 tables, if the schema of one table changes (adding or deleting columns), the job will only scan that table during incremental scanning, skipping the other 2 tables.</li> <li>Example: With 1 RDS instance and 3 tables, if there are no schema changes but rows are added/deleted, none of these 3 tables will be scanned during incremental scanning.</li> </ul>"},{"location":"user-guide/discovery-job-details/","title":"View job details","text":"<p>Sensitive data discovery jobs consist of Glue jobs running in monitored AWS accounts (the same accounts as the data sources).</p> <ul> <li>Return to the list of sensitive data tasks, you can see the job status as <code>Running</code>.</li> <li> <p>To view task progress: Click on the task, in the sidebar, click Task Run Details.  </p> </li> <li> <p>Initially, the progress may remain at 0%. Do not worry, as the system is checking for any changes in the data structure. The progress will update once the actual data scan begins.      </p> </li> </ul> <p>Run Duration</p> <p>The duration depends on the sampling rate, the tables to be scanned, and the number of identifiers in the template.  For example: For one instance with 400 tables, a scan depth of 30, and 21 rules in the template, it might take approximately 25 minutes. Different S3 buckets/database instances are scanned in parallel by the backend.</p> <ul> <li>Wait for the Glue job status to change to <code>SUCCEEDED</code>. This indicates that the scanning task is complete.</li> <li>If the Glue job fails, you can click on the <code>FAILED</code> status to view its error logs.</li> </ul>"},{"location":"user-guide/discovery-job-details/#download-classification-template-snapshot","title":"Download Classification Template Snapshot","text":"<p>You can download a snapshot of the template as it was at the start of the job. The snapshot shows which data identifiers the job was using.</p> <p>On the Job Details page, select Download Snapshot to download the template snapshot in JSON format (.json).</p>"},{"location":"user-guide/discovery-job-pause-and-cancel/","title":"Pause/Continue job","text":"<p>You can only pause or resume a scheduled job. It does NOT mean to pause or resume a running discovery job.</p> <p>To pause a scheduled job, on the Run Sensitive Data Discovery Jobs page, click Actions and select Pause. For instance, if you scheduled a monthly job on the first day of every month and ran a job once in January, choosing Pause will prevent the discovery job from being executed in February.</p> <p>To resume a paused job, on the Run Sensitive Data Discovery Jobs page,click Actions and select Continue.</p>"},{"location":"user-guide/discovery-job-report/","title":"Download job report","text":"<p>On the Run sensitive data discovery job page, click into a specific job to open a window. In the Job history tab, choose a specific job, and choose Download report to download report in Excel format (.xlsx). </p> <p>In the report, each data source is in a separate sheet (tab). For example, a sensitive data discovery job runs for S3 and RDS:</p> <p>The S3 sheet is:</p> account_id region s3_bucket s3_location column_name identifiers sample_data 177416885226 cn-northwest-1 sdps-beta-member s3://sdps-beta-member/ col1 [{identifier=CHINESE-NAME, score=0.6680557137733704}] [cn_*, \u9b4f, \u6881, , , , \u5c39, \u6c49, , \u9c81*] 177416885226 cn-northwest-1 sdps-beta-member s3://sdps-beta-member/ col2 [{identifier=ADDRESS, score=0.6929563446207209}] [cn_*, , \u6d77\u5357\u7701\u7701\u76f4\u8f96\u53bf**, \u6e56\u5317\u7701\u6b66\u6c49\u5e02**, , \u8d35\u5dde\u7701\u516d\u76d8*, \u6e56\u5317\u7701\u968f\u5dde\u5e02**, \u4e0a\u6d77\u5e02\u5e02\u8f96\u533a\u5609**, \u5c71\u897f\u7701\u5ffb\u5dde*, \u8d35\u5dde\u7701\u94dc\u4ec1\u5e02****] <p>The RDS sheet is:</p> account_id region rds_instance_id table_name column_name identifiers sample_data 640463273335 cn-northwest-1 db-instance-2 orderpaymentdb_orderpaymenttable cn_bank_card [{identifier=BANK-CARD-NUMBER, score=0.5269872423945045}] [62426**, 62061*, 627582**, 625612*, 624299**, 3462**, 627119*, 426927*, 620930**, 62385**] 640463273335 cn-northwest-1 db-instance-1-instance-1 shipmenttrackingdb_shipmenttable cn_car_license [{identifier=ADDRESS, score=0.10789832248611221}, {identifier=NUMBER-PLATE, score=1.0}] [\u6d25JE*, \u743cWM, \u8c6bRU, \u85cfGK, \u9ed1YR, \u6caaVK, \u4eacR7, \u664bD3, \u82cfT *, \u6e1dC1*]"},{"location":"user-guide/discovery-job-rerun-and-duplicate/","title":"Re-run/Duplicate job","text":""},{"location":"user-guide/discovery-job-rerun-and-duplicate/#re-run-a-discovery-job","title":"Re-run a discovery Job","text":"<p>On the Run Sensitive Data Discovery Jobs page, click Actions and select Execute once. You can create a new discovery job and run it with the same settings as the previous run.</p>"},{"location":"user-guide/discovery-job-rerun-and-duplicate/#duplicate-a-discovery-job","title":"Duplicate a Discovery Job","text":"<p>On the Run Sensitive Data Discovery Jobs page, click Actions and select Duplicate. You can duplicate a job setting and modify it to start a new job.</p>"},{"location":"user-guide/get-started/","title":"Overview","text":"<p>After successfully deploying the solution, you can access the console to detect sensitive data.</p> <ul> <li>Step 1: Connect to Data Sources Add AWS accounts and create data catalogs.</li> <li>Step 2: Define Classification Templates Manage data identifiers within templates to define sensitive data.</li> <li>Step 3: Run Sensitive Data Discovery Jobs Detect sensitive data by creating and managing data discovery jobs.</li> <li>Step 4: Review Results View the metadata (such as table structures) of data sources through the data catalog, see updated data catalogs, and check the dashboard on the \"Summary\" page.</li> </ul>"}]}